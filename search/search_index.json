{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p> Official Python SDK for Lexsi.ai \u2014 Fast, Secure &amp; Developer-Friendly </p> <p> </p>"},{"location":"#lexsi-sdk","title":"lexsi-sdk","text":"<p><code>lexsi-sdk</code> is the official Python SDK for interacting with the Lexsi.ai platform. It provides a clean, high-performance interface for accessing chat completions, embeddings, model management, logging, and more \u2014 optimized for production workloads, enterprise integration, and research workflows.</p>"},{"location":"#installation","title":"\ud83d\udce6 Installation","text":"<p>Install from PyPI:</p> <pre><code>pip install lexsi-sdk\n</code></pre>"},{"location":"#examples-notebooks-for-sdk","title":"\ud83d\udcd3 Examples Notebooks for SDK","text":"Modality Notebook Description Tabular Classic ML ML Models Tabular TabTune TabTune Project"},{"location":"api/","title":"Lexsi REST API (curl snippets)","text":"<p>Use these ready-to-run curl with your Lexsi API key. Replace the placeholder values before running.</p>"},{"location":"api/#basic","title":"Basic","text":""},{"location":"api/#health-check","title":"Health check","text":"<pre><code>curl -X GET \"https://apiv1.lexsi.ai/healthcheck\"\n</code></pre>"},{"location":"api/#tabular-modality-api","title":"Tabular Modality API","text":""},{"location":"api/#generate-prediction-and-explainability","title":"Generate Prediction and Explainability","text":"<pre><code>curl --http2 -X POST \"https://apiv1.lexsi.ai/v2/project/case-register\" \\\n   -H \"x-api-token: &lt;$X_API_TOKEN&gt;\" \\\n   -H \"Content-Type: application/x-www-form-urlencoded\" \\\n   --data-urlencode \"client_id=&lt;$USERNAME&gt;\" \\\n   --data-urlencode \"project_name=&lt;$PROJECTNAME&gt;\" \\\n   --data-urlencode \"unique_identifier=&lt;$UNIQUE_IDENTIFIER&gt;\" \\\n   --data-urlencode \"tag=&lt;$Tag&gt;\" \\\n   --data-urlencode \"serverless_instance_type=&lt;$SERVERLESS_COMPUTE_TYPE&gt;\" \\\n   --data-urlencode \"xai=&lt;$EXPLAINABILITY_METHOD&gt;\" \\\n   --data-urlencode \"data=[{\n    \\\"$UNIQUE_ID_KEY\\\": \\\"$UNIQUE_IDENTIFIER\\\",\n    \\\"$SAMPLE_KEY\\\": \\\"$SAMPLE_VALUE\\\",\n    \\\"$SAMPLE_KEY\\\": \\\"$SAMPLE_VALUE\\\",\n        .\n        .\n    \\\"$SAMPLE_KEY\\\": \\\"$SAMPLE_VALUE\\\"\n  }]\"\n</code></pre> Placeholder Description <code>$X_API_TOKEN</code> Your Lexsi API token <code>$USERNAME</code> Your Lexsi username / client ID <code>$PROJECTNAME</code> Target project name <code>$UNIQUE_IDENTIFIER</code> Unique row ID for the registered case <code>$Tag</code> Dataset tag to attach to this upload or prediction <code>$SERVERLESS_COMPUTE_TYPE</code> Serverless instance type (e.g. <code>NOVA</code>, <code>GOVA</code>, or <code>local</code>) <code>$EXPLAINABILITY_METHOD</code> Explainability technique to run (e.g. <code>shap</code>, <code>lime</code>) <code>$data</code> List of JSON objects containing feature key/value pairs"},{"location":"api/#image-modality-api","title":"Image Modality API","text":"<pre><code>curl --http2 -X POST \"https://apiv1.lexsi.ai/v2/project/case-register\" \\\n   -H \"x-api-token: &lt;$X_API_TOKEN&gt;\" \\\n   -F \"client_id=&lt;$USERNAME&gt;\" \\\n   -F \"project_name=&lt;$PROJECT_NAME&gt;\" \\\n   -F \"unique_identifier=&lt;$UNIQUE_IDENTIFIER&gt;\" \\\n   -F \"tag=&lt;$TAG&gt;\" \\\n   -F \"xai=&lt;$EXPLAINABILITY_METHOD&gt;\" \\\n   -F \"serverless_instance_type=&lt;$SERVERLESS_COMPUTE_TYPE&gt;\" \\\n   -F \"in_file=&lt;$IMAGE_PATH&gt;\" \\\n   -F \"image_class=&lt;$IMAGE_CLASS&gt;\"\n</code></pre> Placeholder Description <code>$X_API_TOKEN</code> Your Lexsi API token <code>$USERNAME</code> Your Lexsi username / client ID <code>$PROJECT_NAME</code> Target project name for this image case <code>$UNIQUE_IDENTIFIER</code> Filename or unique identifier for the image case <code>$TAG</code> Tag to associate with the upload <code>$EXPLAINABILITY_METHOD</code> Explainability method to run (e.g.  <code>gradcam</code>,<code>dlb</code>,<code>ig</code>) <code>$SERVERLESS_COMPUTE_TYPE</code> Serverless instance type for processing <code>$IMAGE_PATH</code> Local filesystem path to the image file <code>$IMAGE_CLASS</code> Optional class or label for the image"},{"location":"api/#text-modality-apis","title":"Text Modality API's","text":""},{"location":"api/#text-generation-api","title":"Text Generation API","text":"<pre><code>curl --http2 -X POST 'https://apiv1.lexsi.ai/v2/project/case-register' \\\n  -H \"x-api-token: &lt;$API_TOKEN&gt;\" \\\n  -F \"provider= &lt;$MODEL_PROVIDER&gt;\" \\\n  -F \"client_id=&lt;$USERNAME&gt;\" \\\n  -F \"project_name=&lt;$PROJECT_NAME&gt;\" \\\n  -F \"prompt=&lt;$INPUT_PROMPT&gt;\" \\\n  -F \"serverless_instance_type=&lt;$SERVERLESS_COMPUTE_TYPE&gt;\" \\\n  -F \"model_name=&lt;$MODEL_NAME&gt;\" \\\n  -F \"min_tokens=&lt;$MIN_TOKENS&gt;\" \\\n  -F \"max_tokens=&lt;$MAX_TOKENS&gt;\" \\\n  -F \"session_id=&lt;$SESSION_ID&gt;\" \\\n  -F \"instance_type=&lt;$POD_INSTANCE_TYPE&gt;\" \\\n  -F \"explain_model=&lt;$EXPLAINABILTY_FLAG&gt;\"\n</code></pre> Placeholder Description <code>$API_TOKEN</code> Your Lexsi API token <code>$MODEL_PROVIDER</code> Provider identifier (e.g. <code>Lexsi</code>, <code>OpenAI</code>, <code>Grok</code>) <code>$USERNAME</code> Your Lexsi username / client ID <code>$PROJECT_NAME</code> Target text project name <code>$INPUT_PROMPT</code> User prompt to send to the model <code>$SERVERLESS_COMPUTE_TYPE</code> Serverless instance type for processing <code>$MODEL_NAME</code> Model name within the selected provider <code>$MIN_TOKENS</code> / <code>$MAX_TOKENS</code> Minimum and maximum tokens to generate (integers) <code>$SESSION_ID</code> Optional session ID for threaded or multi-turn conversations <code>$POD_INSTANCE_TYPE</code> Optional dedicated instance type for processing <code>$EXPLAINABILTY_FLAG</code> Boolean flag indicating whether explainability is computed"},{"location":"api/#chat-completions","title":"Chat completions","text":"<pre><code>curl --request POST 'https://apiv1.lexsi.ai/gateway/v1/chat/completions' \\\n  --header \"x-api-token: &lt;$API_TOKEN&gt;\" \\\n  --header 'Content-Type: application/json' \\\n  --data \"{\n    \\\"provider\\\": \\\"&lt;$MODEL_PROVIDER_NAME&gt;\\\",\n    \\\"api_key\\\": \\\"&lt;$API_KEY&gt;\\\",\n    \\\"client_id\\\": \\\"&lt;$USERNAME&gt;\\\",\n    \\\"max_tokens\\\": &lt;$MAX_NEW_TOKENS&gt;,\n    \\\"project_name\\\": \\\"&lt;$PROJECT_NAME&gt;\\\",\n    \\\"model\\\": \\\"&lt;$MODEL_NAME&gt;\\\",\n    \\\"messages\\\": [\n      {\n        \\\"role\\\": \\\"&lt;$ROLE&gt;\\\",\n        \\\"content\\\": \\\"&lt;$PROMPT&gt;\\\"\n      }\n    ],\n    \\\"stream\\\": &lt;$STREAM_BOOL&gt;\n  }\"\n</code></pre> Placeholder Description <code>$API_TOKEN</code> Your Lexsi API token <code>$MODEL_PROVIDER_NAME</code> Provider identifier (e.g. <code>openai</code>) <code>$API_KEY</code> Provider-specific API key or token, if required <code>$USERNAME</code> Your Lexsi username / client ID <code>$MAX_NEW_TOKENS</code> Maximum number of tokens to generate (integer) <code>$PROJECT_NAME</code> Target project name <code>$MODEL_NAME</code> Model name within the selected provider <code>$ROLE</code> Message role (e.g. <code>user</code>, <code>system</code>) <code>$PROMPT</code> Input text prompt <code>$STREAM_BOOL</code> Boolean flag (<code>true</code> or <code>false</code>) to enable or disable streaming responses"},{"location":"api/#completions","title":"Completions","text":"<pre><code>curl --request POST 'https://apiv1.lexsi.ai/gateway/v1/completions' \\\n  --header \"x-api-token: &lt;$API_TOKEN&gt;\" \\\n  --header 'Content-Type: application/json' \\\n  --data \"{\n    \\\"provider\\\": \\\"&lt;$MODEL_PROVIDER_NAME&gt;\\\",\n    \\\"api_key\\\": \\\"&lt;$API_KEY&gt;\\\",\n    \\\"client_id\\\": \\\"&lt;$USERNAME&gt;\\\",\n    \\\"max_tokens\\\": &lt;$MAX_NEW_TOKENS&gt;,\n    \\\"project_name\\\": \\\"&lt;$PROJECT_NAME&gt;\\\",\n    \\\"model\\\": \\\"&lt;$MODEL_NAME&gt;\\\",\n    \\\"prompt\\\": \\\"&lt;$PROMPT&gt;\\\",\n    \\\"stream\\\": \\\"&lt;$STREAM_BOOL&gt;\\\"\n  }\"\n</code></pre> Placeholder Description <code>$API_TOKEN</code> Your Lexsi API token <code>$MODEL_PROVIDER_NAME</code> Provider identifier (e.g. <code>openai</code>) <code>$API_KEY</code> Provider-specific API key or token, if required <code>$USERNAME</code> Your Lexsi username / client ID <code>$MAX_NEW_TOKENS</code> Maximum number of tokens to generate (integer) <code>$PROJECT_NAME</code> Target project name <code>$MODEL_NAME</code> Model name within the selected provider <code>$PROMPT</code> Input text prompt <code>$STREAM_BOOL</code> Boolean flag (<code>true</code> or <code>false</code>) to enable or disable streaming responses"},{"location":"api/#embeddings","title":"Embeddings","text":"<pre><code>curl -X POST \"https://apiv1.lexsi.ai/gateway/v1/embeddings\" \\\n  -H \"Authorization: Bearer $LEXSI_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d \"{\n    \\\"model\\\": \\\"&lt;$MODEL_NAME&gt;\\\",\n    \\\"input\\\": [\\\"&lt;$INPUT_PROMPT&gt;\\\"]\n  }\"\n</code></pre> Placeholder Description <code>$LEXSI_API_KEY</code> Your Lexsi API key for gateway endpoints <code>$MODEL_NAME</code> Embedding model to use <code>$INPUT_PROMPT</code> Text input to generate embeddings for (single string or list of strings)"},{"location":"api/#image-generation","title":"Image generation","text":"<pre><code>curl -X POST \"https://apiv1.lexsi.ai/gateway/v1/images/generations\" \\\n  -H \"Authorization: Bearer $LEXSI_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d \"{\n    \\\"model\\\": \\\"&lt;$MODEL_NAME&gt;\\\",\n    \\\"prompt\\\": \\\"&lt;$INPUT_PROMPT&gt;\\\"\n  }\"\n</code></pre> Placeholder Description <code>$LEXSI_API_KEY</code> Your Lexsi API key for gateway endpoints <code>$MODEL_NAME</code> Image generation model to use <code>$INPUT_PROMPT</code> Text prompt describing the image to generate"},{"location":"redoc/","title":"API Playground (Swagger UI)","text":"<p>Use the embedded Swagger UI to explore and call the Lexsi API. Replace the bearer token in the Authorize dialog with your <code>LEXSI_API_KEY</code>. Ensure CORS is allowed from this docs origin.</p> <p></p>"},{"location":"sdk/","title":"SDK Documentation","text":""},{"location":"sdk/#getting-started","title":"Getting Started","text":"<p>SDK token can be generated from the Lexsi Console under Dashboard \u2192 Access Token: https://console.lexsi.ai/dashboard/access-token</p> <pre><code>from lexsi_sdk import lexsi\n\n# Login using your Lexsi SDK Token\nlexsi.login(sdk_access_token=\"YOUR_SDK_TOKEN\")\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.lexsi.LEXSI","title":"LEXSI","text":"<pre><code>LEXSI(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Base entry-point class for interacting with the Lexsi.ai platform. Handles authentication, organization discovery and selection, notification retrieval, and provides access to higher-level SDK abstractions.</p> <p>Initialize the API client using environment-derived settings. Stores configuration and prepares the object for use.</p> Source code in <code>lexsi_sdk/core/lexsi.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize the API client using environment-derived settings.\n    Stores configuration and prepares the object for use.\"\"\"\n    super().__init__(**kwargs)\n\n    debug = self.env.get_debug()\n    base_url = self.env.get_base_url()\n\n    self.api_client = APIClient(debug=debug, base_url=base_url)\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.lexsi.LEXSI.available_node_servers","title":"available_node_servers","text":"<pre><code>available_node_servers(type=None)\n</code></pre> <p>Retrieve a dictionary or list of available custom servers that can be used for deploying models or running compute-heavy workloads.</p> <p>Parameters:</p> Name Type Description Default <code>type</code> <code>Optional[Literal['GPU', 'CPU']]</code> <p>Type of server to filter by GPU/CPU</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>response</p> Source code in <code>lexsi_sdk/core/lexsi.py</code> <pre><code>def available_node_servers(self, type: Optional[Literal[\"GPU\", \"CPU\"]]= None) -&gt; dict:\n    \"\"\"Retrieve a dictionary or list of available custom servers that can be used for deploying models or running compute-heavy workloads.\n    :param type: Type of server to filter by GPU/CPU\n    :return: response\n    \"\"\"\n    if type and type not in [\"GPU\", \"CPU\"]:\n        raise ValueError(\"Invalid type. Must be 'GPU' or 'CPU'.\")\n    res = self.api_client.get(AVAILABLE_CUSTOM_SERVERS_URI)\n    cpu_gpu_dict = split_cpu_gpu_servers(res)\n    if type==\"GPU\":\n        return cpu_gpu_dict[\"gpu_servers\"]\n    elif type==\"CPU\":\n        return cpu_gpu_dict[\"cpu_servers\"]\n    else:\n        return {\"CPU nodes\": cpu_gpu_dict[\"cpu_servers\"], \"GPU nodes\": cpu_gpu_dict[\"gpu_servers\"]}\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.lexsi.LEXSI.available_pod_servers","title":"available_pod_servers","text":"<pre><code>available_pod_servers(type=None)\n</code></pre> <p>Retrieve a dictionary of available batch servers (compute instances) that can be used for running custom batch tasks. Useful for selecting compute resources.</p> <p>Parameters:</p> Name Type Description Default <code>type</code> <code>Optional[Literal['GPU', 'CPU']]</code> <p>Type of server to filter by GPU/CPU</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>response</p> Source code in <code>lexsi_sdk/core/lexsi.py</code> <pre><code>def available_pod_servers(self, type: Optional[Literal[\"GPU\", \"CPU\"]]= None) -&gt; dict:\n    \"\"\"Retrieve a dictionary of available batch servers (compute instances) that can be used for running custom batch tasks. Useful for selecting compute resources.\n    :param type: Type of server to filter by GPU/CPU\n    :return: response\n    \"\"\"\n    if type and type not in [\"GPU\", \"CPU\"]:\n        raise ValueError(\"Invalid type. Must be 'GPU' or 'CPU'.\")\n    res = self.api_client.get(AVAILABLE_BATCH_SERVERS_URI)\n    if type==\"GPU\":\n        return res[\"available_gpu_custom_servers\"]\n    elif type==\"CPU\":\n        return res[\"details\"]\n    else:\n        return {\"CPU pods\": res[\"details\"], \"GPU pods\": res[\"available_gpu_custom_servers\"]}\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.lexsi.LEXSI.available_serverless_types","title":"available_serverless_types","text":"<pre><code>available_serverless_types(type=None)\n</code></pre> <p>Retrieve a list of available serverless types that can be used for deploying models or running workloads.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>response</p> Source code in <code>lexsi_sdk/core/lexsi.py</code> <pre><code>def available_serverless_types(self, type: Optional[Literal[\"GPU\", \"CPU\"]]= None) -&gt; List[str]:\n    \"\"\"Retrieve a list of available serverless types that can be used for deploying models or running workloads.\n    :return: response\n    \"\"\"\n    if type and type not in [\"GPU\", \"CPU\"]:\n        raise ValueError(\"Invalid type. Must be 'GPU' or 'CPU'.\")\n    res = self.api_client.get(AVAILABLE_SERVERLESS_URI)\n    cpu_gpu_dict = split_cpu_gpu_servers(res)\n    if type==\"GPU\":\n        return cpu_gpu_dict[\"gpu_servers\"]\n    elif type==\"CPU\":\n        return cpu_gpu_dict[\"cpu_servers\"]\n    else:\n        return {\"CPU serverless\": cpu_gpu_dict[\"cpu_servers\"], \"GPU serverless\": cpu_gpu_dict[\"gpu_servers\"]}\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.lexsi.LEXSI.clear_notifications","title":"clear_notifications","text":"<pre><code>clear_notifications()\n</code></pre> <p>Clear all notifications for the user by sending a POST request. Returns a confirmation string indicating success.</p> <p>Returns:</p> Type Description <code>str</code> <p>response</p> Source code in <code>lexsi_sdk/core/lexsi.py</code> <pre><code>def clear_notifications(self) -&gt; str:\n    \"\"\"Clear all notifications for the user by sending a POST request. Returns a confirmation string indicating success.\n\n    :return: response\n    \"\"\"\n    res = self.api_client.post(CLEAR_NOTIFICATIONS_URI)\n\n    if not res[\"success\"]:\n        raise Exception(\"Error while clearing user notifications.\")\n\n    return res[\"details\"]\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.lexsi.LEXSI.create_organization","title":"create_organization","text":"<pre><code>create_organization(organization_name)\n</code></pre> <p>Create a new organization with the given name. It sends a POST request to the API and returns an Organization object representing the created organization.</p> <p>Parameters:</p> Name Type Description Default <code>organization_name</code> <code>str</code> <p>Name of the new organization</p> required <p>Returns:</p> Type Description <code>Organization</code> <p>Organization object</p> Source code in <code>lexsi_sdk/core/lexsi.py</code> <pre><code>def create_organization(self, organization_name: str) -&gt; Organization:\n    \"\"\"Create a new organization with the given name. It sends a POST request to the API and returns an Organization object representing the created organization.\n\n    :param organization_name: Name of the new organization\n    :return: Organization object\n    \"\"\"\n    payload = {\"organization_name\": organization_name}\n    res = self.api_client.post(CREATE_ORGANIZATION_URI, payload)\n\n    if not res[\"success\"]:\n        raise Exception(res.get(\"details\", \"Failed to create organization\"))\n\n    return Organization(api_client=self.api_client, **res[\"organization_details\"])\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.lexsi.LEXSI.get_notifications","title":"get_notifications","text":"<pre><code>get_notifications()\n</code></pre> <p>Fetch notifications for the user from Lexsi.ai. Notifications include project names, messages and timestamps and are returned as a DataFrame.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>notification details dataFrame</p> Source code in <code>lexsi_sdk/core/lexsi.py</code> <pre><code>def get_notifications(self) -&gt; pd.DataFrame:\n    \"\"\"Fetch notifications for the user from Lexsi.ai. Notifications include project names, messages and timestamps and are returned as a DataFrame.\n\n    :return: notification details dataFrame\n    \"\"\"\n    res = self.api_client.get(GET_NOTIFICATIONS_URI)\n\n    if not res[\"success\"]:\n        raise Exception(\"Error while getting user notifications.\")\n\n    notifications = res[\"details\"]\n\n    if not notifications:\n        return \"No notifications found.\"\n\n    return pd.DataFrame(notifications).reindex(\n        columns=[\"project_name\", \"message\", \"time\"]\n    )\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.lexsi.LEXSI.login","title":"login","text":"<pre><code>login(sdk_access_token=None)\n</code></pre> <p>Authenticate with Lexsi.ai using an access token. It prompts for or reads the access token from the environment variable XAI_ACCESS_TOKEN and sets it on the API client, enabling subsequent calls to the platform.</p> <p>Parameters:</p> Name Type Description Default <code>sdk_access_token</code> <code>Optional[str]</code> <p>SDK Access Token, defaults to XAI_ACCESS_TOKEN environment variable</p> <code>None</code> Source code in <code>lexsi_sdk/core/lexsi.py</code> <pre><code>def login(self, sdk_access_token: Optional[str] = None):\n    \"\"\"Authenticate with Lexsi.ai using an access token. It prompts for or reads the access token from the environment variable XAI_ACCESS_TOKEN and sets it on the API client, enabling subsequent calls to the platform.\n\n    :param sdk_access_token: SDK Access Token, defaults to XAI_ACCESS_TOKEN environment variable\n    \"\"\"\n    if not sdk_access_token:\n        sdk_access_token = os.environ.get(\"XAI_ACCESS_TOKEN\", None) or getpass.getpass(\n            \"Enter your Lexsi.ai SDK Access Token: \"\n        )\n\n    if not sdk_access_token:\n        raise ValueError(\"Either set XAI_ACCESS_TOKEN or pass the Access token\")\n\n    res = self.api_client.post(LOGIN_URI, payload={\"access_token\": sdk_access_token})\n    self.api_client.update_headers(res[\"access_token\"])\n    self.api_client.set_access_token(sdk_access_token)\n\n    print(\"Authenticated successfully.\")\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.lexsi.LEXSI.organization","title":"organization","text":"<pre><code>organization(organization_name)\n</code></pre> <p>Select a specific organization by its name. If the name is \"personal\", returns the personal organization. Otherwise, it searches the user\u2019s organizations and returns an Organization object for further management.</p> <p>Parameters:</p> Name Type Description Default <code>organization_name</code> <code>str</code> <p>Name of the organization to be used</p> required <p>Returns:</p> Type Description <code>Organization</code> <p>Organization object</p> Source code in <code>lexsi_sdk/core/lexsi.py</code> <pre><code>def organization(self, organization_name: str) -&gt; Organization:\n    \"\"\"Select a specific organization by its name. If the name is \"personal\", returns the personal organization. Otherwise, it searches the user\u2019s organizations and returns an Organization object for further management.\n\n    :param organization_name: Name of the organization to be used\n    :return: Organization object\n    \"\"\"\n    if organization_name == \"personal\":\n        return Organization(\n            api_client=self.api_client,\n            **{\n                \"name\": \"Personal\",\n                \"organization_owner\": True,\n                \"organization_admin\": True,\n                \"current_users\": 1,\n                \"created_by\": \"you\",\n            },\n        )\n\n    organizations = self.api_client.get(USER_ORGANIZATION_URI)\n\n    if not organizations[\"success\"]:\n        raise Exception(organizations.get(\"details\", \"Failed to get organizations\"))\n\n    user_organization = [\n        Organization(api_client=self.api_client, **organization)\n        for organization in organizations[\"details\"]\n    ]\n\n    organization = next(\n        filter(\n            lambda organization: organization.name == organization_name,\n            user_organization,\n        ),\n        None,\n    )\n\n    if organization is None:\n        raise Exception(\"Organization Not Found\")\n\n    return organization\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.lexsi.LEXSI.organizations","title":"organizations","text":"<pre><code>organizations()\n</code></pre> <p>Retrieve all organizations associated with the authenticated user. Returns a DataFrame listing organization names and metadata such as ownership, admin status, number of users, creator, and creation date.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Organization details dataframe</p> Source code in <code>lexsi_sdk/core/lexsi.py</code> <pre><code>def organizations(self) -&gt; pd.DataFrame:\n    \"\"\"Retrieve all organizations associated with the authenticated user. Returns a DataFrame listing organization names and metadata such as ownership, admin status, number of users, creator, and creation date.\n\n    :return: Organization details dataframe\n    \"\"\"\n\n    res = self.api_client.get(USER_ORGANIZATION_URI)\n\n    if not res[\"success\"]:\n        raise Exception(res.get(\"details\", \"Failed to get organizations\"))\n\n    res[\"details\"].insert(\n        0,\n        {\n            \"name\": \"personal\",\n            \"organization_owner\": True,\n            \"organization_admin\": True,\n            \"current_users\": 1,\n            \"created_by\": res.get(\"current_user\", {}).get(\"username\", \"\"),\n            \"created_at\": res.get(\"current_user\", {}).get(\"created_at\", \"\"),\n        },\n    )\n\n    organization_df = pd.DataFrame(\n        res[\"details\"],\n        columns=[\n            \"name\",\n            \"organization_owner\",\n            \"organization_admin\",\n            \"current_users\",\n            \"created_by\",\n            \"created_at\",\n        ],\n    )\n\n    return organization_df\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.lexsi.LEXSI.register_case","title":"register_case","text":"<pre><code>register_case(\n    token,\n    client_id,\n    unique_identifier=None,\n    project_name=None,\n    tag=None,\n    data=None,\n    processed_data=False,\n    merge=False,\n    image_class=None,\n    prompt=None,\n    serverless_type=None,\n    explainability_method=None,\n    explain_model=False,\n    session_id=None,\n    xai=None,\n    file_path=None,\n)\n</code></pre> <p>Register a new case entry with raw or processed payloads. Encapsulates a small unit of SDK logic and returns the computed result.</p> Source code in <code>lexsi_sdk/core/lexsi.py</code> <pre><code>def register_case(\n    self,\n    token: str,\n    client_id: str,\n    unique_identifier: Optional[str] = None,\n    project_name: str = None,\n    tag: Optional[str] = None,\n    data: Optional[str] = None,\n    processed_data: Optional[bool] = False,\n    merge: Optional[bool] = False,\n    image_class: Optional[str] = None,\n    prompt: Optional[str] = None,\n    serverless_type: Optional[str] = None,\n    explainability_method: Optional[str] = None,\n    explain_model: Optional[bool] = False,\n    session_id: Optional[str] = None,\n    xai: Optional[str] = None,\n    file_path: Optional[str] = None,\n):\n    \"\"\"Register a new case entry with raw or processed payloads.\n    Encapsulates a small unit of SDK logic and returns the computed result.\"\"\"\n    form_data = {\n        \"client_id\": client_id,\n        \"project_name\": project_name,\n        \"unique_identifier\": unique_identifier,\n        \"tag\": tag,\n        \"data\": json.dumps(data) if isinstance(data, list) else data,\n        \"processed_data\": str(processed_data).lower(),\n        \"merge\": str(merge).lower(),\n        \"image_class\": image_class,\n        \"prompt\": prompt,\n        \"serverless_type\": serverless_type,\n        \"explainability_method\": explainability_method,\n        \"explain_model\": str(explain_model).lower(),\n        \"session_id\": str(session_id).lower(),\n        \"xai\": xai,\n    }\n    headers = {\"x-api-token\": token}\n    form_data = {k: v for k, v in form_data.items() if v is not None}\n    files = {}\n    if file_path:\n        files[\"in_file\"] = open(file_path, \"rb\")\n    # response = requests.post(\n    #     self.env.get_base_url() + \"/\" + UPLOAD_DATA_PROJECT_URI,\n    #     data=form_data,\n    #     files=files if files else None,\n    #     headers=headers\n    # ).json()\n\n    with httpx.Client(http2=True, timeout=None) as client:\n        response = client.post(\n            self.env.get_base_url() + \"/\" + UPLOAD_DATA_PROJECT_URI,\n            data=form_data,\n            files=files or None,\n            headers=headers,\n        )\n        response.raise_for_status()\n        response = response.json()\n\n    if files:\n        files[\"in_file\"].close()\n    return response\n</code></pre>"},{"location":"sdk/#working-with-organizations","title":"Working With Organizations","text":"<p>The recommended pattern is:</p> <pre><code>organization = lexsi.organization(\"Your Organization name\")\n</code></pre> <p> You can use the following function with organization class :</p>"},{"location":"sdk/#lexsi_sdk.core.organization.Organization","title":"Organization","text":"<pre><code>Organization(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Represents a Lexsi organization. Provides APIs to manage workspaces, users, data connectors, and organization-scoped resources.</p> <p>Attach API client to the organization instance. Stores configuration and prepares the object for use.</p> Source code in <code>lexsi_sdk/core/organization.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Attach API client to the organization instance.\n    Stores configuration and prepares the object for use.\"\"\"\n    super().__init__(**kwargs)\n    self.api_client = kwargs.get(\"api_client\")\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.organization.Organization.add_user_to_organization","title":"add_user_to_organization","text":"<pre><code>add_user_to_organization(user_email)\n</code></pre> <p>Invite a user to join the organization by sending an invitation email. Requires the user\u2019s email address and uses the organization ID internally to associate the user.</p> <p>Parameters:</p> Name Type Description Default <code>user_email</code> <code>str</code> <p>Email of user to be added to organization.</p> required <p>Returns:</p> Type Description <code>str</code> <p>response</p> Source code in <code>lexsi_sdk/core/organization.py</code> <pre><code>def add_user_to_organization(self, user_email: str) -&gt; str:\n    \"\"\"Invite a user to join the organization by sending an invitation email. Requires the user\u2019s email address and uses the organization ID internally to associate the user.\n\n    :param user_email: Email of user to be added to organization.\n    :return: response\n    \"\"\"\n    payload = {\n        \"email\": user_email,\n        \"organization_id\": self.organization_id,\n    }\n    res = self.api_client.post(INVITE_USER_ORGANIZATION_URI, payload)\n\n    if not res[\"success\"]:\n        raise Exception(res.get(\"details\", \"Failed to add user to organization\"))\n\n    return res.get(\"details\", \"User added successfully\")\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.organization.Organization.create_data_connectors","title":"create_data_connectors","text":"<pre><code>create_data_connectors(\n    data_connector_name,\n    data_connector_type,\n    gcs_config=None,\n    s3_config=None,\n    gdrive_config=None,\n    sftp_config=None,\n    hf_token=None,\n)\n</code></pre> <p>Create a data connector for a organization, allowing external data (e.g., S3, GCS, Google Drive, SFTP, Dropbox, HuggingFace) to be linked. Requires the connector name and type, plus the corresponding credential dictionary depending on the connector type. For Dropbox, an authentication link will be generated during execution, and user authorization code is required to complete setup.</p> <p>Parameters:</p> Name Type Description Default <code>data_connector_name</code> <code>str</code> <p>name for data connector</p> required <code>data_connector_type</code> <code>str</code> <p>type of data connector (s3 | gcs | gdrive | dropbox | sftp | HuggingFace)</p> required <code>gcs_config</code> <code>Optional[GCSConfig]</code> <p>credentials from service account json</p> <code>None</code> <code>s3_config</code> <code>Optional[S3Config]</code> <p>credentials of s3 storage</p> <code>None</code> <code>gdrive_config</code> <code>Optional[GDriveConfig]</code> <p>credentials from service account json</p> <code>None</code> <code>sftp_config</code> <code>Optional[SFTPConfig]</code> <p>hostname, port, username and password for sftp connection</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>response</p> Source code in <code>lexsi_sdk/core/organization.py</code> <pre><code>def create_data_connectors(\n    self,\n    data_connector_name: str,\n    data_connector_type: str,\n    gcs_config: Optional[GCSConfig] = None,\n    s3_config: Optional[S3Config] = None,\n    gdrive_config: Optional[GDriveConfig] = None,\n    sftp_config: Optional[SFTPConfig] = None,\n    hf_token: Optional[str] = None,\n) -&gt; str:\n    \"\"\"Create a data connector for a organization, allowing external data (e.g., S3, GCS, Google Drive, SFTP, Dropbox, HuggingFace) to be linked. Requires the connector name and type, plus the corresponding credential dictionary depending on the connector type.\n    For Dropbox, an authentication link will be generated during execution, and user authorization code is required to complete setup.\n\n    :param data_connector_name: name for data connector\n    :param data_connector_type: type of data connector (s3 | gcs | gdrive | dropbox | sftp | HuggingFace)\n    :param gcs_config: credentials from service account json\n    :param s3_config: credentials of s3 storage\n    :param gdrive_config: credentials from service account json\n    :param sftp_config: hostname, port, username and password for sftp connection\n    :return: response\n    \"\"\"\n    if not self.organization_id:\n        return \"No Organization id found\"\n    if data_connector_type.lower() == \"s3\":\n        if not s3_config:\n            return \"No configuration for S3 found\"\n\n        Validate.value_against_list(\n            \"s3 config\",\n            list(s3_config.keys()),\n            [\"region\", \"access_key\", \"secret_key\"],\n        )\n\n        payload = {\n            \"link_service\": {\n                \"service_name\": data_connector_name,\n                \"region\": s3_config.get(\"region\", \"ap-south-1\"),\n                \"access_key\": s3_config.get(\"access_key\"),\n                \"secret_key\": s3_config.get(\"secret_key\"),\n            },\n            \"link_service_type\": data_connector_type,\n        }\n\n    if data_connector_type.lower() == \"gcs\":\n        if not gcs_config:\n            return \"No configuration for GCS found\"\n\n        Validate.value_against_list(\n            \"gcs config\",\n            list(gcs_config.keys()),\n            [\n                \"project_id\",\n                \"gcp_project_name\",\n                \"type\",\n                \"private_key_id\",\n                \"private_key\",\n                \"client_email\",\n                \"client_id\",\n                \"auth_uri\",\n                \"token_uri\",\n            ],\n        )\n\n        payload = {\n            \"link_service\": {\n                \"service_name\": data_connector_name,\n                \"project_id\": gcs_config.get(\"project_id\"),\n                \"gcp_project_name\": gcs_config.get(\"gcp_project_name\"),\n                \"service_account_json\": {\n                    \"type\": gcs_config.get(\"type\"),\n                    \"project_id\": gcs_config.get(\"project_id\"),\n                    \"private_key_id\": gcs_config.get(\"private_key_id\"),\n                    \"private_key\": gcs_config.get(\"private_key\"),\n                    \"client_email\": gcs_config.get(\"client_email\"),\n                    \"client_id\": gcs_config.get(\"client_id\"),\n                    \"auth_uri\": gcs_config.get(\"auth_uri\"),\n                    \"token_uri\": gcs_config.get(\"token_uri\"),\n                },\n            },\n            \"link_service_type\": data_connector_type,\n        }\n\n    if data_connector_type == \"gdrive\":\n        if not gdrive_config:\n            return \"No configuration for Google Drive found\"\n\n        Validate.value_against_list(\n            \"gdrive config\",\n            list(gdrive_config.keys()),\n            [\n                \"project_id\",\n                \"type\",\n                \"private_key_id\",\n                \"private_key\",\n                \"client_email\",\n                \"client_id\",\n                \"auth_uri\",\n                \"token_uri\",\n            ],\n        )\n\n        payload = {\n            \"link_service\": {\n                \"service_name\": data_connector_name,\n                \"service_account_json\": {\n                    \"type\": gdrive_config.get(\"type\"),\n                    \"project_id\": gdrive_config.get(\"project_id\"),\n                    \"private_key_id\": gdrive_config.get(\"private_key_id\"),\n                    \"private_key\": gdrive_config.get(\"private_key\"),\n                    \"client_email\": gdrive_config.get(\"client_email\"),\n                    \"client_id\": gdrive_config.get(\"client_id\"),\n                    \"auth_uri\": gdrive_config.get(\"auth_uri\"),\n                    \"token_uri\": gdrive_config.get(\"token_uri\"),\n                },\n            },\n            \"link_service_type\": data_connector_type,\n        }\n\n    if data_connector_type == \"sftp\":\n        if not sftp_config:\n            return \"No configuration for Google Drive found\"\n\n        Validate.value_against_list(\n            \"sftp config\",\n            list(sftp_config.keys()),\n            [\"hostname\", \"port\", \"username\", \"password\"],\n        )\n\n        payload = {\n            \"link_service\": {\n                \"service_name\": data_connector_name,\n                \"sftp_json\": {\n                    \"hostname\": sftp_config.get(\"hostname\"),\n                    \"port\": sftp_config.get(\"port\"),\n                    \"username\": sftp_config.get(\"username\"),\n                    \"password\": sftp_config.get(\"password\"),\n                },\n            },\n            \"link_service_type\": data_connector_type,\n        }\n\n    if data_connector_type == \"dropbox\":\n        url_data = self.api_client.get(\n            f\"{DROPBOX_OAUTH}?organization_id={self.organization_id}\"\n        )\n        print(f\"Url: {url_data['details']['url']}\")\n        code = input(f\"{url_data['details']['message']}: \")\n\n        if not code:\n            return \"No authentication code provided.\"\n\n        payload = {\n            \"link_service\": {\n                \"service_name\": data_connector_name,\n                \"dropbox_json\": {\"code\": code},\n            },\n            \"link_service_type\": data_connector_type,\n        }\n\n    if data_connector_type == \"HuggingFace\":\n        if not hf_token:\n            return \"No hf_token provided\"\n\n        payload = {\n            \"link_service\": {\n                \"service_name\": data_connector_name,\n                \"hf_token\": hf_token,\n            },\n            \"link_service_type\": data_connector_type,\n        }\n\n    url = build_url(\n        CREATE_DATA_CONNECTORS, data_connector_name, None, self.organization_id\n    )\n    res = self.api_client.post(url, payload)\n    return res[\"details\"]\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.organization.Organization.create_workspace","title":"create_workspace","text":"<pre><code>create_workspace(workspace_name, server_type=None)\n</code></pre> <p>Create a new workspace within the organization. Accepts a workspace name and an optional server_type to specify the compute instance. Returns a Workspace object for the newly created workspace.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_name</code> <code>str</code> <p>name for the workspace</p> required <code>server_type</code> <code>Optional[str]</code> <p>dedicated instance to run workloads for all available instances check lexsi.available_node_servers() defaults to shared</p> <code>None</code> <p>Returns:</p> Type Description <code>Workspace</code> <p>response</p> Source code in <code>lexsi_sdk/core/organization.py</code> <pre><code>def create_workspace(\n    self, workspace_name: str, server_type: Optional[str] = None\n) -&gt; Workspace:\n    \"\"\"Create a new workspace within the organization. Accepts a workspace name and an optional server_type to specify the compute instance. Returns a Workspace object for the newly created workspace.\n\n    :param workspace_name: name for the workspace\n    :param server_type: dedicated instance to run workloads\n        for all available instances check lexsi.available_node_servers()\n        defaults to shared\n    :return: response\n    \"\"\"\n    payload = {\"workspace_name\": workspace_name}\n\n    if server_type:\n        custom_servers = self.api_client.get(AVAILABLE_CUSTOM_SERVERS_URI)\n        Validate.value_against_list(\n            \"server_type\",\n            server_type,\n            [server[\"name\"] for server in custom_servers],\n        )\n\n        payload[\"instance_type\"] = server_type\n        payload[\"server_config\"] = {}\n\n    if self.organization_id:\n        payload[\"organization_id\"] = self.organization_id\n\n    res = self.api_client.post(CREATE_WORKSPACE_URI, payload)\n\n    if not res[\"success\"]:\n        raise Exception(res.get(\"details\"))\n\n    workspace = Workspace(api_client=self.api_client, **res[\"workspace_details\"])\n\n    return workspace\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.organization.Organization.credits","title":"credits","text":"<pre><code>credits()\n</code></pre> <p>Return credit usage and quota information for the organization.</p> Source code in <code>lexsi_sdk/core/organization.py</code> <pre><code>def credits(self):\n    \"\"\"Return credit usage and quota information for the organization.\"\"\"\n    url = build_list_data_connector_url(\n        COMPUTE_CREDIT_URI, None, self.organization_id\n    )\n    res = self.api_client.get(url)\n    return res[\"details\"]\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.organization.Organization.delete_data_connectors","title":"delete_data_connectors","text":"<pre><code>delete_data_connectors(data_connector_name)\n</code></pre> <p>Delete a data connector from the organization using its name. This removes the external data link and returns a confirmation message.</p> <p>Parameters:</p> Name Type Description Default <code>data_connector_name</code> <code>str</code> <p>name of the data connector to be deleted.</p> required Source code in <code>lexsi_sdk/core/organization.py</code> <pre><code>def delete_data_connectors(self, data_connector_name: str) -&gt; str:\n    \"\"\"Delete a data connector from the organization using its name. This removes the external data link and returns a confirmation message.\n\n    :param data_connector_name: name of the data connector to be deleted.\n    \"\"\"\n    if not data_connector_name:\n        return \"Missing argument data_connector_name\"\n    if not self.organization_id:\n        return \"No Project Name or Organization id found\"\n\n    url = build_url(\n        DELETE_DATA_CONNECTORS, data_connector_name, None, self.organization_id\n    )\n    res = self.api_client.post(url)\n    return res[\"details\"]\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.organization.Organization.list_data_connectors","title":"list_data_connectors","text":"<pre><code>list_data_connectors()\n</code></pre> <p>List all data connectors configured in the organization. If successful, returns a DataFrame with details about each connector; otherwise returns an error message.</p> Source code in <code>lexsi_sdk/core/organization.py</code> <pre><code>def list_data_connectors(self) -&gt; str | pd.DataFrame:\n    \"\"\"List all data connectors configured in the organization. If successful, returns a DataFrame with details about each connector; otherwise returns an error message.\"\"\"\n    url = build_list_data_connector_url(\n        LIST_DATA_CONNECTORS, None, self.organization_id\n    )\n    res = self.api_client.post(url)\n\n    if res[\"success\"]:\n        df = pd.DataFrame(res[\"details\"])\n        df = df.drop(\n            [\n                \"_id\",\n                \"region\",\n                \"gcp_project_name\",\n                \"gcp_project_id\",\n                \"gdrive_file_name\",\n                \"project_name\",\n            ],\n            axis=1,\n            errors=\"ignore\",\n        )\n        return df\n\n    return res[\"details\"]\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.organization.Organization.list_data_connectors_buckets","title":"list_data_connectors_buckets","text":"<pre><code>list_data_connectors_buckets(data_connector_name)\n</code></pre> <p>Retrieve the list of buckets (for S3 or GCS connectors) or similar container names for the specified data connector.</p> <p>Parameters:</p> Name Type Description Default <code>data_connector_name</code> <code>str</code> <p>name of the data connector</p> required Source code in <code>lexsi_sdk/core/organization.py</code> <pre><code>def list_data_connectors_buckets(self, data_connector_name: str) -&gt; str | List:\n    \"\"\"Retrieve the list of buckets (for S3 or GCS connectors) or similar container names for the specified data connector.\n\n    :param data_connector_name: name of the data connector\n    \"\"\"\n    if not data_connector_name:\n        return \"Missing argument data_connector_name\"\n    if not self.organization_id:\n        return \"No Organization id found\"\n\n    url = build_url(LIST_BUCKETS, data_connector_name, None, self.organization_id)\n    res = self.api_client.get(url)\n\n    if res.get(\"message\", None):\n        print(res[\"message\"])\n    return res[\"details\"]\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.organization.Organization.list_data_connectors_filepath","title":"list_data_connectors_filepath","text":"<pre><code>list_data_connectors_filepath(\n    data_connector_name, bucket_name=None, root_folder=None\n)\n</code></pre> <p>List file paths within the specified data connector. For S3/GCS connectors you may need to provide a bucket_name; for SFTP connectors you may need to provide a root_folder.</p> <p>Parameters:</p> Name Type Description Default <code>data_connector_name</code> <code>str</code> <p>name of the data connector</p> required <code>bucket_name</code> <code>Optional[str]</code> <p>Required for S3 &amp; GCS</p> <code>None</code> <code>root_folder</code> <code>Optional[str]</code> <p>Root folder of SFTP</p> <code>None</code> Source code in <code>lexsi_sdk/core/organization.py</code> <pre><code>def list_data_connectors_filepath(\n    self,\n    data_connector_name: str,\n    bucket_name: Optional[str] = None,\n    root_folder: Optional[str] = None,\n) -&gt; str | Dict:\n    \"\"\"List file paths within the specified data connector. For S3/GCS connectors you may need to provide a bucket_name; for SFTP connectors you may need to provide a root_folder.\n\n    :param data_connector_name: name of the data connector\n    :param bucket_name: Required for S3 &amp; GCS\n    :param root_folder: Root folder of SFTP\n    \"\"\"\n    if not data_connector_name:\n        return \"Missing argument data_connector_name\"\n    if not self.organization_id:\n        return \"No Organization id found\"\n\n    def get_connector() -&gt; str | pd.DataFrame:\n        \"\"\"Retrieve connector metadata for the given link service name.\n        Reads from internal state or a backend client as needed.\"\"\"\n        url = build_list_data_connector_url(\n            LIST_DATA_CONNECTORS, None, self.organization_id\n        )\n        res = self.api_client.post(url)\n\n        if res[\"success\"]:\n            df = pd.DataFrame(res[\"details\"])\n            filtered_df = df.loc[df[\"link_service_name\"] == data_connector_name]\n            if filtered_df.empty:\n                return \"No data connector found\"\n            return filtered_df\n\n        return res[\"details\"]\n\n    connectors = get_connector()\n    if isinstance(connectors, pd.DataFrame):\n        value = connectors.loc[\n            connectors[\"link_service_name\"] == data_connector_name,\n            \"link_service_type\",\n        ].values[0]\n        ds_type = value\n\n        if ds_type == \"s3\" or ds_type == \"gcs\":\n            if not bucket_name:\n                return \"Missing argument bucket_name\"\n\n        if ds_type == \"sftp\":\n            if not root_folder:\n                return \"Missing argument root_folder\"\n\n    if self.organization_id:\n        url = f\"{LIST_FILEPATHS}?organization_id={self.organization_id}&amp;link_service_name={data_connector_name}&amp;bucket_name={bucket_name}&amp;root_folder={root_folder}\"\n    res = self.api_client.get(url)\n\n    if res.get(\"message\", None):\n        print(res[\"message\"])\n    return res[\"details\"]\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.organization.Organization.member_details","title":"member_details","text":"<pre><code>member_details()\n</code></pre> <p>Return a DataFrame containing details about members of the organization, including their names, emails, roles (owner/admin), and creation dates.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>member details dataframe</p> Source code in <code>lexsi_sdk/core/organization.py</code> <pre><code>def member_details(self) -&gt; pd.DataFrame:\n    \"\"\"Return a DataFrame containing details about members of the organization, including their names, emails, roles (owner/admin), and creation dates.\n\n    :return: member details dataframe\n    \"\"\"\n    res = self.api_client.get(\n        f\"{ORGANIZATION_MEMBERS_URI}?organization_id={self.organization_id}\"\n    )\n\n    if not res[\"success\"]:\n        raise Exception(\n            res.get(\"details\", \"Failed to get organization member details\")\n        )\n\n    member_details_df = pd.DataFrame(\n        res.get(\"details\").get(\"users\"),\n        columns=[\n            \"full_name\",\n            \"email\",\n            \"organization_owner\",\n            \"organization_admin\",\n            \"created_at\",\n        ],\n    )\n\n    return member_details_df\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.organization.Organization.remove_user_from_organization","title":"remove_user_from_organization","text":"<pre><code>remove_user_from_organization(user_email)\n</code></pre> <p>Remove an existing user from the organization using their email address. Returns a confirmation message on success.</p> <p>Parameters:</p> Name Type Description Default <code>user_email</code> <code>str</code> <p>Email of user to be removed from organization.</p> required <p>Returns:</p> Type Description <code>str</code> <p>response</p> Source code in <code>lexsi_sdk/core/organization.py</code> <pre><code>def remove_user_from_organization(self, user_email: str) -&gt; str:\n    \"\"\"Remove an existing user from the organization using their email address. Returns a confirmation message on success.\n\n    :param user_email: Email of user to be removed from organization.\n    :return: response\n    \"\"\"\n    payload = {\n        \"organization_user_email\": user_email,\n        \"organization_id\": self.organization_id,\n    }\n    res = self.api_client.post(REMOVE_USER_ORGANIZATION_URI, payload)\n\n    if not res[\"success\"]:\n        raise Exception(\n            res.get(\"details\", \"Failed to remove user from organization\")\n        )\n\n    return res.get(\"details\", \"User removed successfully\")\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.organization.Organization.test_data_connectors","title":"test_data_connectors","text":"<pre><code>test_data_connectors(data_connector_name)\n</code></pre> <p>Test the connection of an existing data connector to ensure credentials and connectivity are valid. Takes the connector name as input and returns the status of the connection test.</p> <p>Parameters:</p> Name Type Description Default <code>data_connector_name</code> <code>str</code> <p>name of the data connector to be tested.</p> required Source code in <code>lexsi_sdk/core/organization.py</code> <pre><code>def test_data_connectors(self, data_connector_name: str) -&gt; str:\n    \"\"\"Test the connection of an existing data connector to ensure credentials and connectivity are valid. Takes the connector name as input and returns the status of the connection test.\n\n    :param data_connector_name: name of the data connector to be tested.\n    \"\"\"\n    if not data_connector_name:\n        return \"Missing argument data_connector_name\"\n    if not self.organization_id:\n        return \"No Project Name or Organization id found\"\n    url = build_url(\n        TEST_DATA_CONNECTORS, data_connector_name, None, self.organization_id\n    )\n    res = self.api_client.post(url)\n    return res[\"details\"]\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.organization.Organization.update_user_access_for_organization","title":"update_user_access_for_organization","text":"<pre><code>update_user_access_for_organization(\n    user_email, access_type\n)\n</code></pre> <p>Change the role of a user within the organization. Accepts the user\u2019s email and the new access type (admin or user) and returns a confirmation message.</p> <p>Parameters:</p> Name Type Description Default <code>user_email</code> <code>str</code> <p>Email of user to be added to project.</p> required <code>access_type</code> <code>Literal['admin', 'user']</code> <p>access type to be given to user (admin | user)</p> required <p>Returns:</p> Type Description <code>str</code> <p>response</p> Source code in <code>lexsi_sdk/core/organization.py</code> <pre><code>def update_user_access_for_organization(\n    self,\n    user_email: str,\n    access_type: Literal[\"admin\", \"user\"],\n) -&gt; str:\n    \"\"\"Change the role of a user within the organization. Accepts the user\u2019s email and the new access type (admin or user) and returns a confirmation message.\n\n    :param user_email: Email of user to be added to project.\n    :param access_type: access type to be given to user (admin | user)\n    :return: response\n    \"\"\"\n    if access_type not in [\"admin\", \"user\"]:\n        raise ValueError(\"access_type must be either 'admin' or 'user'\")\n    payload = {\n        \"organization_user_email\": user_email,\n        \"organization_id\": self.organization_id,\n        \"organization_admin\": True if access_type == \"admin\" else False,\n    }\n    res = self.api_client.post(UPDATE_ORGANIZATION_URI, payload)\n\n    if not res[\"success\"]:\n        raise Exception(res.get(\"details\", \"Failed to update user access\"))\n\n    return res.get(\"details\", \"User access updated successfully\")\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.organization.Organization.workspace","title":"workspace","text":"<pre><code>workspace(workspace_name)\n</code></pre> <p>Select a specific workspace by name within the organization and return a Workspace object for further operations.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_name</code> <code>str</code> <p>Name of the workspace to be used</p> required <p>Returns:</p> Type Description <code>Workspace</code> <p>Workspace</p> Source code in <code>lexsi_sdk/core/organization.py</code> <pre><code>def workspace(self, workspace_name: str) -&gt; Workspace:\n    \"\"\"Select a specific workspace by name within the organization and return a Workspace object for further operations.\n\n    :param workspace_name: Name of the workspace to be used\n    :return: Workspace\n    \"\"\"\n\n    url = GET_WORKSPACES_URI\n    if self.organization_id:\n        url = url + f\"?organization_id={self.organization_id}\"\n    workspaces = self.api_client.get(url)\n    user_workspaces = [\n        Workspace(api_client=self.api_client, **workspace)\n        for workspace in workspaces[\"details\"]\n    ]\n\n    workspace = next(\n        filter(\n            lambda workspace: workspace.user_workspace_name == workspace_name,\n            user_workspaces,\n        ),\n        None,\n    )\n\n    if workspace is None:\n        raise Exception(\"Workspace Not Found\")\n\n    return workspace\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.organization.Organization.workspaces","title":"workspaces","text":"<pre><code>workspaces()\n</code></pre> <p>List all workspaces associated with the organization. Returns a DataFrame with workspace names, access types, creator, and instance details.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>workspace details dataframe</p> Source code in <code>lexsi_sdk/core/organization.py</code> <pre><code>def workspaces(self) -&gt; pd.DataFrame:\n    \"\"\"List all workspaces associated with the organization. Returns a DataFrame with workspace names, access types, creator, and instance details.\n\n    :return: workspace details dataframe\n    \"\"\"\n\n    url = GET_WORKSPACES_URI\n    if self.organization_id:\n        url = url + f\"?organization_id={self.organization_id}\"\n    workspaces = self.api_client.get(url)\n\n    workspace_df = pd.DataFrame(\n        workspaces[\"details\"],\n        columns=[\n            \"user_workspace_name\",\n            \"access_type\",\n            \"created_by\",\n            \"created_at\",\n            \"updated_at\",\n            \"instance_type\",\n            \"instance_status\",\n        ],\n    )\n\n    return workspace_df\n</code></pre>"},{"location":"sdk/#working-with-workspaces","title":"Working With Workspaces","text":"<p>The recommended pattern is:</p> <pre><code>workspace = organization.workspace(\"Your workspace name\")\n</code></pre> <p> You can use the following function with workspace class :</p>"},{"location":"sdk/#lexsi_sdk.core.workspace.Workspace","title":"Workspace","text":"<pre><code>Workspace(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Logical container inside an organization that groups projects, users, and compute resources. Supports workspace-level user access and project lifecycle management.</p> <p>Attach API client for workspace operations. Stores configuration and prepares the object for use.</p> Source code in <code>lexsi_sdk/core/workspace.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Attach API client for workspace operations.\n    Stores configuration and prepares the object for use.\"\"\"\n    super().__init__(**kwargs)\n    self.api_client = kwargs.get(\"api_client\")\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.workspace.Workspace.add_user_to_workspace","title":"add_user_to_workspace","text":"<pre><code>add_user_to_workspace(email, role)\n</code></pre> <p>Add a user to the workspace with a specified role. Valid roles include admin, manager, or user.</p> <p>Parameters:</p> Name Type Description Default <code>email</code> <code>str</code> <p>user email</p> required <code>role</code> <code>str</code> <p>user role [\"admin\", \"manager\", \"user\"]</p> required <p>Returns:</p> Type Description <code>str</code> <p>response</p> Source code in <code>lexsi_sdk/core/workspace.py</code> <pre><code>def add_user_to_workspace(self, email: str, role: str) -&gt; str:\n    \"\"\"Add a user to the workspace with a specified role. Valid roles include admin, manager, or user.\n\n    :param email: user email\n    :param role: user role [\"admin\", \"manager\", \"user\"]\n    :return: response\n    \"\"\"\n    payload = {\n        \"workspace_name\": self.workspace_name,\n        \"modify_req\": {\n            \"add_user_workspace\": {\n                \"email\": email,\n                \"role\": role,\n            },\n        },\n    }\n    res = self.api_client.post(UPDATE_WORKSPACE_URI, payload)\n    return res.get(\"details\")\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.workspace.Workspace.clear_notifications","title":"clear_notifications","text":"<pre><code>clear_notifications()\n</code></pre> <p>Clear all notifications for the workspace. Sends a POST request and returns a confirmation message.</p> <p>Returns:</p> Type Description <code>str</code> <p>str</p> <p>Raises:</p> Type Description <code>Exception</code> <p>description</p> Source code in <code>lexsi_sdk/core/workspace.py</code> <pre><code>def clear_notifications(self) -&gt; str:\n    \"\"\"Clear all notifications for the workspace. Sends a POST request and returns a confirmation message.\n\n    :raises Exception: _description_\n    :return: str\n    \"\"\"\n    url = f\"{CLEAR_NOTIFICATIONS_URI}?workspace_name={self.workspace_name}\"\n\n    res = self.api_client.post(url)\n\n    if not res[\"success\"]:\n        raise Exception(\"Error while clearing workspace notifications.\")\n\n    return res[\"details\"]\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.workspace.Workspace.create_project","title":"create_project","text":"<pre><code>create_project(\n    project_name, modality, project_type, server_type=None\n)\n</code></pre> <p>Create a new project within the workspace. Requires project_name, modality (e.g., tabular, text, image), project_type (e.g., classification), and optional project_sub_type and server_type. Returns the created Project object.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>name for the project</p> required <code>modality</code> <code>str</code> <p>modality for the project Eg:- tabular, image, text</p> required <code>project_type</code> <code>str</code> <p>type for the project Eg:- classification, regression</p> required <code>server_type</code> <code>Optional[str]</code> <p>dedicated node to run workloads for all available nodes check lexsi.available_node_servers()</p> <code>None</code> <p>Returns:</p> Type Description <code>Project</code> <p>response</p> Source code in <code>lexsi_sdk/core/workspace.py</code> <pre><code>def create_project(\n    self,\n    project_name: str,\n    modality: str,\n    project_type: str,\n    server_type: Optional[str] = None,\n) -&gt; Project:\n    \"\"\"Create a new project within the workspace. Requires project_name, modality (e.g., tabular, text, image), project_type (e.g., classification), and optional project_sub_type and server_type. Returns the created Project object.\n\n    :param project_name: name for the project\n    :param modality: modality for the project\n        Eg:- tabular, image, text\n    :param project_type: type for the project\n        Eg:- classification, regression\n    :param server_type: dedicated node to run workloads\n        for all available nodes check lexsi.available_node_servers()\n    :return: response\n    \"\"\"\n    payload = {\n        \"project_name\": project_name,\n        \"modality\": modality,\n        \"project_type\": project_type,\n        \"workspace_name\": self.workspace_name,\n    }\n\n    if self.organization_id:\n        payload[\"organization_id\"] = self.organization_id\n\n    if server_type:\n        custom_servers = self.api_client.get(AVAILABLE_CUSTOM_SERVERS_URI)\n        Validate.value_against_list(\n            \"server_type\",\n            server_type,\n            [server[\"name\"] for server in custom_servers],\n        )\n\n        payload[\"instance_type\"] = server_type\n        payload[\"server_config\"] = {}\n\n    res = self.api_client.post(CREATE_PROJECT_URI, payload)\n\n    if not res[\"success\"]:\n        raise Exception(res[\"details\"])\n\n    if modality == \"text\":\n        project = TextProject(api_client=self.api_client, **res[\"details\"])\n    elif modality == \"agent\":\n        project = AgentProject(api_client=self.api_client, **res[\"details\"])\n    else:\n        project = Project(api_client=self.api_client, **res[\"details\"])\n\n    return project\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.workspace.Workspace.delete_workspace","title":"delete_workspace","text":"<pre><code>delete_workspace()\n</code></pre> <p>Delete the current workspace by sending a delete request. Returns a confirmation message upon success.</p> <p>Returns:</p> Type Description <code>str</code> <p>response</p> Source code in <code>lexsi_sdk/core/workspace.py</code> <pre><code>def delete_workspace(self) -&gt; str:\n    \"\"\"Delete the current workspace by sending a delete request. Returns a confirmation message upon success.\n    :return: response\n    \"\"\"\n    payload = {\n        \"workspace_name\": self.workspace_name,\n        \"modify_req\": {\"delete_workspace\": self.user_workspace_name},\n    }\n    res = self.api_client.post(UPDATE_WORKSPACE_URI, payload)\n    return res.get(\"details\")\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.workspace.Workspace.get_notifications","title":"get_notifications","text":"<pre><code>get_notifications()\n</code></pre> <p>Get notifications specific to the workspace. Returns a DataFrame listing notifications including the project name, message, and timestamp.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame</p> Source code in <code>lexsi_sdk/core/workspace.py</code> <pre><code>def get_notifications(self) -&gt; pd.DataFrame:\n    \"\"\"Get notifications specific to the workspace. Returns a DataFrame listing notifications including the project name, message, and timestamp.\n\n    :return: DataFrame\n    \"\"\"\n    url = f\"{GET_NOTIFICATIONS_URI}?workspace_name={self.workspace_name}\"\n\n    res = self.api_client.get(url)\n\n    if not res[\"success\"]:\n        raise Exception(\"Error while getting workspace notifications.\")\n\n    notifications = res[\"details\"]\n\n    if not notifications:\n        return \"No notifications found.\"\n\n    return pd.DataFrame(notifications).reindex(\n        columns=[\"project_name\", \"message\", \"time\"]\n    )\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.workspace.Workspace.project","title":"project","text":"<pre><code>project(project_name)\n</code></pre> <p>Select a specific project by name from the workspace. Returns a Project object (or a subclass like TextProject or AgentProject) for the chosen project.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>Name of the project</p> required <p>Returns:</p> Type Description <code>Project</code> <p>Project</p> Source code in <code>lexsi_sdk/core/workspace.py</code> <pre><code>def project(self, project_name: str) -&gt; Project:\n    \"\"\"Select a specific project by name from the workspace. Returns a Project object (or a subclass like TextProject or AgentProject) for the chosen project.\n\n    :param project_name: Name of the project\n    :return: Project\n    \"\"\"\n    workspace = self.api_client.get(\n        f\"{GET_WORKSPACES_DETAILS_URI}?workspace_name={self.workspace_name}\"\n    )\n\n    project = next(\n        filter(\n            lambda project: project.get(\"user_project_name\") == project_name,\n            workspace.get(\"data\", {}).get(\"projects\", []),\n        ),\n        None,\n    )\n\n    if project is None:\n        raise Exception(\"Project Not Found\")\n    if project.get(\"metadata\", {}).get(\"modality\") == \"tabular\":\n        return TabularProject(api_client=self.api_client, **project)\n    elif project.get(\"metadata\", {}).get(\"modality\") == \"image\":\n        return ImageProject(api_client=self.api_client, **project)\n    elif project.get(\"metadata\", {}).get(\"modality\") == \"text\":\n        return TextProject(api_client=self.api_client, **project)\n    elif project.get(\"metadata\", {}).get(\"modality\") == \"agent\":\n        return AgentProject(api_client=self.api_client, **project)\n\n    return Project(api_client=self.api_client, **project)\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.workspace.Workspace.projects","title":"projects","text":"<pre><code>projects()\n</code></pre> <p>Retrieve a DataFrame listing all projects in the workspace, with details like project name, access type, creator, and instance type.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Projects details dataframe</p> Source code in <code>lexsi_sdk/core/workspace.py</code> <pre><code>def projects(self) -&gt; pd.DataFrame:\n    \"\"\"Retrieve a DataFrame listing all projects in the workspace, with details like project name, access type, creator, and instance type.\n\n    :return: Projects details dataframe\n    \"\"\"\n    workspace = self.api_client.get(\n        f\"{GET_WORKSPACES_DETAILS_URI}?workspace_name={self.workspace_name}\"\n    )\n    projects_df = pd.DataFrame(\n        workspace.get(\"data\", {}).get(\"projects\", []),\n        columns=[\n            \"user_project_name\",\n            \"access_type\",\n            \"created_by\",\n            \"created_at\",\n            \"updated_at\",\n            \"instance_type\",\n            \"instance_status\",\n        ],\n    )\n    return projects_df\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.workspace.Workspace.remove_user_from_workspace","title":"remove_user_from_workspace","text":"<pre><code>remove_user_from_workspace(email)\n</code></pre> <p>Remove a user from the workspace using their email address. Returns a response message.</p> <p>Parameters:</p> Name Type Description Default <code>email</code> <code>str</code> <p>user email</p> required <p>Returns:</p> Type Description <code>str</code> <p>response</p> Source code in <code>lexsi_sdk/core/workspace.py</code> <pre><code>def remove_user_from_workspace(self, email: str) -&gt; str:\n    \"\"\"Remove a user from the workspace using their email address. Returns a response message.\n\n    :param email: user email\n    :return: response\n    \"\"\"\n    payload = {\n        \"workspace_name\": self.workspace_name,\n        \"modify_req\": {\n            \"remove_user_workspace\": email,\n        },\n    }\n    res = self.api_client.post(UPDATE_WORKSPACE_URI, payload)\n    return res.get(\"details\")\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.workspace.Workspace.rename_workspace","title":"rename_workspace","text":"<pre><code>rename_workspace(new_workspace_name)\n</code></pre> <p>Rename the current workspace to a new name by sending an update request to the API. Updates internal properties and returns the response message.</p> <p>Parameters:</p> Name Type Description Default <code>new_workspace_name</code> <code>str</code> <p>name for the workspace to be renamed to</p> required <p>Returns:</p> Type Description <code>str</code> <p>response</p> Source code in <code>lexsi_sdk/core/workspace.py</code> <pre><code>def rename_workspace(self, new_workspace_name: str) -&gt; str:\n    \"\"\"Rename the current workspace to a new name by sending an update request to the API. Updates internal properties and returns the response message.\n\n    :param new_workspace_name: name for the workspace to be renamed to\n    :return: response\n    \"\"\"\n    payload = {\n        \"workspace_name\": self.workspace_name,\n        \"modify_req\": {\n            \"update_workspace\": {\n                \"workspace_name\": new_workspace_name,\n            }\n        },\n    }\n    res = self.api_client.post(UPDATE_WORKSPACE_URI, payload)\n    self.user_workspace_name = new_workspace_name\n    return res.get(\"details\")\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.workspace.Workspace.start_server","title":"start_server","text":"<pre><code>start_server()\n</code></pre> <p>Start a dedicated compute server for the workspace, enabling compute-intensive tasks.</p> <p>Returns:</p> Type Description <code>str</code> <p>response</p> Source code in <code>lexsi_sdk/core/workspace.py</code> <pre><code>def start_server(self) -&gt; str:\n    \"\"\"Start a dedicated compute server for the workspace, enabling compute-intensive tasks.\n\n    :return: response\n    \"\"\"\n\n    res = self.api_client.post(\n        f\"{START_CUSTOM_SERVER_URI}?workspace_name={self.workspace_name}\"\n    )\n\n    if not res[\"success\"]:\n        raise Exception(res.get(\"message\"))\n\n    return res[\"message\"]\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.workspace.Workspace.stop_server","title":"stop_server","text":"<pre><code>stop_server()\n</code></pre> <p>Stop the dedicated compute server associated with the workspace.</p> <p>Returns:</p> Type Description <code>str</code> <p>response</p> Source code in <code>lexsi_sdk/core/workspace.py</code> <pre><code>def stop_server(self) -&gt; str:\n    \"\"\"Stop the dedicated compute server associated with the workspace.\n\n    :return: response\n    \"\"\"\n    res = self.api_client.post(\n        f\"{STOP_CUSTOM_SERVER_URI}?workspace_name={self.workspace_name}\"\n    )\n\n    if not res[\"success\"]:\n        raise Exception(res.get(\"message\"))\n\n    return res[\"message\"]\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.workspace.Workspace.update_server","title":"update_server","text":"<pre><code>update_server(server_type)\n</code></pre> <p>Change the compute instance type for the workspace by specifying a new server_type. Valid values depend on available custom servers.</p> <p>Parameters:</p> Name Type Description Default <code>server_type</code> <code>str</code> <p>dedicated instance to run workloads for all available instances check xai.available_custom_servers()</p> required <p>Returns:</p> Type Description <code>str</code> <p>response</p> Source code in <code>lexsi_sdk/core/workspace.py</code> <pre><code>def update_server(self, server_type: str) -&gt; str:\n    \"\"\"Change the compute instance type for the workspace by specifying a new server_type. Valid values depend on available custom servers.\n    :param server_type: dedicated instance to run workloads\n        for all available instances check xai.available_custom_servers()\n\n    :return: response\n    \"\"\"\n    custom_servers = self.api_client.get(AVAILABLE_CUSTOM_SERVERS_URI)\n    Validate.value_against_list(\n        \"server_type\",\n        server_type,\n        [server[\"name\"] for server in custom_servers],\n    )\n\n    payload = {\n        \"workspace_name\": self.workspace_name,\n        \"modify_req\": {\n            \"update_workspace\": {\n                \"workspace_name\": self.user_workspace_name,\n                \"instance_type\": server_type,\n            },\n            \"update_operational_hours\": {},\n        },\n    }\n\n    res = self.api_client.post(UPDATE_WORKSPACE_URI, payload)\n\n    if not res[\"success\"]:\n        raise Exception(res.get(\"details\"))\n\n    return \"Server Updated\"\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.workspace.Workspace.update_user_access_for_workspace","title":"update_user_access_for_workspace","text":"<pre><code>update_user_access_for_workspace(email, role)\n</code></pre> <p>Update the role of a user in the workspace. Accepts the user\u2019s email and the new role (admin or user).</p> <p>Parameters:</p> Name Type Description Default <code>email</code> <code>str</code> <p>user email</p> required <code>role</code> <code>UserRole</code> <p>new user role [\"admin\", \"user\"]</p> required <p>Returns:</p> Type Description <code>str</code> <p>description</p> Source code in <code>lexsi_sdk/core/workspace.py</code> <pre><code>def update_user_access_for_workspace(self, email: str, role: UserRole) -&gt; str:\n    \"\"\"Update the role of a user in the workspace. Accepts the user\u2019s email and the new role (admin or user).\n\n    :param email: user email\n    :param role: new user role [\"admin\", \"user\"]\n    :return: _description_\n    \"\"\"\n    payload = {\n        \"workspace_name\": self.workspace_name,\n        \"modify_req\": {\n            \"update_user_workspace\": {\n                \"email\": email,\n                \"role\": role,\n            }\n        },\n    }\n    res = self.api_client.post(UPDATE_WORKSPACE_URI, payload)\n    return res.get(\"details\")\n</code></pre>"},{"location":"sdk/#working-with-projects","title":"Working With Projects","text":"<p>The recommended pattern is:</p> <pre><code>project = workspace.project(\"Your Project name\")\n</code></pre> <p> You can use the following function with organization class :</p>"},{"location":"sdk/#lexsi_sdk.core.project.Project","title":"Project","text":"<pre><code>Project(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Represents a project within a workspace. Provides APIs for model monitoring, explainability cases, alerts, dashboards, and data uploads.</p> <p>Initialize a <code>Project</code> instance and attach the API client. Populates model fields from <code>kwargs</code> and stores <code>api_client</code> for later requests.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>Project fields used to construct the instance (including <code>api_client</code>).</p> <code>{}</code>"},{"location":"sdk/#lexsi_sdk.core.project.Project.activate_model","title":"activate_model","text":"<pre><code>activate_model(model_name)\n</code></pre> <p>Sets the provided model to active for the project</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>name of the model</p> required <p>Returns:</p> Type Description <code>str</code> <p>response</p>"},{"location":"sdk/#lexsi_sdk.core.project.Project.active_model","title":"active_model","text":"<pre><code>active_model()\n</code></pre> <p>Current Active Model for project</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>current active model dataframe</p>"},{"location":"sdk/#lexsi_sdk.core.project.Project.add_user_to_project","title":"add_user_to_project","text":"<pre><code>add_user_to_project(email, role)\n</code></pre> <p>Add a user to the project with a specified role such as admin, manager, or user.</p> <p>Parameters:</p> Name Type Description Default <code>email</code> <code>str</code> <p>user email</p> required <code>role</code> <code>str</code> <p>user role [\"admin\", \"manager\", \"user\"]</p> required <p>Returns:</p> Type Description <code>str</code> <p>response</p>"},{"location":"sdk/#lexsi_sdk.core.project.Project.all_tags","title":"all_tags","text":"<pre><code>all_tags()\n</code></pre> <p>Available All Tags for Project</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>list of tags</p>"},{"location":"sdk/#lexsi_sdk.core.project.Project.available_tags","title":"available_tags","text":"<pre><code>available_tags()\n</code></pre> <p>Return a list of tags that are available for data categorization within the project.</p> <p>Returns:</p> Type Description <code>str</code> <p>response</p>"},{"location":"sdk/#lexsi_sdk.core.project.Project.case_logs","title":"case_logs","text":"<pre><code>case_logs(page=1)\n</code></pre> <p>Get already viewed case logs</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>Optional[int]</code> <p>page number, defaults to 1</p> <code>1</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Case object with details</p>"},{"location":"sdk/#lexsi_sdk.core.project.Project.case_record","title":"case_record","text":"<pre><code>case_record(case_id)\n</code></pre> <p>Get already viewed case</p> <p>Parameters:</p> Name Type Description Default <code>case_id</code> <code>str</code> <p>case id</p> required <p>Returns:</p> Type Description <p>Case object with details</p>"},{"location":"sdk/#lexsi_sdk.core.project.Project.clear_notifications","title":"clear_notifications","text":"<pre><code>clear_notifications()\n</code></pre> <p>clear user project notifications</p> <p>Returns:</p> Type Description <code>str</code> <p>response</p> <p>Raises:</p> Type Description <code>Exception</code> <p>description</p>"},{"location":"sdk/#lexsi_sdk.core.project.Project.create_data_connectors","title":"create_data_connectors","text":"<pre><code>create_data_connectors(\n    data_connector_name,\n    data_connector_type,\n    gcs_config=None,\n    s3_config=None,\n    gdrive_config=None,\n    sftp_config=None,\n    hf_token=None,\n)\n</code></pre> <p>Create a data connector for a project, allowing external data (e.g., S3, GCS, Google Drive, SFTP, Dropbox, HuggingFace) to be linked. Requires the connector name and type, plus the corresponding credential dictionary depending on the connector type.  For Dropbox, an authentication link will be generated during execution, and user authorization code is required to complete setup.</p> <p>Parameters:</p> Name Type Description Default <code>data_connector_name</code> <code>str</code> <p>name for data connector</p> required <code>data_connector_type</code> <code>str</code> <p>type of data connector (s3 | gcs | gdrive | dropbox | sftp | HuggingFace)</p> required <code>gcs_config</code> <code>Optional[GCSConfig]</code> <p>credentials from service account json</p> <code>None</code> <code>s3_config</code> <code>Optional[S3Config]</code> <p>credentials of s3 storage</p> <code>None</code> <code>gdrive_config</code> <code>Optional[GDriveConfig]</code> <p>credentials from service account json</p> <code>None</code> <code>sftp_config</code> <code>Optional[SFTPConfig]</code> <p>hostname, port, username and password for sftp connection</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>response</p>"},{"location":"sdk/#lexsi_sdk.core.project.Project.delete_data_connectors","title":"delete_data_connectors","text":"<pre><code>delete_data_connectors(data_connector_name)\n</code></pre> <p>Delete a data connector from the organization using its name. This removes the external data link and returns a confirmation message.</p> <p>Parameters:</p> Name Type Description Default <code>data_connector_name</code> <code>str</code> <p>name of the data connector to be deleted.</p> required <p>Returns:</p> Type Description <code>str</code> <p>str</p>"},{"location":"sdk/#lexsi_sdk.core.project.Project.delete_file","title":"delete_file","text":"<pre><code>delete_file(file_name)\n</code></pre> <p>Delete a file with the given name from the project. Accepts the file name.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>uploaded file name</p> required <p>Returns:</p> Type Description <code>str</code> <p>response</p>"},{"location":"sdk/#lexsi_sdk.core.project.Project.delete_project","title":"delete_project","text":"<pre><code>delete_project()\n</code></pre> <p>Delete the project. Sends a delete request to the API and returns the response message.</p> <p>Returns:</p> Type Description <code>str</code> <p>response</p>"},{"location":"sdk/#lexsi_sdk.core.project.Project.events","title":"events","text":"<pre><code>events(event_id=None, event_names=None, status=None)\n</code></pre> <p>Fetches event details for the project.</p> <p>Parameters:</p> Name Type Description Default <code>event_id</code> <code>Optional[str]</code> <p>Optional event id to filter by.</p> <code>None</code> <code>event_names</code> <code>Optional[List[str]]</code> <p>Optional list of event names to filter by.</p> <code>None</code> <code>status</code> <code>Optional[List[str]]</code> <p>Optional list of statuses to filter by.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>event details</p>"},{"location":"sdk/#lexsi_sdk.core.project.Project.files","title":"files","text":"<pre><code>files()\n</code></pre> <p>List files uploaded to the project. Returns a DataFrame with file names and statuses. Only active files are included.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>user uploaded files dataframe</p>"},{"location":"sdk/#lexsi_sdk.core.project.Project.get_notifications","title":"get_notifications","text":"<pre><code>get_notifications()\n</code></pre> <p>get user project notifications</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame</p>"},{"location":"sdk/#lexsi_sdk.core.project.Project.get_tag_data","title":"get_tag_data","text":"<pre><code>get_tag_data(tag)\n</code></pre> <p>Run model inference on data</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>data tag for downloading</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dataframe</p>"},{"location":"sdk/#lexsi_sdk.core.project.Project.list_data_connectors","title":"list_data_connectors","text":"<pre><code>list_data_connectors()\n</code></pre> <p>List all data connectors configured in the project and organization. If successful, returns a DataFrame with details about each connector; otherwise returns an error message.</p>"},{"location":"sdk/#lexsi_sdk.core.project.Project.list_data_connectors_buckets","title":"list_data_connectors_buckets","text":"<pre><code>list_data_connectors_buckets(data_connector_name)\n</code></pre> <p>Retrieve the list of buckets (for S3 or GCS connectors) or similar container names for the specified data connector.</p> <p>Parameters:</p> Name Type Description Default <code>data_connector_name</code> <code>str</code> <p>name of the data connector</p> required <p>Returns:</p> Type Description <code>str | List</code> <p>str | List</p>"},{"location":"sdk/#lexsi_sdk.core.project.Project.list_data_connectors_filepath","title":"list_data_connectors_filepath","text":"<pre><code>list_data_connectors_filepath(\n    data_connector_name, bucket_name=None, root_folder=None\n)\n</code></pre> <p>List file paths within the specified data connector. For S3/GCS connectors you may need to provide a bucket_name; for SFTP connectors you may need to provide a root_folder.</p> <p>Parameters:</p> Name Type Description Default <code>data_connector_name</code> <code>str</code> <p>name of the data connector</p> required <code>bucket_name</code> <code>Optional[str]</code> <p>Required for S3 &amp; GCS</p> <code>None</code> <code>root_folder</code> <code>Optional[str]</code> <p>Root folder of SFTP</p> <code>None</code>"},{"location":"sdk/#lexsi_sdk.core.project.Project.model_summary","title":"model_summary","text":"<pre><code>model_summary(model_name=None)\n</code></pre> <p>Model Summary for the project</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>Optional[str]</code> <p>name of the model, defaults to active model for project</p> <code>None</code> <p>Returns:</p> Type Description <code>ModelSummary</code> <p>model summary</p>"},{"location":"sdk/#lexsi_sdk.core.project.Project.models","title":"models","text":"<pre><code>models()\n</code></pre> <p>List of models trained for the project</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Dataframe with details of all models</p>"},{"location":"sdk/#lexsi_sdk.core.project.Project.remove_model","title":"remove_model","text":"<pre><code>remove_model(model_name)\n</code></pre> <p>Removes the trained model from the project</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>name of the model</p> required <p>Returns:</p> Type Description <code>str</code> <p>response</p>"},{"location":"sdk/#lexsi_sdk.core.project.Project.remove_user_from_project","title":"remove_user_from_project","text":"<pre><code>remove_user_from_project(email)\n</code></pre> <p>Remove a user from the project using their email address. Returns a confirmation message.</p> <p>Parameters:</p> Name Type Description Default <code>email</code> <code>str</code> <p>user email</p> required <p>Returns:</p> Type Description <code>str</code> <p>response</p>"},{"location":"sdk/#lexsi_sdk.core.project.Project.rename_project","title":"rename_project","text":"<pre><code>rename_project(new_project_name)\n</code></pre> <p>Rename the project by providing a new name. Sends an update request and returns a confirmation message.</p> <p>Parameters:</p> Name Type Description Default <code>new_project_name</code> <code>str</code> <p>new name for the project</p> required <p>Returns:</p> Type Description <code>str</code> <p>response</p>"},{"location":"sdk/#lexsi_sdk.core.project.Project.start_server","title":"start_server","text":"<pre><code>start_server()\n</code></pre> <p>Start a dedicated server for the project, enabling training or inference activities.</p> <p>Returns:</p> Type Description <code>str</code> <p>response</p>"},{"location":"sdk/#lexsi_sdk.core.project.Project.stop_server","title":"stop_server","text":"<pre><code>stop_server()\n</code></pre> <p>Stop the dedicated project server to release compute resources.</p> <p>Returns:</p> Type Description <code>str</code> <p>response</p>"},{"location":"sdk/#lexsi_sdk.core.project.Project.tag_data","title":"tag_data","text":"<pre><code>tag_data(tag, page=1)\n</code></pre> <p>Tag Data</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>Tag name to filter data by.</p> required <code>page</code> <code>Optional[int]</code> <p>Page number for paginated results.</p> <code>1</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>tag data dataframe</p>"},{"location":"sdk/#lexsi_sdk.core.project.Project.tags","title":"tags","text":"<pre><code>tags()\n</code></pre> <p>Available User Tags for Project</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>list of tags</p>"},{"location":"sdk/#lexsi_sdk.core.project.Project.test_data_connectors","title":"test_data_connectors","text":"<pre><code>test_data_connectors(data_connector_name)\n</code></pre> <p>Test the connection of an existing data connector to ensure credentials and connectivity are valid. Takes the connector name as input and returns the status of the connection test.</p> <p>Parameters:</p> Name Type Description Default <code>data_connector_name</code> <code>str</code> <p>name of the data connector to be tested.</p> required"},{"location":"sdk/#lexsi_sdk.core.project.Project.update_server","title":"update_server","text":"<pre><code>update_server(server_type)\n</code></pre> <p>Update the dedicated server for the project by specifying a new instance type.</p> <p>Parameters:</p> Name Type Description Default <code>server_type</code> <code>str</code> <p>dedicated instance to run workloads for all available instances check xai.available_custom_servers()</p> required <p>Returns:</p> Type Description <code>str</code> <p>response</p>"},{"location":"sdk/#lexsi_sdk.core.project.Project.update_user_access_for_project","title":"update_user_access_for_project","text":"<pre><code>update_user_access_for_project(email, role)\n</code></pre> <p>Update the role of a user within the project. Accepts an email and new role and returns a response.</p> <p>Parameters:</p> Name Type Description Default <code>email</code> <code>str</code> <p>user email</p> required <code>role</code> <code>str</code> <p>user role</p> required <p>Returns:</p> Type Description <code>str</code> <p>response</p>"},{"location":"sdk/#lexsi_sdk.core.project.Project.upload_data_dataconnectors","title":"upload_data_dataconnectors","text":"<pre><code>upload_data_dataconnectors(\n    data_connector_name,\n    tag,\n    model_path=None,\n    model_name=None,\n    model_architecture=None,\n    model_type=None,\n    bucket_name=None,\n    file_path=None,\n    config=None,\n)\n</code></pre> <p>Uploads data for the current project with data connectors</p> <p>Parameters:</p> Name Type Description Default <code>data_connector_name</code> <code>str</code> <p>name of the data connector</p> required <code>tag</code> <code>str</code> <p>tag for data</p> required <code>bucket_name</code> <code>Optional[str]</code> <p>if data connector has buckets # Example: s3/gcs buckets</p> <code>None</code> <code>file_path</code> <code>Optional[str]</code> <p>filepath from the bucket for the data to read</p> <code>None</code> <code>config</code> <code>Optional[ProjectConfig]</code> <p>project config { \"project_type\": \"\", \"unique_identifier\": \"\", \"true_label\": \"\", \"pred_label\": \"\", \"feature_exclude\": [], \"drop_duplicate_uid: \"\", \"handle_errors\": False, \"feature_encodings\": Dict[str, str]   # {\"feature_name\":\"labelencode | countencode | onehotencode\"} }, defaults to None</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>response</p>"},{"location":"sdk/#lexsi_sdk.core.case.CaseImage","title":"CaseImage","text":"<pre><code>CaseImage(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Represents an explainability case for a prediction. Provides visualization helpers such as SHAP, LIME, Integrated Gradients, GradCAM</p> <p>Capture API client used to fetch additional explainability data. Stores configuration and prepares the object for use.</p> Source code in <code>lexsi_sdk/core/case.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Capture API client used to fetch additional explainability data.\n    Stores configuration and prepares the object for use.\"\"\"\n    super().__init__(**kwargs)\n    self.api_client = kwargs.get(\"api_client\")\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.case.CaseImage.alerts_trail","title":"alerts_trail","text":"<pre><code>alerts_trail(page_num=1, days=7)\n</code></pre> <p>Fetch alerts for this case over the given window. Encapsulates a small unit of SDK logic and returns the computed result.</p> Source code in <code>lexsi_sdk/core/case.py</code> <pre><code>def alerts_trail(self, page_num: Optional[int] = 1, days: Optional[int] = 7):\n    \"\"\"Fetch alerts for this case over the given window.\n    Encapsulates a small unit of SDK logic and returns the computed result.\"\"\"\n    if days == 7:\n        return pd.DataFrame(self.audit_trail.get(\"alerts\", {}))\n    resp = self.api_client.post(\n        f\"{GET_TRIGGERS_DAYS_URI}?project_name={self.project_name}&amp;page_num={page_num}&amp;days={days}\"\n    )\n    if resp.get(\"details\"):\n        return pd.DataFrame(resp.get(\"details\"))\n    else:\n        return \"No alerts found.\"\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.case.CaseImage.audit","title":"audit","text":"<pre><code>audit()\n</code></pre> <p>Return stored audit trail. Encapsulates a small unit of SDK logic and returns the computed result.</p> Source code in <code>lexsi_sdk/core/case.py</code> <pre><code>def audit(self):\n    \"\"\"Return stored audit trail.\n    Encapsulates a small unit of SDK logic and returns the computed result.\"\"\"\n    return self.audit_trail\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.case.CaseImage.inference_output","title":"inference_output","text":"<pre><code>inference_output()\n</code></pre> <p>Return a DataFrame summarizing the final decision for the case, including the true value, predicted value, predicted category, and final decision.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>decision dataframe</p> Source code in <code>lexsi_sdk/core/case.py</code> <pre><code>def inference_output(self) -&gt; pd.DataFrame:\n    \"\"\"Return a DataFrame summarizing the final decision for the case, including the true value, predicted value, predicted category, and final decision.\n\n    :return: decision dataframe\n    \"\"\"\n    data = {\n        \"True Value\": self.true_value,\n        \"Prediction Value\": self.pred_value,\n        \"Prediction Category\": self.pred_category,\n        \"Final Prediction\": self.final_decision,\n    }\n    decision_df = pd.DataFrame([data])\n\n    return decision_df\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.case.CaseImage.xai_gradcam","title":"xai_gradcam","text":"<pre><code>xai_gradcam()\n</code></pre> <p>Visualize Grad-CAM results for image data, showing heatmaps and superimposed regions that contributed to the prediction.</p> Source code in <code>lexsi_sdk/core/case.py</code> <pre><code>def xai_gradcam(self):\n    \"\"\"Visualize Grad-CAM results for image data, showing heatmaps and superimposed regions that contributed to the prediction.\"\"\"\n    if not self.image_data.get(\"gradcam\", None):\n        return \"No Gradcam method found for this case\"\n    fig = go.Figure()\n\n    fig.add_layout_image(\n        dict(\n            source=self.image_data.get(\"gradcam\", {}).get(\"heatmap\"),\n            xref=\"x\",\n            yref=\"y\",\n            x=0,\n            y=1,\n            sizex=1,\n            sizey=1,\n            xanchor=\"left\",\n            yanchor=\"top\",\n            layer=\"below\",\n        )\n    )\n\n    fig.add_layout_image(\n        dict(\n            source=self.image_data.get(\"gradcam\", {}).get(\"superimposed\"),\n            xref=\"x\",\n            yref=\"y\",\n            x=1.2,\n            y=1,\n            sizex=1,\n            sizey=1,\n            xanchor=\"left\",\n            yanchor=\"top\",\n            layer=\"below\",\n        )\n    )\n\n    fig.add_annotation(\n        x=0.5,\n        y=0.1,\n        text=\"Attributions\",\n        showarrow=False,\n        font=dict(size=16),\n        xref=\"x\",\n        yref=\"y\",\n    )\n    fig.add_annotation(\n        x=1.7,\n        y=0.1,\n        text=\"Superimposed\",\n        showarrow=False,\n        font=dict(size=16),\n        xref=\"x\",\n        yref=\"y\",\n    )\n    fig.update_layout(\n        xaxis=dict(visible=False, range=[0, 2.5]),\n        yaxis=dict(visible=False, range=[0, 1]),\n        margin=dict(l=30, r=30, t=30, b=30),\n    )\n\n    fig.show(config={\"displaylogo\": False})\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.case.CaseImage.xai_ig","title":"xai_ig","text":"<pre><code>xai_ig()\n</code></pre> <p>Render an integrated gradients attribution plot for image cases, showing positive and negative attributions side-by-side.</p> Source code in <code>lexsi_sdk/core/case.py</code> <pre><code>def xai_ig(self):\n    \"\"\"Render an integrated gradients attribution plot for image cases, showing positive and negative attributions side-by-side.\"\"\"\n    if not self.image_data.get(\"integrated_gradients\", None):\n        return \"No Integrated Gradients method found for this case\"\n    fig = go.Figure()\n\n    fig.add_layout_image(\n        dict(\n            source=self.image_data.get(\"integrated_gradients\", {}).get(\n                \"attributions\"\n            ),\n            xref=\"x\",\n            yref=\"y\",\n            x=0,\n            y=1,\n            sizex=1,\n            sizey=1,\n            xanchor=\"left\",\n            yanchor=\"top\",\n            layer=\"below\",\n        )\n    )\n\n    fig.add_layout_image(\n        dict(\n            source=self.image_data.get(\"integrated_gradients\", {}).get(\n                \"positive_attributions\"\n            ),\n            xref=\"x\",\n            yref=\"y\",\n            x=1.2,\n            y=1,\n            sizex=1,\n            sizey=1,\n            xanchor=\"left\",\n            yanchor=\"top\",\n            layer=\"below\",\n        )\n    )\n\n    fig.add_layout_image(\n        dict(\n            source=self.image_data.get(\"integrated_gradients\", {}).get(\n                \"negative_attributions\"\n            ),\n            xref=\"x\",\n            yref=\"y\",\n            x=2.4,\n            y=1,\n            sizex=1,\n            sizey=1,\n            xanchor=\"left\",\n            yanchor=\"top\",\n            layer=\"below\",\n        )\n    )\n\n    fig.add_annotation(\n        x=0.5,\n        y=0.1,\n        text=\"Attributions\",\n        showarrow=False,\n        font=dict(size=16),\n        xref=\"x\",\n        yref=\"y\",\n    )\n    fig.add_annotation(\n        x=1.7,\n        y=0.1,\n        text=\"Positive Attributions\",\n        showarrow=False,\n        font=dict(size=16),\n        xref=\"x\",\n        yref=\"y\",\n    )\n    fig.add_annotation(\n        x=2.9,\n        y=0.1,\n        text=\"Negative Attributions\",\n        showarrow=False,\n        font=dict(size=16),\n        xref=\"x\",\n        yref=\"y\",\n    )\n    fig.update_layout(\n        xaxis=dict(visible=False, range=[0, 2.5]),\n        yaxis=dict(visible=False, range=[0, 1]),\n        margin=dict(l=30, r=30, t=30, b=30),\n    )\n\n    fig.show(config={\"displaylogo\": False})\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.case.CaseImage.xai_lime","title":"xai_lime","text":"<pre><code>xai_lime()\n</code></pre> <p>Render a LIME attribution plot for image cases, displaying attributions as an overlay on the original image.</p> Source code in <code>lexsi_sdk/core/case.py</code> <pre><code>def xai_lime(self):\n    \"\"\"Render a LIME attribution plot for image cases, displaying attributions as an overlay on the original image.\"\"\"\n    if not self.image_data.get(\"lime\", None):\n        return \"No Lime method found for this case\"\n    fig = go.Figure()\n\n    fig.add_layout_image(\n        dict(\n            source=self.image_data.get(\"lime\", {}).get(\"plot\"),\n            xref=\"x\",\n            yref=\"y\",\n            x=0,\n            y=1,\n            sizex=1,\n            sizey=1,\n            xanchor=\"left\",\n            yanchor=\"top\",\n            layer=\"below\",\n        )\n    )\n\n    fig.update_layout(\n        xaxis=dict(visible=False, range=[0, 2.5]),\n        yaxis=dict(visible=False, range=[0, 1]),\n        margin=dict(l=30, r=30, t=30, b=30),\n    )\n\n    fig.show(config={\"displaylogo\": False})\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.case.CaseImage.xai_shap","title":"xai_shap","text":"<pre><code>xai_shap()\n</code></pre> <p>Render a SHAP attribution plot for image cases, displaying attributions as an overlay on the original image.</p> Source code in <code>lexsi_sdk/core/case.py</code> <pre><code>def xai_shap(self):\n    \"\"\"Render a SHAP attribution plot for image cases, displaying attributions as an overlay on the original image.\"\"\"\n    if not self.image_data.get(\"shap\", None):\n        return \"No Shap method found for this case\"\n    fig = go.Figure()\n\n    fig.add_layout_image(\n        dict(\n            source=self.image_data.get(\"shap\", {}).get(\"plot\"),\n            xref=\"x\",\n            yref=\"y\",\n            x=0,\n            y=1,\n            sizex=1,\n            sizey=1,\n            xanchor=\"left\",\n            yanchor=\"top\",\n            layer=\"below\",\n        )\n    )\n\n    fig.update_layout(\n        xaxis=dict(visible=False, range=[0, 2.5]),\n        yaxis=dict(visible=False, range=[0, 1]),\n        margin=dict(l=30, r=30, t=30, b=30),\n    )\n\n    fig.show(config={\"displaylogo\": False})\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.case.CaseTabular","title":"CaseTabular","text":"<pre><code>CaseTabular(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Represents an explainability case for a prediction. Provides visualization helpers such as SHAP, LIME, DLB and decision paths for tabular data.</p> <p>Capture API client used to fetch additional explainability data. Stores configuration and prepares the object for use.</p> Source code in <code>lexsi_sdk/core/case.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Capture API client used to fetch additional explainability data.\n    Stores configuration and prepares the object for use.\"\"\"\n    super().__init__(**kwargs)\n    self.api_client = kwargs.get(\"api_client\")\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.case.CaseTabular.alerts_trail","title":"alerts_trail","text":"<pre><code>alerts_trail(page_num=1, days=7)\n</code></pre> <p>Fetch alerts for this case over the given window. Encapsulates a small unit of SDK logic and returns the computed result.</p> Source code in <code>lexsi_sdk/core/case.py</code> <pre><code>def alerts_trail(self, page_num: Optional[int] = 1, days: Optional[int] = 7):\n    \"\"\"Fetch alerts for this case over the given window.\n    Encapsulates a small unit of SDK logic and returns the computed result.\"\"\"\n    if days == 7:\n        return pd.DataFrame(self.audit_trail.get(\"alerts\", {}))\n    resp = self.api_client.post(\n        f\"{GET_TRIGGERS_DAYS_URI}?project_name={self.project_name}&amp;page_num={page_num}&amp;days={days}\"\n    )\n    if resp.get(\"details\"):\n        return pd.DataFrame(resp.get(\"details\"))\n    else:\n        return \"No alerts found.\"\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.case.CaseTabular.audit","title":"audit","text":"<pre><code>audit()\n</code></pre> <p>Return stored audit trail. Encapsulates a small unit of SDK logic and returns the computed result.</p> Source code in <code>lexsi_sdk/core/case.py</code> <pre><code>def audit(self):\n    \"\"\"Return stored audit trail.\n    Encapsulates a small unit of SDK logic and returns the computed result.\"\"\"\n    return self.audit_trail\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.case.CaseTabular.feature_importance","title":"feature_importance","text":"<pre><code>feature_importance(feature)\n</code></pre> <p>Return feature importance values for a specific feature. Encapsulates a small unit of SDK logic and returns the computed result.</p> Source code in <code>lexsi_sdk/core/case.py</code> <pre><code>def feature_importance(self, feature: str):\n    \"\"\"Return feature importance values for a specific feature.\n    Encapsulates a small unit of SDK logic and returns the computed result.\"\"\"\n    if self.shap_feature_importance:\n        return self.shap_feature_importance.get(feature, {})\n    elif self.lime_feature_importance:\n        return self.lime_feature_importance.get(feature, {})\n    elif self.ig_features_importance:\n        return self.ig_features_importance.get(feature, {})\n    else:\n        return \"No Feature Importance found for the case\"\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.case.CaseTabular.inference_output","title":"inference_output","text":"<pre><code>inference_output()\n</code></pre> <p>Return a DataFrame summarizing the final decision for the case, including the true value, predicted value, predicted category, and final decision.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>decision dataframe</p> Source code in <code>lexsi_sdk/core/case.py</code> <pre><code>def inference_output(self) -&gt; pd.DataFrame:\n    \"\"\"Return a DataFrame summarizing the final decision for the case, including the true value, predicted value, predicted category, and final decision.\n\n    :return: decision dataframe\n    \"\"\"\n    data = {\n        \"True Value\": self.true_value,\n        \"Prediction Value\": self.pred_value,\n        \"Prediction Category\": self.pred_category,\n        \"Final Prediction\": self.final_decision,\n    }\n    decision_df = pd.DataFrame([data])\n\n    return decision_df\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.case.CaseTabular.xai_dlb","title":"xai_dlb","text":"<pre><code>xai_dlb()\n</code></pre> <p>Plot a horizontal bar chart showing Deep Lift Bayesian (DLB)-based feature importance for the case.</p> Source code in <code>lexsi_sdk/core/case.py</code> <pre><code>def xai_dlb(self):\n    \"\"\"Plot a horizontal bar chart showing Deep Lift Bayesian (DLB)-based feature importance for the case.\"\"\"\n    fig = go.Figure()\n    if len(list(self.dlb_feature_importance.values())) &lt; 1:\n        return \"No DLB Feature Importance for the case\"\n\n    if isinstance(list(self.dlb_feature_importance.values())[0], dict):\n        for col in self.dlb_feature_importance.keys():\n            fig.add_trace(\n                go.Bar(\n                    x=list(self.dlb_feature_importance[col].values()),\n                    y=list(self.dlb_feature_importance[col].keys()),\n                    orientation=\"h\",\n                    name=col,\n                )\n            )\n    else:\n        fig.add_trace(\n            go.Bar(\n                x=list(self.dlb_feature_importance.values()),\n                y=list(self.dlb_feature_importance.keys()),\n                orientation=\"h\",\n            )\n        )\n    fig.update_layout(\n        barmode=\"relative\",\n        height=800,\n        width=800,\n        yaxis_autorange=\"reversed\",\n        bargap=0.01,\n        legend_orientation=\"h\",\n        legend_x=0.1,\n        legend_y=1.1,\n    )\n    fig.show(config={\"displaylogo\": False})\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.case.CaseTabular.xai_ig","title":"xai_ig","text":"<pre><code>xai_ig()\n</code></pre> <p>Plot a horizontal bar chart showing Integrated Gradients-based feature importance for the case.</p> Source code in <code>lexsi_sdk/core/case.py</code> <pre><code>def xai_ig(self):\n    \"\"\"Plot a horizontal bar chart showing Integrated Gradients-based feature importance for the case.\"\"\"\n    fig = go.Figure()\n\n    if len(list(self.ig_features_importance.values())) &lt; 1:\n        return \"No IG Feature Importance for the case\"\n\n    if isinstance(list(self.ig_features_importance.values())[0], dict):\n        for col in self.ig_features_importance.keys():\n            fig.add_trace(\n                go.Bar(\n                    x=list(self.ig_features_importance[col].values()),\n                    y=list(self.ig_features_importance[col].keys()),\n                    orientation=\"h\",\n                    name=col,\n                )\n            )\n    else:\n        fig.add_trace(\n            go.Bar(\n                x=list(self.ig_features_importance.values()),\n                y=list(self.ig_features_importance.keys()),\n                orientation=\"h\",\n            )\n        )\n    fig.update_layout(\n        barmode=\"relative\",\n        height=800,\n        width=800,\n        yaxis_autorange=\"reversed\",\n        bargap=0.01,\n        legend_orientation=\"h\",\n        legend_x=0.1,\n        legend_y=1.1,\n    )\n    fig.show(config={\"displaylogo\": False})\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.case.CaseTabular.xai_lime","title":"xai_lime","text":"<pre><code>xai_lime()\n</code></pre> <p>Plot a horizontal bar chart showing LIME-based feature importance for the case.</p> Source code in <code>lexsi_sdk/core/case.py</code> <pre><code>def xai_lime(self):\n    \"\"\"Plot a horizontal bar chart showing LIME-based feature importance for the case.\"\"\"\n    fig = go.Figure()\n\n    if len(list(self.lime_feature_importance.values())) &lt; 1:\n        return \"No Lime Feature Importance for the case\"\n\n    if isinstance(list(self.lime_feature_importance.values())[0], dict):\n        for col in self.lime_feature_importance.keys():\n            fig.add_trace(\n                go.Bar(\n                    x=list(self.lime_feature_importance[col].values()),\n                    y=list(self.lime_feature_importance[col].keys()),\n                    orientation=\"h\",\n                    name=col,\n                )\n            )\n    else:\n        fig.add_trace(\n            go.Bar(\n                x=list(self.lime_feature_importance.values()),\n                y=list(self.lime_feature_importance.keys()),\n                orientation=\"h\",\n            )\n        )\n    fig.update_layout(\n        barmode=\"relative\",\n        height=800,\n        width=800,\n        yaxis_autorange=\"reversed\",\n        bargap=0.01,\n        legend_orientation=\"h\",\n        legend_x=0.1,\n        legend_y=1.1,\n    )\n    fig.show(config={\"displaylogo\": False})\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.case.CaseTabular.xai_observations","title":"xai_observations","text":"<pre><code>xai_observations()\n</code></pre> <p>Return a DataFrame listing the checklist of observations (e.g., heuristics or warnings) associated with the case.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>observations dataframe</p> Source code in <code>lexsi_sdk/core/case.py</code> <pre><code>def xai_observations(self) -&gt; pd.DataFrame:\n    \"\"\"Return a DataFrame listing the checklist of observations (e.g., heuristics or warnings) associated with the case.\n\n    :return: observations dataframe\n    \"\"\"\n    observations_df = pd.DataFrame(self.observation_checklist)\n\n    return observations_df\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.case.CaseTabular.xai_policies","title":"xai_policies","text":"<pre><code>xai_policies()\n</code></pre> <p>Return a DataFrame listing policies or rules applied during the model\u2019s decision for the case.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>policies dataframe</p> Source code in <code>lexsi_sdk/core/case.py</code> <pre><code>def xai_policies(self) -&gt; pd.DataFrame:\n    \"\"\"Return a DataFrame listing policies or rules applied during the model\u2019s decision for the case.\n\n    :return: policies dataframe\n    \"\"\"\n    policy_df = pd.DataFrame(self.policy_checklist)\n\n    return policy_df\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.case.CaseTabular.xai_prediction_path","title":"xai_prediction_path","text":"<pre><code>xai_prediction_path()\n</code></pre> <p>Display the model\u2019s prediction path as a sequence of decision nodes for the case, typically visualized as an SVG or plot.</p> Source code in <code>lexsi_sdk/core/case.py</code> <pre><code>def xai_prediction_path(self):\n    \"\"\"Display the model\u2019s prediction path as a sequence of decision nodes for the case, typically visualized as an SVG or plot.\"\"\"\n    svg = SVG(self.case_prediction_svg)\n    display(svg)\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.case.CaseTabular.xai_raw_data","title":"xai_raw_data","text":"<pre><code>xai_raw_data()\n</code></pre> <p>Return the raw data used for the case as a DataFrame, with feature names and values.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>raw data dataframe</p> Source code in <code>lexsi_sdk/core/case.py</code> <pre><code>def xai_raw_data(self) -&gt; pd.DataFrame:\n    \"\"\"Return the raw data used for the case as a DataFrame, with feature names and values.\n\n    :return: raw data dataframe\n    \"\"\"\n    raw_data_df = (\n        pd.DataFrame([self.data])\n        .transpose()\n        .reset_index()\n        .rename(columns={\"index\": \"Feature\", 0: \"Value\"})\n    )\n    return raw_data_df\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.case.CaseTabular.xai_shap","title":"xai_shap","text":"<pre><code>xai_shap()\n</code></pre> <p>Plot a horizontal bar chart showing SHAP-based feature importance for the case. Uses stored Shapley values for features.</p> Source code in <code>lexsi_sdk/core/case.py</code> <pre><code>def xai_shap(self):\n    \"\"\"Plot a horizontal bar chart showing SHAP-based feature importance for the case. Uses stored Shapley values for features.\"\"\"\n    fig = go.Figure()\n\n    if len(list(self.shap_feature_importance.values())) &lt; 1:\n        return \"No Shap Feature Importance for the case\"\n\n    if isinstance(list(self.shap_feature_importance.values())[0], dict):\n        for col in self.shap_feature_importance.keys():\n            fig.add_trace(\n                go.Bar(\n                    x=list(self.shap_feature_importance[col].values()),\n                    y=list(self.shap_feature_importance[col].keys()),\n                    orientation=\"h\",\n                    name=col,\n                )\n            )\n    else:\n        fig.add_trace(\n            go.Bar(\n                x=list(self.shap_feature_importance.values()),\n                y=list(self.shap_feature_importance.keys()),\n                orientation=\"h\",\n            )\n        )\n    fig.update_layout(\n        barmode=\"relative\",\n        height=800,\n        width=800,\n        yaxis_autorange=\"reversed\",\n        bargap=0.01,\n        legend_orientation=\"h\",\n        legend_x=0.1,\n        legend_y=1.1,\n    )\n    fig.show(config={\"displaylogo\": False})\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.case.CaseTabular.xai_similar_cases","title":"xai_similar_cases","text":"<pre><code>xai_similar_cases()\n</code></pre> <p>Return a DataFrame of cases similar to the current case (if similar cases are available). If no similar cases are found, returns a message.</p> <p>Returns:</p> Type Description <code>DataFrame | str</code> <p>similar cases dataframe</p> Source code in <code>lexsi_sdk/core/case.py</code> <pre><code>def xai_similar_cases(self) -&gt; pd.DataFrame | str:\n    \"\"\"Return a DataFrame of cases similar to the current case (if similar cases are available). If no similar cases are found, returns a message.\n\n    :return: similar cases dataframe\n    \"\"\"\n    if not self.similar_cases_data:\n        return \"No similar cases found. Or add 'similar_cases' in components case_info()\"\n\n    similar_cases_df = pd.DataFrame(self.similar_cases_data)\n    return similar_cases_df\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.case.CaseTabular.xai_summary","title":"xai_summary","text":"<pre><code>xai_summary()\n</code></pre> <p>Request or return cached explainability summary text. Encapsulates a small unit of SDK logic and returns the computed result.</p> Source code in <code>lexsi_sdk/core/case.py</code> <pre><code>def xai_summary(self):\n    \"\"\"Request or return cached explainability summary text.\n    Encapsulates a small unit of SDK logic and returns the computed result.\"\"\"\n    if self.data_id and not self.summary:\n        payload = {\n            \"project_name\": self.project_name,\n            \"viewed_case_id\": self.data_id,\n        }\n        res = self.api_client.post(EXPLAINABILITY_SUMMARY, payload)\n        if not res.get(\"success\"):\n            raise Exception(res.get(\"details\", \"Failed to summarize\"))\n\n        self.summary = res.get(\"details\")\n        return res.get(\"details\")\n\n    return self.summary\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.case.CaseText","title":"CaseText","text":"<p>               Bases: <code>BaseModel</code></p> <p>Explainability view for text-based cases. Supports token-level importance, attention visualization, and LLM output analysis.</p>"},{"location":"sdk/#lexsi_sdk.core.case.CaseText.audit","title":"audit","text":"<pre><code>audit()\n</code></pre> <p>Return audit details for the text case. Encapsulates a small unit of SDK logic and returns the computed result.</p> Source code in <code>lexsi_sdk/core/case.py</code> <pre><code>def audit(self):\n    \"\"\"Return audit details for the text case.\n    Encapsulates a small unit of SDK logic and returns the computed result.\"\"\"\n    return self.audit_trail\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.case.CaseText.explainability_feature_importance","title":"explainability_feature_importance","text":"<pre><code>explainability_feature_importance()\n</code></pre> <p>Plots Feature Importance chart Encapsulates a small unit of SDK logic and returns the computed result.</p> Source code in <code>lexsi_sdk/core/case.py</code> <pre><code>def explainability_feature_importance(self):\n    \"\"\"Plots Feature Importance chart\n    Encapsulates a small unit of SDK logic and returns the computed result.\"\"\"\n    fig = go.Figure()\n    feature_importance = self.explainability.get(\"feature_importance\", {})\n\n    if not feature_importance:\n        return \"No Feature Importance for the case\"\n    raw_data_df = (\n        pd.DataFrame([feature_importance])\n        .transpose()\n        .reset_index()\n        .rename(columns={\"index\": \"Feature\", 0: \"Value\"})\n        .sort_values(by=\"Value\", ascending=False)\n    )\n    fig.add_trace(\n        go.Bar(x=raw_data_df[\"Value\"], y=raw_data_df[\"Feature\"], orientation=\"h\")\n    )\n    fig.update_layout(\n        barmode=\"relative\",\n        height=max(400, len(raw_data_df) * 20),\n        width=800,\n        yaxis=dict(\n            autorange=\"reversed\",\n            tickmode=\"array\",\n            tickvals=list(raw_data_df[\"Feature\"]),\n            ticktext=list(raw_data_df[\"Feature\"]),\n            tickfont=dict(size=10),\n        ),\n        bargap=0.01,\n        margin=dict(l=150, r=20, t=30, b=30),\n        legend_orientation=\"h\",\n        legend_x=0.1,\n        legend_y=0.5,\n    )\n\n    fig.show(config={\"displaylogo\": False})\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.case.CaseText.explainability_raw_data","title":"explainability_raw_data","text":"<pre><code>explainability_raw_data()\n</code></pre> <p>Return the raw data used for the case as a DataFrame, with feature names and values.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>raw data dataframe</p> Source code in <code>lexsi_sdk/core/case.py</code> <pre><code>def explainability_raw_data(self) -&gt; pd.DataFrame:\n    \"\"\"Return the raw data used for the case as a DataFrame, with feature names and values.\n\n    :return: raw data dataframe\n    \"\"\"\n    raw_data_df = (\n        pd.DataFrame([self.explainability.get(\"feature_importance\", {})])\n        .transpose()\n        .reset_index()\n        .rename(columns={\"index\": \"Feature\", 0: \"Value\"})\n        .sort_values(by=\"Value\", ascending=False)\n    )\n    return raw_data_df\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.case.CaseText.network_graph","title":"network_graph","text":"<pre><code>network_graph()\n</code></pre> <p>Decode and return a base64-encoded network graph image. Encapsulates a small unit of SDK logic and returns the computed result.</p> Source code in <code>lexsi_sdk/core/case.py</code> <pre><code>def network_graph(self):\n    \"\"\"Decode and return a base64-encoded network graph image.\n    Encapsulates a small unit of SDK logic and returns the computed result.\"\"\"\n    network_graph_data = self.explainability.get(\"network_graph\", {})\n    if not network_graph_data:\n        return \"No Network graph found for this case\"\n    base64_str = network_graph_data\n    try:\n        img_bytes = base64.b64decode(base64_str)\n        image = Image.open(BytesIO(img_bytes))\n        return image\n    except Exception as e:\n        print(f\"Error decoding base64 image: {e}\")\n        return None\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.case.CaseText.output","title":"output","text":"<pre><code>output()\n</code></pre> <p>Get output Encapsulates a small unit of SDK logic and returns the computed result.</p> Source code in <code>lexsi_sdk/core/case.py</code> <pre><code>def output(self):\n    \"\"\"Get output\n    Encapsulates a small unit of SDK logic and returns the computed result.\"\"\"\n    return self.output\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.case.CaseText.prompt","title":"prompt","text":"<pre><code>prompt()\n</code></pre> <p>Get prompt Encapsulates a small unit of SDK logic and returns the computed result.</p> Source code in <code>lexsi_sdk/core/case.py</code> <pre><code>def prompt(self):\n    \"\"\"Get prompt\n    Encapsulates a small unit of SDK logic and returns the computed result.\"\"\"\n    return self.prompt\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.case.CaseText.token_attribution_graph","title":"token_attribution_graph","text":"<pre><code>token_attribution_graph()\n</code></pre> <p>Decode and return a base64-encoded token attribution graph. Encapsulates a small unit of SDK logic and returns the computed result.</p> Source code in <code>lexsi_sdk/core/case.py</code> <pre><code>def token_attribution_graph(self):\n    \"\"\"Decode and return a base64-encoded token attribution graph.\n    Encapsulates a small unit of SDK logic and returns the computed result.\"\"\"\n    relevance_data = self.explainability.get(\"relevance\", {})\n    if not relevance_data:\n        return \"No Token Attribution graph found for this case\"\n    base64_str = relevance_data\n    try:\n        img_bytes = base64.b64decode(base64_str)\n        image = Image.open(BytesIO(img_bytes))\n        return image\n    except Exception as e:\n        print(f\"Error decoding base64 image: {e}\")\n        return None\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.dashboard.Dashboard","title":"Dashboard","text":"<pre><code>Dashboard(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Client-side wrapper for dashboards generated by Lexsi. Enables fetching dashboard metadata and underlying data used for monitoring and analysis.</p> <p>Print configuration then render the dashboard frame. Stores configuration and prepares the object for use.</p> Source code in <code>lexsi_sdk/core/dashboard.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Print configuration then render the dashboard frame.\n    Stores configuration and prepares the object for use.\"\"\"\n    super().__init__(**kwargs)\n\n    self.print_config()\n    self.plot()\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.dashboard.Dashboard.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Return a copy of the dashboard configuration dictionary (excluding metadata) so you can inspect settings like type and metrics.</p> Source code in <code>lexsi_sdk/core/dashboard.py</code> <pre><code>def get_config(self) -&gt; dict:\n    \"\"\"Return a copy of the dashboard configuration dictionary (excluding metadata) so you can inspect settings like type and metrics.\"\"\"\n    config_copy = {**self.config}\n    config_copy.pop(\"metadata\", None)\n    return config_copy\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.dashboard.Dashboard.get_raw_data","title":"get_raw_data","text":"<pre><code>get_raw_data()\n</code></pre> <p>Return a dictionary containing the raw data underlying the dashboard, tailored to the dashboard\u2019s type (e.g., data drift, target drift, performance metrics).</p> Source code in <code>lexsi_sdk/core/dashboard.py</code> <pre><code>def get_raw_data(self) -&gt; dict:\n    \"\"\"Return a dictionary containing the raw data underlying the dashboard, tailored to the dashboard\u2019s type (e.g., data drift, target drift, performance metrics).\"\"\"\n    raw_data = {\"created_at\": self.config.get(\"created_at\")}\n\n    if self.config[\"type\"] == \"data_drift\":\n        data_drift_table = next(\n            filter(\n                lambda data: data[\"metric\"] == \"DataDriftTable\",\n                self.raw_data.get(\"metrics\"),\n            ),\n            None,\n        )\n        if data_drift_table:\n            for item in data_drift_table[\"result\"].get(\"drift_by_columns\"):\n                item.pop(\"current_small_distribution\", None)\n                item.pop(\"reference_small_distribution\", None)\n                item.pop(\"current_big_distribution\", None)\n                item.pop(\"reference_big_distribution\", None)\n                item.pop(\"current_mean\", None)\n                item.pop(\"reference_std\", None)\n            raw_data.update(data_drift_table[\"result\"])\n\n    if self.config[\"type\"] == \"target_drift\":\n        column_drift_metric = next(\n            filter(\n                lambda data: data[\"metric\"] == \"ColumnDriftMetric\",\n                self.raw_data.get(\"metrics\"),\n            ),\n            None,\n        )\n        if column_drift_metric:\n            column_drift_metric[\"result\"].pop(\"data\", None)\n\n            raw_data.update(column_drift_metric[\"result\"])\n\n    if self.config[\"type\"] == \"performance\":\n        classification_quality_metric = next(\n            filter(\n                lambda data: data[\"metric\"] == \"ClassificationQualityMetric\",\n                self.raw_data.get(\"metrics\"),\n            ),\n            None,\n        )\n        if classification_quality_metric:\n            for curr_ref in [\"current\", \"reference\"]:\n                classification_quality_metric[\"result\"][curr_ref].pop(\n                    \"rate_plots_data\", None\n                )\n                classification_quality_metric[\"result\"][curr_ref].pop(\n                    \"plot_data\", None\n                )\n            raw_data.update(classification_quality_metric[\"result\"])\n\n    return raw_data\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.dashboard.Dashboard.plot","title":"plot","text":"<pre><code>plot(width='100%', height=800)\n</code></pre> <p>Render the dashboard in an iframe, specifying the width and height of the frame. Displays the HTML or fetches the dashboard from the SDK portal.</p> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>int</code> <p>Width of the embedded frame.</p> <code>'100%'</code> <code>height</code> <code>int</code> <p>Height of the embedded frame.</p> <code>800</code> Source code in <code>lexsi_sdk/core/dashboard.py</code> <pre><code>def plot(self, width: int = \"100%\", height: int = 800):\n    \"\"\"Render the dashboard in an iframe, specifying the width and height of the frame. Displays the HTML or fetches the dashboard from the SDK portal.\n\n    :param width: Width of the embedded frame.\n    :param height: Height of the embedded frame.\n    \"\"\"\n    if isinstance(self.raw_data, str) and \"&lt;/html&gt;\" in self.raw_data:\n        display(HTML(self.raw_data))\n    else:\n        uri = os.environ.get(\"XAI_APP_URL\", XAI_APP_URI)\n        url = f\"{uri}/sdk/dashboard{self.query_params}\"\n        display(IFrame(src=f\"{url}\", width=width, height=height))\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.dashboard.Dashboard.print_config","title":"print_config","text":"<pre><code>print_config()\n</code></pre> <p>Pretty-print the dashboard configuration in JSON format for inspection.</p> Source code in <code>lexsi_sdk/core/dashboard.py</code> <pre><code>def print_config(self):\n    \"\"\"Pretty-print the dashboard configuration in JSON format for inspection.\"\"\"\n    config = {k: v for k, v in self.config.items() if v is not None}\n    config.pop(\"metadata\", None)\n    print(\"Using config: \", end=\"\")\n    print(json.dumps(config, indent=4))\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.alert.Alert","title":"Alert","text":"<pre><code>Alert(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Represents a monitoring alert generated by a model. Provides helper methods to inspect drift, unused features, and alert metadata.</p> <p>Initialize base model without extra behavior. Stores configuration and prepares the object for use.</p> Source code in <code>lexsi_sdk/core/alert.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize base model without extra behavior.\n    Stores configuration and prepares the object for use.\"\"\"\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.alert.Alert.view_detailed_report","title":"view_detailed_report","text":"<pre><code>view_detailed_report()\n</code></pre> <p>Return a detailed report for the alert, containing analysis and potential remediation recommendations.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>description</p> Source code in <code>lexsi_sdk/core/alert.py</code> <pre><code>def view_detailed_report(self) -&gt; pd.DataFrame:\n    \"\"\"Return a detailed report for the alert, containing analysis and potential remediation recommendations.\n\n    :return: _description_\n    \"\"\"\n    if not self.detailed_report:\n        return \"No detailed report found for the alert.\"\n\n    if isinstance(self.detailed_report, list):\n        detailed_report = [\n            {\n                key: value\n                for key, value in feature.items()\n                if key != \"current_small_hist\" and key != \"ref_small_hist\"\n            }\n            for feature in self.detailed_report\n        ]\n    if isinstance(self.detailed_report, dict):\n        detailed_report = [self.detailed_report]\n\n    return pd.DataFrame(detailed_report)\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.alert.Alert.view_info","title":"view_info","text":"<pre><code>view_info()\n</code></pre> <p>Return basic information about an alert, such as its type, category, severity, and timestamp.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>description</p> Source code in <code>lexsi_sdk/core/alert.py</code> <pre><code>def view_info(self) -&gt; pd.DataFrame:\n    \"\"\"Return basic information about an alert, such as its type, category, severity, and timestamp.\n\n    :return: _description_\n    \"\"\"\n    if not self.info:\n        return \"There was an error while executing the alert.\"\n\n    return pd.DataFrame([self.info])\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.alert.Alert.view_not_used_features","title":"view_not_used_features","text":"<pre><code>view_not_used_features()\n</code></pre> <p>Return features that were not used in the model\u2019s decision-making for the alert. Helps identify features excluded from analysis.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>description</p> Source code in <code>lexsi_sdk/core/alert.py</code> <pre><code>def view_not_used_features(self) -&gt; pd.DataFrame:\n    \"\"\"Return features that were not used in the model\u2019s decision-making for the alert. Helps identify features excluded from analysis.\n\n    :return: _description_\n    \"\"\"\n    if not self.not_used_features:\n        return \"Not used features is empty.\"\n\n    return pd.DataFrame(self.not_used_features)\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.text.TextProject","title":"TextProject","text":"<pre><code>TextProject(**kwargs)\n</code></pre> <p>               Bases: <code>Project</code></p> <p>Specialized project abstraction for text and LLM-based workloads. Supports sessions, messages, traces, guardrails, and token-level explainability.</p> Source code in <code>lexsi_sdk/core/project.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize a `Project` instance and attach the API client.\n    Populates model fields from `kwargs` and stores `api_client` for later requests.\n\n    :param kwargs: Project fields used to construct the instance (including `api_client`).\n    \"\"\"\n    super().__init__(**kwargs)\n    self.api_client = kwargs.get(\"api_client\")\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.text.TextProject.available_guardrails","title":"available_guardrails","text":"<pre><code>available_guardrails()\n</code></pre> <p>Return a DataFrame of all guardrails available to configure in this project. Each row describes a single guardrail type.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>a DataFrame containing all available guardrail types</p> Source code in <code>lexsi_sdk/core/text.py</code> <pre><code>def available_guardrails(self) -&gt; pd.DataFrame:\n    \"\"\"Return a DataFrame of all guardrails available to configure in this project.\n    Each row describes a single guardrail type.\n\n    :return: a DataFrame containing all available guardrail types\n    \"\"\"\n    res = self.api_client.get(AVAILABLE_GUARDRAILS_URI)\n    if not res[\"success\"]:\n        raise Exception(res.get(\"details\"))\n\n    return pd.DataFrame(res.get(\"details\"))\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.text.TextProject.chat_completion","title":"chat_completion","text":"<pre><code>chat_completion(\n    model,\n    messages,\n    provider,\n    api_key=None,\n    session_id=None,\n    max_tokens=None,\n    stream=False,\n)\n</code></pre> <p>Generate a chat completion using an OpenAI-compliant interface.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Name of the model to use for generating the chat completion</p> required <code>messages</code> <code>List[Dict[str, Any]]</code> <p>List of chat messages, where each message contains a role and content</p> required <code>provider</code> <code>str</code> <p>Model provider (e.g., \"OpenAI\", \"Anthropic\")</p> required <code>api_key</code> <code>Optional[str]</code> <p>API key for the selected provider, if required</p> <code>None</code> <code>session_id</code> <code>Optional[UUID]</code> <p>Session ID associated with this chat completion, if provided</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>Maximum number of tokens to generate</p> <code>None</code> <code>stream</code> <code>Optional[bool]</code> <p>Whether to stream the response</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[dict, Iterator[str]]</code> <p>a chat completion response dictionary or a streaming iterator of response chunks</p> Source code in <code>lexsi_sdk/core/text.py</code> <pre><code>def chat_completion(\n    self,\n    model: str,\n    messages: List[Dict[str, Any]],\n    provider: str,\n    api_key: Optional[str] = None,\n    session_id: Optional[UUID] = None,\n    max_tokens: Optional[int] = None,\n    stream: Optional[bool] = False,\n) -&gt; Union[dict, Iterator[str]]:\n    \"\"\"Generate a chat completion using an OpenAI-compliant interface.\n\n    :param model: Name of the model to use for generating the chat completion\n    :param messages: List of chat messages, where each message contains a role and content\n    :param provider: Model provider (e.g., \"OpenAI\", \"Anthropic\")\n    :param api_key: API key for the selected provider, if required\n    :param session_id: Session ID associated with this chat completion, if provided\n    :param max_tokens: Maximum number of tokens to generate\n    :param stream: Whether to stream the response\n    :return: a chat completion response dictionary or a streaming iterator of response chunks\n    \"\"\"\n    payload = {\n        \"model\": model,\n        \"messages\": messages,\n        \"max_tokens\": max_tokens,\n        \"stream\": stream,\n        \"project_name\": self.project_name,\n        \"provider\": provider,\n        \"api_key\": api_key,\n        \"session_id\": session_id,\n    }\n\n    if not stream:\n        return self.api_client.post(RUN_CHAT_COMPLETION, payload=payload)\n\n    return self.api_client.stream(\n        uri=RUN_CHAT_COMPLETION, method=\"POST\", payload=payload\n    )\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.text.TextProject.completion","title":"completion","text":"<pre><code>completion(\n    model,\n    prompt,\n    provider,\n    api_key=None,\n    session_id=None,\n    max_tokens=None,\n    stream=False,\n)\n</code></pre> <p>Generate a text completion using an OpenAI-compliant interface.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Name of the model to use for generating the completion</p> required <code>prompt</code> <code>str</code> <p>Input prompt to be provided to the model</p> required <code>provider</code> <code>str</code> <p>Model provider (e.g., \"OpenAI\", \"Anthropic\")</p> required <code>api_key</code> <code>Optional[str]</code> <p>API key for the selected provider, if required</p> <code>None</code> <code>session_id</code> <code>Optional[UUID]</code> <p>Session ID associated with this completion request, if provided</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>Maximum number of tokens to generate</p> <code>None</code> <code>stream</code> <code>Optional[bool]</code> <p>Whether to stream the response</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>a completion response dictionary or a streaming iterator of response chunks</p> Source code in <code>lexsi_sdk/core/text.py</code> <pre><code>def completion(\n    self,\n    model: str,\n    prompt: str,\n    provider: str,\n    api_key: Optional[str] = None,\n    session_id: Optional[UUID] = None,\n    max_tokens: Optional[int] = None,\n    stream: Optional[bool] = False,\n) -&gt; dict:\n    \"\"\"Generate a text completion using an OpenAI-compliant interface.\n\n    :param model: Name of the model to use for generating the completion\n    :param prompt: Input prompt to be provided to the model\n    :param provider: Model provider (e.g., \"OpenAI\", \"Anthropic\")\n    :param api_key: API key for the selected provider, if required\n    :param session_id: Session ID associated with this completion request, if provided\n    :param max_tokens: Maximum number of tokens to generate\n    :param stream: Whether to stream the response\n    :return: a completion response dictionary or a streaming iterator of response chunks\n    \"\"\"\n\n    payload = {\n        \"model\": model,\n        \"prompt\": prompt,\n        \"max_tokens\": max_tokens,\n        \"stream\": stream,\n        \"project_name\": self.project_name,\n        \"provider\": provider,\n        \"api_key\": api_key,\n        \"session_id\": session_id,\n    }\n    if not stream:\n        return self.api_client.post(RUN_COMPLETION, payload=payload)\n\n    return self.api_client.stream(\n        uri=RUN_COMPLETION, method=\"POST\", payload=payload\n    )\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.text.TextProject.configure_guardrail","title":"configure_guardrail","text":"<pre><code>configure_guardrail(\n    guardrail_name, guardrail_config, model_name, apply_on\n)\n</code></pre> <p>Configure a new guardrail in the project. Requires the guardrail name, a configuration dictionary, the model name, and where to apply it (input or output). Returns a confirmation message.</p> <p>Parameters:</p> Name Type Description Default <code>guardrail_name</code> <code>str</code> <p>Name of the guardrail</p> required <code>guardrail_config</code> <code>dict</code> <p>Configuration dictionary for the guardrail</p> required <code>model_name</code> <code>str</code> <p>Name of the model to which the guardrail applies</p> required <code>apply_on</code> <code>str</code> <p>Specifies when to apply the guardrail (\"input\" or \"output\")</p> required <p>Returns:</p> Type Description <code>str</code> <p>a response indicating the result of the configuration operation</p> Source code in <code>lexsi_sdk/core/text.py</code> <pre><code>def configure_guardrail(\n    self,\n    guardrail_name: str,\n    guardrail_config: dict,\n    model_name: str,\n    apply_on: str,\n) -&gt; str:\n    \"\"\"Configure a new guardrail in the project.\n    Requires the guardrail name, a configuration dictionary, the model name, and where to apply it (input or output).\n    Returns a confirmation message.\n\n    :param guardrail_name: Name of the guardrail\n    :param guardrail_config: Configuration dictionary for the guardrail\n    :param model_name: Name of the model to which the guardrail applies\n    :param apply_on: Specifies when to apply the guardrail (\"input\" or \"output\")\n    :return: a response indicating the result of the configuration operation\n    \"\"\"\n    payload = {\n        \"name\": guardrail_name,\n        \"config\": guardrail_config,\n        \"model_name\": model_name,\n        \"apply_on\": apply_on,\n        \"project_name\": self.project_name,\n    }\n    res = self.api_client.post(CONFIGURE_GUARDRAILS_URI, payload)\n    if not res[\"success\"]:\n        raise Exception(res.get(\"details\"))\n\n    return res.get(\"details\")\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.text.TextProject.create_embeddings","title":"create_embeddings","text":"<pre><code>create_embeddings(\n    input, model, provider, api_key=None, session_id=None\n)\n</code></pre> <p>Create embeddings using an OpenAI-compliant embeddings interface.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Union[str, List[str]]</code> <p>Input text or list of text strings to generate embeddings for</p> required <code>model</code> <code>str</code> <p>Name of the model to use for generating embeddings</p> required <code>provider</code> <code>str</code> <p>Model provider (e.g., \"OpenAI\", \"Anthropic\")</p> required <code>api_key</code> <code>Optional[str]</code> <p>API key for the selected provider, if required</p> <code>None</code> <code>session_id</code> <code>Optional[UUID]</code> <p>Session ID associated with this embeddings request, if provided</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>a dictionary containing the embeddings response</p> Source code in <code>lexsi_sdk/core/text.py</code> <pre><code>def create_embeddings(\n    self,\n    input: Union[str, List[str]],\n    model: str,\n    provider: str,\n    api_key : Optional[str] = None,\n    session_id : Optional[UUID] = None,\n) -&gt; dict:  \n    \"\"\"Create embeddings using an OpenAI-compliant embeddings interface.\n\n    :param input: Input text or list of text strings to generate embeddings for\n    :param model: Name of the model to use for generating embeddings\n    :param provider: Model provider (e.g., \"OpenAI\", \"Anthropic\")\n    :param api_key: API key for the selected provider, if required\n    :param session_id: Session ID associated with this embeddings request, if provided\n    :return: a dictionary containing the embeddings response\n    \"\"\"\n    payload = {\n        \"model\": model,\n        \"input\": input,\n        \"project_name\": self.project_name,\n        \"provider\": provider,\n        \"api_key\": api_key,\n        \"session_id\": session_id,\n    }\n\n    res = self.api_client.post(RUN_CREATE_EMBEDDING, payload=payload)\n    return res\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.text.TextProject.delete_guardrail","title":"delete_guardrail","text":"<pre><code>delete_guardrail(guardrail_id)\n</code></pre> <p>Delete a guardrail from the project using its ID. Returns the API response message.</p> <p>Parameters:</p> Name Type Description Default <code>guardrail_id</code> <code>str</code> <p>ID of the guardrail</p> required <p>Returns:</p> Type Description <code>str</code> <p>a response indicating the result of the delete operation</p> Source code in <code>lexsi_sdk/core/text.py</code> <pre><code>def delete_guardrail(self, guardrail_id: str) -&gt; str:\n    \"\"\"Delete a guardrail from the project using its ID.\n    Returns the API response message.\n\n    :param guardrail_id: ID of the guardrail\n    :return: a response indicating the result of the delete operation\n    \"\"\"\n    payload = {\n        \"project_name\": self.project_name,\n        \"guardrail_id\": guardrail_id,\n    }\n    res = self.api_client.post(DELETE_GUARDRAILS_URI, payload=payload)\n    if not res[\"success\"]:\n        raise Exception(res.get(\"details\"))\n\n    return res.get(\"details\")\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.text.TextProject.generate_text_case","title":"generate_text_case","text":"<pre><code>generate_text_case(\n    model_name,\n    prompt,\n    serverless_instance_type,\n    instance_type=None,\n    explainability_method=[\"DLB\"],\n    explain_model=False,\n    session_id=None,\n    max_tokens=None,\n    min_tokens=None,\n    stream=False,\n)\n</code></pre> <p>Generate a text inference case using the specified model and prompt.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to use for text generation</p> required <code>prompt</code> <code>str</code> <p>Input prompt to be provided to the model</p> required <code>serverless_instance_type</code> <code>ServerlessInstanceTypeValues</code> <p>Serverless instance type used for case inference Use str values from supported instance types defined in classes: - <code>ServerlessInstanceTypeValues</code></p> required <code>instance_type</code> <code>Optional[Union[BatchCPUInstanceTypeValues, BatchGPUInstanceTypeValues]]</code> <p>Instance type used for explainability processing, defaults to None Use str values from supported instance types defined in classes: - <code>BatchCPUInstanceTypeValues</code> - <code>BatchGPUInstanceTypeValues</code></p> <code>None</code> <code>explainability_method</code> <code>Optional[list]</code> <p>Explainability method(s) for the case, defaults to [\"DLB\"]</p> <code>['DLB']</code> <code>explain_model</code> <code>Optional[bool]</code> <p>Boolean flag indicating whether to run explainability for the case, defaults to False</p> <code>False</code> <code>session_id</code> <code>Optional[str]</code> <p>Session ID associated with this case, if applicable</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>Maximum number of tokens to generate</p> <code>None</code> <code>min_tokens</code> <code>Optional[int]</code> <p>Minimum number of tokens to generate</p> <code>None</code> <code>stream</code> <code>Optional[bool]</code> <p>Whether to stream the response</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>a dictionary containing the generated text and related metadata</p> Source code in <code>lexsi_sdk/core/text.py</code> <pre><code>def generate_text_case(\n    self,\n    model_name: str,\n    prompt: str,\n    serverless_instance_type: ServerlessInstanceTypeValues,\n    instance_type: Optional[Union[BatchCPUInstanceTypeValues, BatchGPUInstanceTypeValues]] = None,\n    explainability_method: Optional[list] = [\"DLB\"],\n    explain_model: Optional[bool] = False,\n    session_id: Optional[str] = None,\n    max_tokens: Optional[int] = None,\n    min_tokens: Optional[int] = None,\n    stream: Optional[bool] = False,\n) -&gt; dict:\n    \"\"\"Generate a text inference case using the specified model and prompt.\n\n    :param model_name: Name of the model to use for text generation\n    :param prompt: Input prompt to be provided to the model\n    :param serverless_instance_type: Serverless instance type used for case inference\n        Use str values from supported instance types defined in classes:\n        - ``ServerlessInstanceTypeValues``\n    :param instance_type: Instance type used for explainability processing, defaults to None\n        Use str values from supported instance types defined in classes:\n        - ``BatchCPUInstanceTypeValues``\n        - ``BatchGPUInstanceTypeValues``\n    :param explainability_method: Explainability method(s) for the case, defaults to [\"DLB\"]\n    :param explain_model: Boolean flag indicating whether to run explainability for the case, defaults to False\n    :param session_id: Session ID associated with this case, if applicable\n    :param max_tokens: Maximum number of tokens to generate\n    :param min_tokens: Minimum number of tokens to generate\n    :param stream: Whether to stream the response\n    :return: a dictionary containing the generated text and related metadata\n    \"\"\"\n    if explain_model and not instance_type:\n        raise Exception(\"instance_type required for explainability.\")\n    llm = monitor(\n        project=self,\n        client=LexsiModels(project=self, api_client=self.api_client),\n        session_id=session_id,\n    )\n    res = llm.generate_text_case(\n        model_name=model_name,\n        prompt=prompt,\n        instance_type=instance_type,\n        serverless_instance_type=serverless_instance_type,\n        explainability_method=explainability_method,\n        explain_model=explain_model,\n        max_tokens=max_tokens,\n        min_tokens=min_tokens,\n        stream=stream,\n    )\n    return res\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.text.TextProject.guardrails","title":"guardrails","text":"<pre><code>guardrails()\n</code></pre> <p>List all guardrails currently configured for the project. Returns a DataFrame describing each guardrail and its configuration.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>a DataFrame containing the configured guardrails and their details</p> Source code in <code>lexsi_sdk/core/text.py</code> <pre><code>def guardrails(self) -&gt; pd.DataFrame:\n    \"\"\"List all guardrails currently configured for the project.\n    Returns a DataFrame describing each guardrail and its configuration.\n\n    :return: a DataFrame containing the configured guardrails and their details\n    \"\"\"\n    res = self.api_client.get(\n        f\"{GET_GUARDRAILS_URI}?project_name={self.project_name}\"\n    )\n    if not res[\"success\"]:\n        raise Exception(res.get(\"details\"))\n\n    return pd.DataFrame(res.get(\"details\"))\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.text.TextProject.image_generation","title":"image_generation","text":"<pre><code>image_generation(\n    model, prompt, provider, api_key=None, session_id=None\n)\n</code></pre> <p>Generate images using an OpenAI-compliant image generation interface.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Name of the model to use for image generation</p> required <code>prompt</code> <code>str</code> <p>Text prompt describing the image to generate</p> required <code>provider</code> <code>str</code> <p>Model provider (e.g., \"OpenAI\", \"Anthropic\")</p> required <code>api_key</code> <code>Optional[str]</code> <p>API key for the selected provider, if required</p> <code>None</code> <code>session_id</code> <code>Optional[UUID]</code> <p>Session ID associated with this image generation request, if provided</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>a dictionary containing the image generation response</p> Source code in <code>lexsi_sdk/core/text.py</code> <pre><code>def image_generation(\n    self,\n    model: str,\n    prompt: str,\n    provider: str,\n    api_key: Optional[str] = None,\n    session_id : Optional[UUID] = None,\n) -&gt; dict:\n    \"\"\"Generate images using an OpenAI-compliant image generation interface.\n\n    :param model: Name of the model to use for image generation\n    :param prompt: Text prompt describing the image to generate\n    :param provider: Model provider (e.g., \"OpenAI\", \"Anthropic\")\n    :param api_key: API key for the selected provider, if required\n    :param session_id: Session ID associated with this image generation request, if provided\n    :return: a dictionary containing the image generation response\n    \"\"\"\n\n    payload = {\n        \"model\": model,\n        \"prompt\": prompt,\n        \"project_name\": self.project_name,\n        \"provider\": provider,\n        \"api_key\": api_key,\n        \"session_id\": session_id,\n    }\n\n    res = self.api_client.post(RUN_IMAGE_GENERATION, payload=payload)\n\n    return res\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.text.TextProject.initialize_text_model","title":"initialize_text_model","text":"<pre><code>initialize_text_model(\n    model_provider,\n    model_name,\n    model_task_type,\n    model_architecture,\n    inference_compute=None,\n    inference_settings=None,\n    assets=None,\n    requirements_file=None,\n    app_file=None,\n)\n</code></pre> <p>Initialize a text model for the project, specifying the model provider, model name, task type, model type (classification/regression), inference compute settings, inference settings, and optional assets. Polls for completion and returns when done.</p> <p>Parameters:</p> Name Type Description Default <code>model_provider</code> <code>str</code> <p>model provider name for initialization Model Providers - <code>Hugging Face</code> - <code>OpenAI</code> - <code>Anthropic</code> - <code>Groq</code> - <code>Grok</code> - <code>Gemini</code> - <code>Together</code> - <code>Replicate</code> - <code>Mistral</code> - <code>AWS Bedrock</code> - <code>Open Router</code></p> required <code>model_name</code> <code>str</code> <p>name of the model to be initialized (e.g., meta-llama/Llama-3.2-1B-Instruct).</p> required <code>model_task_type</code> <code>str</code> <p>task type of model Model Task Types - <code>question-answering</code> - <code>summarization</code> - <code>text-classification</code> - <code>text-generation</code> - <code>text2text-generation</code> - <code>token-classification</code></p> required <code>model_architecture</code> <code>str</code> <p>architecture of the model to be initialized Model Architecture - <code>bert</code> - <code>llm</code></p> required <code>inference_compute</code> <code>Optional[InferenceCompute]</code> <p>inference compute configuration used to run the model during inference (e.g., CPU/GPU type, memory, replicas, and other hardware or scaling settings). Required for the Hugging Face provider models, not required for other providers</p> <code>None</code> <code>inference_settings</code> <code>Optional[InferenceSettings]</code> <p>inference runtime settings. Required for the Hugging Face provider models, not required for other providers</p> <code>None</code> <code>assets</code> <code>Optional[dict]</code> <p>assets required for the model, including provider credentials, access tokens, or other secrets needed at runtime (e.g., {\"HF_TOKEN\":\"hf_njbjkfdsnjfkdnskbfk\"}).</p> <code>None</code> <code>requirements_file</code> <code>Optional[str]</code> <p>file path for the requirements file a YAML file defining the runtime environment, including base Docker image, system-level dependencies, and Python packages required for model deployment. Not required for the transformers serverless inference engine.  Example:: image: nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04 system_packages: - build-essential python_packages: - fastapi&gt;=0.115.5 - uvicorn&gt;=0.30.6 - transformers==4.52.3 - pydantic&gt;=2.9.2 - torch==2.7.0 - accelerate==1.8.1</p> <code>None</code> <code>app_file</code> <code>Optional[str]</code> <p>file path for the app file a Python application file that implements the model inference logic, including how inputs are processed and how predictions are generated and returned</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>response</p> Source code in <code>lexsi_sdk/core/text.py</code> <pre><code>def initialize_text_model(\n    self, \n    model_provider: str, \n    model_name: str, \n    model_task_type:str, \n    model_architecture: str,  \n    inference_compute: Optional[InferenceCompute] = None,\n    inference_settings: Optional[InferenceSettings] = None,\n    assets: Optional[dict] = None,\n    requirements_file: Optional[str] = None,\n    app_file: Optional[str] = None\n) -&gt; str:\n    \"\"\"Initialize a text model for the project, specifying the model provider, model name, task type, model type (classification/regression), inference compute settings, inference settings, and optional assets. Polls for completion and returns when done.\n\n    :param model_provider: model provider name for initialization\n        **Model Providers**\n        - ``Hugging Face``\n        - ``OpenAI``\n        - ``Anthropic``\n        - ``Groq``\n        - ``Grok``\n        - ``Gemini``\n        - ``Together``\n        - ``Replicate``\n        - ``Mistral``\n        - ``AWS Bedrock``\n        - ``Open Router``\n\n    :param model_name: name of the model to be initialized\n        (e.g., meta-llama/Llama-3.2-1B-Instruct).\n\n    :param model_task_type: task type of model\n        **Model Task Types**\n        - ``question-answering``\n        - ``summarization``\n        - ``text-classification``\n        - ``text-generation``\n        - ``text2text-generation``\n        - ``token-classification``\n\n    :param model_architecture: architecture of the model to be initialized\n        **Model Architecture**\n        - ``bert``\n        - ``llm``\n\n    :param inference_compute: inference compute configuration used to run the model during inference\n        (e.g., CPU/GPU type, memory, replicas, and other hardware or scaling settings).\n        Required for the Hugging Face provider models, not required for other providers\n    :type inference_compute: InferenceCompute | None\n\n    :param inference_settings: inference runtime settings.\n        Required for the Hugging Face provider models, not required for other providers\n    :type inference_settings: InferenceSettings | None\n\n    :param assets: assets required for the model, including provider credentials, access tokens,\n        or other secrets needed at runtime\n        (e.g., {\"HF_TOKEN\":\"hf_njbjkfdsnjfkdnskbfk\"}).\n\n    :param requirements_file: file path for the requirements file\n        a YAML file defining the runtime environment, including base Docker image,\n        system-level dependencies, and Python packages required for model deployment.\n        Not required for the transformers serverless inference engine.\n\n        Example::\n            image: nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04\n            system_packages:\n                - build-essential\n            python_packages:\n                - fastapi&gt;=0.115.5\n                - uvicorn&gt;=0.30.6\n                - transformers==4.52.3\n                - pydantic&gt;=2.9.2\n                - torch==2.7.0\n                - accelerate==1.8.1\n\n    :param app_file: file path for the app file\n        a Python application file that implements the model inference logic,\n        including how inputs are processed and how predictions are generated and returned\n\n    :return: response\n    \"\"\"\n    data = {\n        \"model_provider\": model_provider,\n        \"model_name\": model_name,\n        \"model_task_type\": model_task_type,\n        \"project_name\": self.project_name,\n        \"model_type\": model_architecture,\n        \"inference_compute\": inference_compute,\n        \"inference_settings\": inference_settings,\n        \"assets\": assets\n    }\n    if inference_compute:\n        data[\"inference_compute\"] = {**inference_compute, \"instance_type\": inference_compute.get(\"compute_type\")}\n\n    payload ={\n        \"data\": (None,json.dumps(data)),\n    }\n    if requirements_file:\n        payload[\"requirements_file\"] = (\"requirements.yaml\", open(requirements_file, \"rb\"))\n    if app_file:\n        payload[\"app_file\"] = (\"app.py\", open(app_file, \"rb\"))\n\n    res = self.api_client.file(f\"{INITIALIZE_TEXT_MODEL_URI}\", payload)\n    if not res.get(\"success\"):\n        raise Exception(res.get(\"details\", \"Model Initialization Failed\"))\n    poll_events(self.api_client, self.project_name, res[\"event_id\"])\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.text.TextProject.llm_monitor","title":"llm_monitor","text":"<pre><code>llm_monitor(client, session_id=None)\n</code></pre> <p>Monitor a custom large language model (LLM) client for inference. Accepts a client object (e.g., an OpenAI API wrapper) and an optional session_id to monitor a specific conversation.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <p>client to monitor like OpenAI</p> required <code>session_id</code> <p>id of the session</p> <code>None</code> <p>Returns:</p> Type Description <p>response</p> Source code in <code>lexsi_sdk/core/text.py</code> <pre><code>def llm_monitor(self, client, session_id=None):\n    \"\"\"Monitor a custom large language model (LLM) client for inference. Accepts a client object (e.g., an OpenAI API wrapper) and an optional session_id to monitor a specific conversation.\n\n    :param client: client to monitor like OpenAI\n    :param session_id: id of the session\n    :return: response\n    \"\"\"\n    return monitor(project=self, client=client, session_id=session_id)\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.text.TextProject.messages","title":"messages","text":"<pre><code>messages(session_id)\n</code></pre> <p>Return a DataFrame listing all messages in a given session. Requires the session_id. Each row corresponds to a single message record.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>UUID of the session (e.g., 10f2510c-17dd-4b99-8926-ef4625513a2f).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>a DataFrame containing all messages for the specified session</p> Source code in <code>lexsi_sdk/core/text.py</code> <pre><code>def messages(self, session_id: str) -&gt; pd.DataFrame:\n    \"\"\"Return a DataFrame listing all messages in a given session. Requires the session_id.\n    Each row corresponds to a single message record.\n\n    :param session_id: UUID of the session\n        (e.g., 10f2510c-17dd-4b99-8926-ef4625513a2f).\n    :return: a DataFrame containing all messages for the specified session\n    \"\"\"\n    res = self.api_client.get(\n        f\"{MESSAGES_URI}?project_name={self.project_name}&amp;session_id={session_id}\"\n    )\n    if not res[\"success\"]:\n        raise Exception(res.get(\"details\"))\n\n    return pd.DataFrame(res.get(\"details\"))\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.text.TextProject.model_inference_settings","title":"model_inference_settings","text":"<pre><code>model_inference_settings(\n    model_name,\n    inference_compute,\n    inference_settings=None,\n    requirements_file=None,\n    app_file=None,\n)\n</code></pre> <p>Configure inference compute and runtime settings for a model.  Only for Hugging Face provider models</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model for inference settings update (e.g., meta-llama/Llama-3.2-1B-Instruct).</p> required <code>inference_compute</code> <code>InferenceCompute</code> <p>Inference compute configuration for the model.</p> required <code>inference_settings</code> <code>Optional[InferenceSettings]</code> <p>Inference runtime settings for the model.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>a response indicating the result of the inference settings configuration</p> Source code in <code>lexsi_sdk/core/text.py</code> <pre><code>def model_inference_settings(\n    self,\n    model_name: str,\n    inference_compute: InferenceCompute,\n    inference_settings: Optional[InferenceSettings] = None,\n    requirements_file: Optional[str] = None,\n    app_file: Optional[str] = None\n) -&gt; str:\n    \"\"\"Configure inference compute and runtime settings for a model.\n     Only for Hugging Face provider models\n\n    :param model_name: Name of the model for inference settings update\n        (e.g., meta-llama/Llama-3.2-1B-Instruct).\n\n    :param inference_compute: Inference compute configuration for the model.\n    :type inference_compute: InferenceCompute | None\n\n    :param inference_settings: Inference runtime settings for the model.\n    :type inference_settings: InferenceSettings | None\n\n    :return: a response indicating the result of the inference settings configuration\n    \"\"\"\n    data = {\n        \"model_name\": model_name,\n        \"project_name\": self.project_name,\n        \"inference_compute\": {**inference_compute, \"instance_type\": inference_compute.get(\"compute_type\")},\n        \"inference_settings\": inference_settings,\n    }\n    payload ={\n        \"data\": (None,json.dumps(data)),\n    }\n    if requirements_file:\n        payload[\"requirements_file\"] = (\"requirements.yaml\", open(requirements_file, \"rb\"))\n    if app_file:\n        payload[\"app_file\"] = (\"app.py\", open(app_file, \"rb\"))\n\n    res = self.api_client.file(f\"{TEXT_MODEL_INFERENCE_SETTINGS_URI}\", payload)\n    if not res.get(\"success\"):\n        raise Exception(res.get(\"details\", \"Failed to update inference settings\"))\n\n    return res.get(\"details\", \"Inference Settings Updated\")\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.text.TextProject.quantize_model","title":"quantize_model","text":"<pre><code>quantize_model(\n    model_name,\n    quant_name,\n    quantization_type,\n    qbit,\n    instance_type,\n    tag=None,\n    input_column=None,\n    no_of_samples=None,\n)\n</code></pre> <p>Quantize a trained model to reduce its size and improve inference efficiency. Requires the model name, quantization method, quantization type,number of bits, and compute instance type.  Optional parameters allow specifying a tag,input column, and number of samples used during quantization.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the base model to be quantized</p> required <code>quant_name</code> <code>str</code> <p>Name of the quantization method to use Quantization Methods - <code>quanto</code> - <code>bnb</code> - <code>hqq</code> - <code>torchao</code> - <code>gptq</code> - <code>awq</code> - <code>llmcomp-awq</code> - <code>llmcomp-gptq</code> - <code>llmcomp-simple</code> - <code>llmcomp-smoothquant</code></p> required <code>quantization_type</code> <code>str</code> <p>Type of quantization to apply Quantization Types - <code>static</code> - <code>dynamic</code></p> required <code>qbit</code> <code>int</code> <p>Number of bits to use for quantization Quantization Bits - <code>4</code> - <code>8</code></p> required <code>instance_type</code> <code>str</code> <p>Instance type used for performing quantization</p> required <code>tag</code> <code>Optional[str]</code> <p>Optional tag name to associate with the quantized model</p> <code>None</code> <code>input_column</code> <code>Optional[str]</code> <p>Optional input column used from the dataset for quantization</p> <code>None</code> <code>no_of_samples</code> <code>Optional[str]</code> <p>Optional number of samples to use for quantization</p> <code>None</code> <p>Returns:</p> Type Description <p>a response indicating the result of the quantization operation</p> Source code in <code>lexsi_sdk/core/text.py</code> <pre><code>def quantize_model(\n    self,\n    model_name: str,\n    quant_name: str,\n    quantization_type: str,\n    qbit: int,\n    instance_type: str,\n    tag: Optional[str] = None,\n    input_column: Optional[str] = None,\n    no_of_samples: Optional[str] = None,\n):\n    \"\"\"Quantize a trained model to reduce its size and improve inference efficiency.\n    Requires the model name, quantization method, quantization type,number of bits, and compute instance type. \n    Optional parameters allow specifying a tag,input column, and number of samples used during quantization.\n\n    :param model_name: Name of the base model to be quantized\n    :param quant_name: Name of the quantization method to use\n        **Quantization Methods**\n        - ``quanto``\n        - ``bnb``\n        - ``hqq``\n        - ``torchao``\n        - ``gptq``\n        - ``awq``\n        - ``llmcomp-awq``\n        - ``llmcomp-gptq``\n        - ``llmcomp-simple``\n        - ``llmcomp-smoothquant``\n    :param quantization_type: Type of quantization to apply\n        **Quantization Types**\n        - ``static``\n        - ``dynamic``\n    :param qbit: Number of bits to use for quantization\n        **Quantization Bits**\n        - ``4``\n        - ``8``\n    :param instance_type: Instance type used for performing quantization\n    :param tag: Optional tag name to associate with the quantized model\n    :param input_column: Optional input column used from the dataset for quantization\n    :param no_of_samples: Optional number of samples to use for quantization\n    :return: a response indicating the result of the quantization operation\n    \"\"\"\n    payload = {\n        \"project_name\": self.project_name,\n        \"model_name\": model_name,\n        \"quant_name\": quant_name,\n        \"quantization_type\": quantization_type,\n        \"qbit\": qbit,\n        \"instance_type\": instance_type,\n        \"tag\": tag,\n        \"input_column\": input_column,\n        \"no_of_samples\": no_of_samples,\n    }\n\n    res = self.api_client.post(QUANTIZE_MODELS_URI, payload)\n    if not res[\"success\"]:\n        raise Exception(res.get(\"details\"))\n\n    poll_events(self.api_client, self.project_name, res.get(\"event_id\"))\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.text.TextProject.sessions","title":"sessions","text":"<pre><code>sessions()\n</code></pre> <p>Return a DataFrame listing all conversation sessions for this text project. Each row corresponds to a single session metadata record.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>a DataFrame containing the conversation session metadata</p> Source code in <code>lexsi_sdk/core/text.py</code> <pre><code>def sessions(self) -&gt; pd.DataFrame:\n    \"\"\"Return a DataFrame listing all conversation sessions for this text project.\n    Each row corresponds to a single session metadata record.\n\n    :return: a DataFrame containing the conversation session metadata\n    \"\"\"\n    res = self.api_client.get(f\"{SESSIONS_URI}?project_name={self.project_name}\")\n    if not res[\"success\"]:\n        raise Exception(res.get(\"details\"))\n\n    return pd.DataFrame(res.get(\"details\"))\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.text.TextProject.traces","title":"traces","text":"<pre><code>traces(trace_id)\n</code></pre> <p>Retrieve the execution traces for a given trace ID and return them as a DataFrame. Each row corresponds to a single trace record.</p> <p>Parameters:</p> Name Type Description Default <code>trace_id</code> <code>str</code> <p>UUID of the trace (e.g., 10f2510c-17dd-4b99-8926-ef4625513a2f).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>a DataFrame containing the execution traces for the specified trace ID</p> Source code in <code>lexsi_sdk/core/text.py</code> <pre><code>def traces(self, trace_id: str) -&gt; pd.DataFrame:\n    \"\"\"Retrieve the execution traces for a given trace ID and return them as a DataFrame.\n    Each row corresponds to a single trace record.\n\n    :param trace_id: UUID of the trace\n        (e.g., 10f2510c-17dd-4b99-8926-ef4625513a2f).\n    :return: a DataFrame containing the execution traces for the specified trace ID\n    \"\"\"\n    res = self.api_client.get(\n        f\"{TRACES_URI}?project_name={self.project_name}&amp;trace_id={trace_id}\"\n    )\n    if not res[\"success\"]:\n        raise Exception(res.get(\"details\"))\n\n    return pd.DataFrame(res.get(\"details\"))\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.text.TextProject.update_guardrail_status","title":"update_guardrail_status","text":"<pre><code>update_guardrail_status(guardrail_id, status)\n</code></pre> <p>Update the status (active or inactive) of a specified guardrail. Requires the guardrail_id and a boolean status value.</p> <p>Parameters:</p> Name Type Description Default <code>guardrail_id</code> <code>str</code> <p>ID of the guardrail</p> required <code>status</code> <code>bool</code> <p>Boolean value indicating whether the guardrail should be active (True) or inactive (False)</p> required <p>Returns:</p> Type Description <code>str</code> <p>a response indicating the result of the update operation</p> Source code in <code>lexsi_sdk/core/text.py</code> <pre><code>def update_guardrail_status(self, guardrail_id: str, status: bool) -&gt; str:\n    \"\"\"Update the status (active or inactive) of a specified guardrail.\n    Requires the guardrail_id and a boolean status value.\n\n    :param guardrail_id: ID of the guardrail\n    :param status: Boolean value indicating whether the guardrail should be active (True) or inactive (False)\n    :return: a response indicating the result of the update operation\n    \"\"\"\n\n    payload = {\n        \"project_name\": self.project_name,\n        \"guardrail_id\": guardrail_id,\n        \"status\": status,\n    }\n    res = self.api_client.post(UPDATE_GUARDRAILS_STATUS_URI, payload=payload)\n    if not res[\"success\"]:\n        raise Exception(res.get(\"details\"))\n\n    return res.get(\"details\")\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.text.TextProject.update_inference_model_status","title":"update_inference_model_status","text":"<pre><code>update_inference_model_status(model_name, activate)\n</code></pre> <p>Sets the provided model to active for inferencing</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>name of the model</p> required <p>Returns:</p> Type Description <code>str</code> <p>response</p> Source code in <code>lexsi_sdk/core/text.py</code> <pre><code>def update_inference_model_status(self, model_name: str, activate: bool) -&gt; str:\n    \"\"\"Sets the provided model to active for inferencing\n\n    :param model_name: name of the model\n    :return: response\n    \"\"\"\n    payload = {\n        \"project_name\": self.project_name,\n        \"model_name\": model_name,\n        \"activate\": activate,\n    }\n\n    res = self.api_client.post(UPDATE_ACTIVE_INFERENCE_MODEL_URI, payload)\n\n    if not res[\"success\"]:\n        raise Exception(res[\"details\"])\n\n    return res.get(\"details\")\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.text.TextProject.upload_data","title":"upload_data","text":"<pre><code>upload_data(data, tag)\n</code></pre> <p>Upload text data to the project by specifying either a file path or a pandas DataFrame and a tag. Handles conversion to CSV for DataFrame uploads and returns the API response.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str | DataFrame</code> <p>File path or pandas DataFrame containing the rows to upload</p> required <code>tag</code> <code>str</code> <p>Tag to associate with the uploaded data</p> required <p>Returns:</p> Type Description <code>str</code> <p>a response containing the server\u2019s upload result</p> Source code in <code>lexsi_sdk/core/text.py</code> <pre><code>def upload_data(\n    self,\n    data: str | pd.DataFrame,\n    tag: str,\n) -&gt; str:\n    \"\"\"Upload text data to the project by specifying either a file path or a pandas DataFrame and a tag.\n    Handles conversion to CSV for DataFrame uploads and returns the API response.\n\n    :param data: File path or pandas DataFrame containing the rows to upload\n    :param tag: Tag to associate with the uploaded data\n    :return: a response containing the server\u2019s upload result\n    \"\"\"\n\n    def build_upload_data(data):\n        \"\"\"Prepare file payload from path or DataFrame.\n\n        :param data: File path or DataFrame to convert.\n        :return: Tuple or file handle suitable for multipart upload.\n        \"\"\"\n        if isinstance(data, str):\n            file = open(data, \"rb\")\n            return file\n        elif isinstance(data, pd.DataFrame):\n            csv_buffer = io.BytesIO()\n            data.to_csv(csv_buffer, index=False, encoding=\"utf-8\")\n            csv_buffer.seek(0)\n            file_name = f\"{tag}_sdk_{datetime.now().replace(microsecond=0)}.csv\"\n            file = (file_name, csv_buffer.getvalue())\n            return file\n        else:\n            raise Exception(\"Invalid Data Type\")\n\n    def upload_file_and_return_path(data, data_type, tag=None) -&gt; str:\n        \"\"\"Upload a file and return the stored path.\n\n        :param data: Data payload (path or DataFrame).\n        :param data_type: Type of data being uploaded.\n        :param tag: Optional tag.\n        :return: File path stored on the server.\n        \"\"\"\n        files = {\"in_file\": build_upload_data(data)}\n        res = self.api_client.file(\n            f\"{UPLOAD_DATA_FILE_URI}?project_name={self.project_name}&amp;data_type={data_type}&amp;tag={tag}\",\n            files,\n        )\n\n        if not res[\"success\"]:\n            raise Exception(res.get(\"details\"))\n        uploaded_path = res.get(\"metadata\").get(\"filepath\")\n\n        return uploaded_path\n\n    uploaded_path = upload_file_and_return_path(data, \"data\", tag)\n\n    payload = {\n        \"path\": uploaded_path,\n        \"tag\": tag,\n        \"type\": \"data\",\n        \"project_name\": self.project_name,\n    }\n    res = self.api_client.post(UPLOAD_DATA_URI, payload)\n\n    if not res[\"success\"]:\n        self.delete_file(uploaded_path)\n        raise Exception(res.get(\"details\"))\n\n    return res.get(\"details\")\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.text.TextProject.upload_data_dataconnectors","title":"upload_data_dataconnectors","text":"<pre><code>upload_data_dataconnectors(\n    data_connector_name,\n    tag,\n    bucket_name=None,\n    file_path=None,\n    dataset_name=None,\n)\n</code></pre> <p>Upload text data stored in a configured data connector (such as S3 or GCS). Requires the connector name, a tag, and optionally the bucket name and file path. Returns the API response.</p> <p>Parameters:</p> Name Type Description Default <code>data_connector_name</code> <code>str</code> <p>Name of the configured data connector</p> required <code>tag</code> <code>str</code> <p>Tag to associate with the uploaded data</p> required <code>bucket_name</code> <code>Optional[str]</code> <p>Name of the bucket or storage location, if required by the connector</p> <code>None</code> <code>file_path</code> <code>Optional[str]</code> <p>File path within the connector storage</p> <code>None</code> <code>dataset_name</code> <code>Optional[str]</code> <p>Optional dataset name to persist the uploaded data</p> <code>None</code> <p>Returns:</p> Type Description <p>a response containing the server\u2019s upload result</p> Source code in <code>lexsi_sdk/core/text.py</code> <pre><code>def upload_data_dataconnectors(\n    self,\n    data_connector_name: str,\n    tag: str,\n    bucket_name: Optional[str] = None,\n    file_path: Optional[str] = None,\n    dataset_name: Optional[str] = None,\n):\n    \"\"\"Upload text data stored in a configured data connector (such as S3 or GCS).\n    Requires the connector name, a tag, and optionally the bucket name and file path.\n    Returns the API response.\n\n    :param data_connector_name: Name of the configured data connector\n    :param tag: Tag to associate with the uploaded data\n    :param bucket_name: Name of the bucket or storage location, if required by the connector\n    :param file_path: File path within the connector storage\n    :param dataset_name: Optional dataset name to persist the uploaded data\n    :return: a response containing the server\u2019s upload result\n    \"\"\"\n\n    def get_connector() -&gt; str | pd.DataFrame:\n        \"\"\"Fetch connector metadata for the requested link service.\n\n        :return: DataFrame of connector info or error string.\n        \"\"\"\n        url = build_list_data_connector_url(\n            LIST_DATA_CONNECTORS, self.project_name, self.organization_id\n        )\n        res = self.api_client.post(url)\n\n        if res[\"success\"]:\n            df = pd.DataFrame(res[\"details\"])\n            filtered_df = df.loc[df[\"link_service_name\"] == data_connector_name]\n            if filtered_df.empty:\n                return \"No data connector found\"\n            return filtered_df\n\n        return res[\"details\"]\n\n    connectors = get_connector()\n    if isinstance(connectors, pd.DataFrame):\n        value = connectors.loc[\n            connectors[\"link_service_name\"] == data_connector_name,\n            \"link_service_type\",\n        ].values[0]\n        ds_type = value\n\n        if ds_type == \"s3\" or ds_type == \"gcs\":\n            if not bucket_name:\n                return \"Missing argument bucket_name\"\n            if not file_path:\n                return \"Missing argument file_path\"\n    else:\n        return connectors\n\n    def upload_file_and_return_path(file_path, data_type, tag=None) -&gt; str:\n        \"\"\"Upload a file from connector storage and return stored path.\n\n        :param file_path: Path within the connector store.\n        :param data_type: Type of data being uploaded.\n        :param tag: Optional tag for the upload.\n        :return: Stored file path returned by the API.\n        \"\"\"\n        if not self.project_name:\n            return \"Missing Project Name\"\n        query_params = f\"project_name={self.project_name}&amp;link_service_name={data_connector_name}&amp;data_type={data_type}&amp;tag={tag}&amp;bucket_name={bucket_name}&amp;file_path={file_path}&amp;dataset_name={dataset_name}\"\n        if self.organization_id:\n            query_params += f\"&amp;organization_id={self.organization_id}\"\n        res = self.api_client.post(f\"{UPLOAD_FILE_DATA_CONNECTORS}?{query_params}\")\n        if not res[\"success\"]:\n            raise Exception(res.get(\"details\"))\n        uploaded_path = res.get(\"metadata\").get(\"filepath\")\n\n        return uploaded_path\n\n    uploaded_path = upload_file_and_return_path(file_path, \"data\", tag)\n\n    payload = {\n        \"path\": uploaded_path,\n        \"tag\": tag,\n        \"type\": \"data\",\n        \"project_name\": self.project_name,\n    }\n    res = self.api_client.post(UPLOAD_DATA_URI, payload)\n\n    if not res[\"success\"]:\n        self.delete_file(uploaded_path)\n        raise Exception(res.get(\"details\"))\n\n    return res.get(\"details\")\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.tracer.Tracer","title":"Tracer","text":"<pre><code>Tracer()\n</code></pre> <p>Helpers to instrument various agent frameworks with OpenTelemetry.</p> <p>Initialize exporter endpoint from environment. Stores configuration and prepares the object for use.</p> Source code in <code>lexsi_sdk/core/tracer.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize exporter endpoint from environment.\n    Stores configuration and prepares the object for use.\"\"\"\n    self.base_url = os.getenv(\"XAI_API_URL\", \"https://apiv1.lexsi.ai\")\n    self.endpoint = f\"{self.base_url}\"\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.tracer.Tracer.setup_agents_tracing","title":"setup_agents_tracing","text":"<pre><code>setup_agents_tracing(project, session_id=None)\n</code></pre> <p>Sets up OpenTelemetry tracing for a given project with OTLP and console exporters.</p> <p>Parameters:</p> Name Type Description Default <code>project</code> <code>object</code> <p>Object containing project details; must expose project_name.</p> required <code>session_id</code> <code>str</code> <p>Optional session identifier to annotate spans.</p> <code>None</code> Source code in <code>lexsi_sdk/core/tracer.py</code> <pre><code>def setup_agents_tracing(self, project: object, session_id: str = None) -&gt; None:\n    \"\"\"\n    Sets up OpenTelemetry tracing for a given project with OTLP and console exporters.\n\n    :param project: Object containing project details; must expose project_name.\n    :param session_id: Optional session identifier to annotate spans.\n    \"\"\"\n\n    # Extract project name or use default\n\n    project_name = getattr(project, \"project_name\")\n    # Create resource with service and project details\n    resource = Resource(\n        attributes={\n            \"service.name\": \"OpenAI-Agents\",\n            \"project_name\": project_name,\n            \"session_id\": session_id if session_id else \"None\",\n        }\n    )\n\n    # Initialize tracer provider\n    tracer_provider = trace_sdk.TracerProvider(resource=resource)\n    trace_api.set_tracer_provider(tracer_provider)\n\n    # Add OTLP and console span processors\n    tracer_provider.add_span_processor(\n        SimpleSpanProcessor(OTLPSpanExporter(self.endpoint))\n    )\n    # tracer_provider.add_span_processor(ConsoleSpanExporter())\n    # Instrument OpenAI\n    OpenAIAgentsInstrumentor().instrument()\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.tracer.Tracer.setup_autogen_tracing","title":"setup_autogen_tracing","text":"<pre><code>setup_autogen_tracing(project, session_id=None)\n</code></pre> <p>Sets up OpenTelemetry tracing for a given project with OTLP and console exporters.</p> <p>Parameters:</p> Name Type Description Default <code>project</code> <code>object</code> <p>Object containing project details; must expose project_name.</p> required <code>session_id</code> <code>str</code> <p>Optional session identifier to annotate spans.</p> <code>None</code> Source code in <code>lexsi_sdk/core/tracer.py</code> <pre><code>def setup_autogen_tracing(self, project: object, session_id: str = None) -&gt; None:\n    \"\"\"\n    Sets up OpenTelemetry tracing for a given project with OTLP and console exporters.\n\n    :param project: Object containing project details; must expose project_name.\n    :param session_id: Optional session identifier to annotate spans.\n    \"\"\"\n\n    # Extract project name or use default\n\n    project_name = getattr(project, \"project_name\")\n    # Create resource with service and project details\n    resource = Resource(\n        attributes={\n            \"service.name\": \"Autogen\",\n            \"project_name\": project_name,\n            \"session_id\": session_id if session_id else \"None\",\n        }\n    )\n\n    # Initialize tracer provider\n    tracer_provider = trace_sdk.TracerProvider(resource=resource)\n    trace_api.set_tracer_provider(tracer_provider)\n\n    # Add OTLP and console span processors\n    tracer_provider.add_span_processor(\n        SimpleSpanProcessor(OTLPSpanExporter(self.endpoint))\n    )\n    # Instrument Autogen\n    AutogenAgentChatInstrumentor().instrument()\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.tracer.Tracer.setup_crewai_tracing","title":"setup_crewai_tracing","text":"<pre><code>setup_crewai_tracing(project, session_id=None)\n</code></pre> <p>Sets up OpenTelemetry tracing for a given project with OTLP and console exporters.</p> <p>Parameters:</p> Name Type Description Default <code>project</code> <code>object</code> <p>Object containing project details; must expose project_name.</p> required <code>session_id</code> <code>str</code> <p>Optional session identifier to annotate spans.</p> <code>None</code> Source code in <code>lexsi_sdk/core/tracer.py</code> <pre><code>def setup_crewai_tracing(self, project: object, session_id: str = None) -&gt; None:\n    \"\"\"\n    Sets up OpenTelemetry tracing for a given project with OTLP and console exporters.\n\n    :param project: Object containing project details; must expose project_name.\n    :param session_id: Optional session identifier to annotate spans.\n    \"\"\"\n\n    # Extract project name or use default\n\n    project_name = getattr(project, \"project_name\")\n    # Create resource with service and project details\n    resource = Resource(\n        attributes={\n            \"service.name\": \"Crewai\",\n            \"project_name\": project_name,\n            \"session_id\": session_id if session_id else \"None\",\n        }\n    )\n\n    # Initialize tracer provider\n    tracer_provider = trace_sdk.TracerProvider(resource=resource)\n    trace_api.set_tracer_provider(tracer_provider)\n\n    # Add OTLP and console span processors\n    tracer_provider.add_span_processor(\n        SimpleSpanProcessor(OTLPSpanExporter(self.endpoint))\n    )\n    # tracer_provider.add_span_processor(ConsoleSpanExporter())\n    # Instrument CrewAI\n    CrewAIInstrumentor().instrument()\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.tracer.Tracer.setup_dspy_tracing","title":"setup_dspy_tracing","text":"<pre><code>setup_dspy_tracing(project, session_id=None)\n</code></pre> <p>Sets up OpenTelemetry tracing for a given project with OTLP and console exporters.</p> <p>Parameters:</p> Name Type Description Default <code>project</code> <code>object</code> <p>Object containing project details; must expose project_name.</p> required <code>session_id</code> <code>str</code> <p>Optional session identifier to annotate spans.</p> <code>None</code> Source code in <code>lexsi_sdk/core/tracer.py</code> <pre><code>def setup_dspy_tracing(self, project: object, session_id: str = None) -&gt; None:\n    \"\"\"\n    Sets up OpenTelemetry tracing for a given project with OTLP and console exporters.\n\n    :param project: Object containing project details; must expose project_name.\n    :param session_id: Optional session identifier to annotate spans.\n    \"\"\"\n\n    # Extract project name or use default\n\n    project_name = getattr(project, \"project_name\")\n    # Create resource with service and project details\n    resource = Resource(\n        attributes={\n            \"service.name\": \"DSPy\",\n            \"project_name\": project_name,\n            \"session_id\": session_id if session_id else \"None\",\n        }\n    )\n\n    # Initialize tracer provider\n    tracer_provider = trace_sdk.TracerProvider(resource=resource)\n    trace_api.set_tracer_provider(tracer_provider)\n\n    # Add OTLP and console span processors\n    tracer_provider.add_span_processor(\n        SimpleSpanProcessor(OTLPSpanExporter(self.endpoint))\n    )\n    # tracer_provider.add_span_processor(ConsoleSpanExporter())\n    # Instrument DSPy\n    DSPyInstrumentor().instrument()\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.tracer.Tracer.setup_langchain_tracing","title":"setup_langchain_tracing","text":"<pre><code>setup_langchain_tracing(project, session_id=None)\n</code></pre> <p>Sets up OpenTelemetry tracing for a given project with OTLP and console exporters.</p> <p>Parameters:</p> Name Type Description Default <code>project</code> <code>object</code> <p>Object containing project details; must expose project_name.</p> required <code>session_id</code> <code>str</code> <p>Optional session identifier to annotate spans.</p> <code>None</code> Source code in <code>lexsi_sdk/core/tracer.py</code> <pre><code>def setup_langchain_tracing(self, project: object, session_id: str = None) -&gt; None:\n    \"\"\"\n    Sets up OpenTelemetry tracing for a given project with OTLP and console exporters.\n\n    :param project: Object containing project details; must expose project_name.\n    :param session_id: Optional session identifier to annotate spans.\n    \"\"\"\n\n    # Extract project name or use default\n\n    project_name = getattr(project, \"project_name\")\n    # Create resource with service and project details\n    resource = Resource(\n        attributes={\n            \"service.name\": \"Langgraph\",\n            \"project_name\": project_name,\n            \"session_id\": session_id if session_id else \"None\",\n        }\n    )\n\n    # Initialize tracer provider\n    tracer_provider = trace_sdk.TracerProvider(resource=resource)\n    trace_api.set_tracer_provider(tracer_provider)\n\n    # Add OTLP and console span processors\n    tracer_provider.add_span_processor(\n        SimpleSpanProcessor(OTLPSpanExporter(self.endpoint))\n    )\n    # Instrument LangChain\n    LangChainInstrumentor().instrument()\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.tracer.Tracer.setup_llamaindex_tracing","title":"setup_llamaindex_tracing","text":"<pre><code>setup_llamaindex_tracing(project, session_id=None)\n</code></pre> <p>Enable tracing for LlamaIndex runs. Encapsulates a small unit of SDK logic and returns the computed result.</p> Source code in <code>lexsi_sdk/core/tracer.py</code> <pre><code>def setup_llamaindex_tracing(self, project: object, session_id: str = None) -&gt; None:\n    \"\"\"Enable tracing for LlamaIndex runs.\n    Encapsulates a small unit of SDK logic and returns the computed result.\"\"\"\n\n    # Extract project name or use default\n\n    project_name = getattr(project, \"project_name\")\n    # Create resource with service and project details\n    resource = Resource(\n        attributes={\n            \"service.name\": \"Llamaindex\",\n            \"project_name\": project_name,\n            \"session_id\": session_id if session_id else \"None\",\n        }\n    )\n\n    # Initialize tracer provider\n    tracer_provider = trace_sdk.TracerProvider(resource=resource)\n    trace_api.set_tracer_provider(tracer_provider)\n\n    # Add OTLP and console span processors\n    tracer_provider.add_span_processor(\n        SimpleSpanProcessor(OTLPSpanExporter(self.endpoint))\n    )\n    # tracer_provider.add_span_processor(ConsoleSpanExporter())\n    # Instrument llama\n    LlamaIndexInstrumentor().instrument()\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.tracer.Tracer.setup_smolagents_tracing","title":"setup_smolagents_tracing","text":"<pre><code>setup_smolagents_tracing(project, session_id=None)\n</code></pre> <p>Enable tracing for Smolagents runs. Encapsulates a small unit of SDK logic and returns the computed result.</p> Source code in <code>lexsi_sdk/core/tracer.py</code> <pre><code>def setup_smolagents_tracing(self, project: object, session_id: str = None) -&gt; None:\n    \"\"\"Enable tracing for Smolagents runs.\n    Encapsulates a small unit of SDK logic and returns the computed result.\"\"\"\n\n    # Extract project name or use default\n\n    project_name = getattr(project, \"project_name\")\n    # Create resource with service and project details\n    resource = Resource(\n        attributes={\n            \"service.name\": \"Smolagents\",\n            \"project_name\": project_name,\n            \"session_id\": session_id if session_id else \"None\",\n        }\n    )\n\n    # Initialize tracer provider\n    tracer_provider = trace_sdk.TracerProvider(resource=resource)\n    trace_api.set_tracer_provider(tracer_provider)\n\n    # Add OTLP and console span processors\n    tracer_provider.add_span_processor(\n        SimpleSpanProcessor(OTLPSpanExporter(self.endpoint))\n    )\n    # tracer_provider.add_span_processor(ConsoleSpanExporter())\n    # Instrument Smol\n    SmolagentsInstrumentor().instrument()\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.model_summary.ModelSummary","title":"ModelSummary","text":"<pre><code>ModelSummary(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Provides high-level summaries of trained models including features, metrics, drift indicators, and metadata for quick inspection.</p> <p>Store API client reference for subsequent calls. Stores configuration and prepares the object for use.</p> Source code in <code>lexsi_sdk/core/model_summary.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Store API client reference for subsequent calls.\n    Stores configuration and prepares the object for use.\"\"\"\n    super().__init__(**kwargs)\n    self.api_client = kwargs.get(\"api_client\")\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.model_summary.ModelSummary.data_config","title":"data_config","text":"<pre><code>data_config()\n</code></pre> <p>Return the data configuration used for the project (e.g., feature exclusions and encodings) by calling the API.</p> <p>Returns:</p> Type Description <code>dict</code> <p>response</p> Source code in <code>lexsi_sdk/core/model_summary.py</code> <pre><code>def data_config(self) -&gt; dict:\n    \"\"\"Return the data configuration used for the project (e.g., feature exclusions and encodings) by calling the API.\n\n    :return: response\n    \"\"\"\n    model_name = self.model_results.get(\"model_name\")\n    res = self.api_client.get(\n        f\"{GET_PROJECT_CONFIG}?project_name={self.project_name}&amp;model_name={model_name}\"\n    )\n    if res.get(\"details\") != \"Not Found\":\n        res[\"details\"].pop(\"updated_by\")\n        res[\"details\"][\"metadata\"].pop(\"path\")\n        res[\"details\"][\"metadata\"].pop(\"avaialble_tags\")\n\n    return res.get(\"details\")\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.model_summary.ModelSummary.feature_importance","title":"feature_importance","text":"<pre><code>feature_importance(xai_method)\n</code></pre> <p>Plot global feature importance for the model using the specified explainability method (SHAP or LIME).</p> Source code in <code>lexsi_sdk/core/model_summary.py</code> <pre><code>def feature_importance(self, xai_method: str):\n    \"\"\"Plot global feature importance for the model using the specified explainability method (SHAP or LIME).\"\"\"\n    global_features = None\n    if xai_method == \"shap\":\n        global_features = self.model_results.get(\"GFI\", {}).get(\"shap_gfi\", None)\n    if xai_method == \"lime\":\n        global_features = self.model_results.get(\"GFI\", {}).get(\"lime_gfi\", None)\n    # if not global_features:\n    #     global_features = self.model_results.get(\"GFI\")\n    if not global_features:\n        return f\"No feature importance found for {xai_method}\"\n    fig = go.Figure()\n\n    fig.add_trace(\n        go.Bar(\n            y=list(global_features.keys()),\n            x=list(global_features.values()),\n            orientation=\"h\",\n        )\n    )\n\n    fig.update_layout(\n        title=\"Global Feaure\",\n        xaxis_title=\"Values\",\n        yaxis_title=\"Features\",\n        width=800,\n        height=600,\n        yaxis_autorange=\"reversed\",\n        bargap=0.01,\n        legend_orientation=\"h\",\n        legend_x=0.1,\n        legend_y=1.1,\n    )\n\n    fig.show(config={\"displaylogo\": False})\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.model_summary.ModelSummary.info","title":"info","text":"<pre><code>info()\n</code></pre> <p>Return a dictionary summarizing model details such as source, name, type, parameters, data tags used for modeling, and modelling info.</p> <p>Returns:</p> Type Description <code>dict</code> <p>model info dict</p> Source code in <code>lexsi_sdk/core/model_summary.py</code> <pre><code>def info(self) -&gt; dict:\n    \"\"\"Return a dictionary summarizing model details such as source, name, type, parameters, data tags used for modeling, and modelling info.\n\n    :return: model info dict\n    \"\"\"\n    info = {\n        \"source\": self.Source,\n        \"model_name\": self.model_results.get(\"model_name\"),\n        \"model_type\": self.model_results.get(\"model_type\"),\n        \"model_param\": self.model_results.get(\"model_params\"),\n        \"data_tags_used_for_modelling\": self.model_results.get(\"data_used_tags\"),\n        \"modelling_info\": self.model_results.get(\"modelling_info\"),\n    }\n\n    return info\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.model_summary.ModelSummary.prediction_path","title":"prediction_path","text":"<pre><code>prediction_path()\n</code></pre> <p>Display the model\u2019s prediction path as an SVG for the current case, retrieving it from the API.</p> Source code in <code>lexsi_sdk/core/model_summary.py</code> <pre><code>def prediction_path(self):\n    \"\"\"Display the model\u2019s prediction path as an SVG for the current case, retrieving it from the API.\"\"\"\n    model_name = self.model_results.get(\"model_name\")\n    res = self.api_client.get(\n        f\"{MODEL_SVG_URI}?project_name={self.project_name}&amp;model_name={model_name}\"\n    )\n\n    if not res[\"success\"]:\n        raise Exception(res.get(\"details\"))\n\n    svg = SVG(res.get(\"details\"))\n    display(svg)\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.synthetic.SyntheticDataTag","title":"SyntheticDataTag","text":"<pre><code>SyntheticDataTag(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Represents metadata for synthetic datasets generated within Lexsi. Used to track lineage, configuration, and dataset properties.</p> <p>Bind API client reference for follow-up requests. Stores configuration and prepares the object for use.</p> Source code in <code>lexsi_sdk/core/synthetic.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Bind API client reference for follow-up requests.\n    Stores configuration and prepares the object for use.\"\"\"\n    super().__init__(**kwargs)\n    self.api_client = kwargs.get(\"api_client\")\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.synthetic.SyntheticDataTag.get_metadata","title":"get_metadata","text":"<pre><code>get_metadata()\n</code></pre> <p>Return the metadata dictionary for the synthetic data tag.</p> Source code in <code>lexsi_sdk/core/synthetic.py</code> <pre><code>def get_metadata(self) -&gt; dict:\n    \"\"\"Return the metadata dictionary for the synthetic data tag.\"\"\"\n\n    return self.metadata\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.synthetic.SyntheticDataTag.get_model_name","title":"get_model_name","text":"<pre><code>get_model_name()\n</code></pre> <p>Return the name of the synthetic model associated with the tag.</p> <p>Returns:</p> Type Description <code>str</code> <p>model type</p> Source code in <code>lexsi_sdk/core/synthetic.py</code> <pre><code>def get_model_name(self) -&gt; str:\n    \"\"\"Return the name of the synthetic model associated with the tag.\n\n    :return: model type\n    \"\"\"\n    return self.model_name\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.synthetic.SyntheticDataTag.view_metadata","title":"view_metadata","text":"<pre><code>view_metadata()\n</code></pre> <p>Pretty-print the metadata associated with the synthetic data tag using JSON indentation.</p> Source code in <code>lexsi_sdk/core/synthetic.py</code> <pre><code>def view_metadata(self) -&gt; dict:\n    \"\"\"Pretty-print the metadata associated with the synthetic data tag using JSON indentation.\"\"\"\n\n    print(json.dumps(self.metadata, indent=4))\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.synthetic.SyntheticModel","title":"SyntheticModel","text":"<pre><code>SyntheticModel(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Represents a synthetic model configuration used for data generation. Exposes model parameters and generation statistics.</p> <p>Bind API client reference for this synthetic model. Stores configuration and prepares the object for use.</p> Source code in <code>lexsi_sdk/core/synthetic.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Bind API client reference for this synthetic model.\n    Stores configuration and prepares the object for use.\"\"\"\n    super().__init__(**kwargs)\n    self.api_client = kwargs.get(\"api_client\")\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.synthetic.SyntheticModel.anonymity_score","title":"anonymity_score","text":"<pre><code>anonymity_score()\n</code></pre> <p>get anonymity score</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>anonymity score dataframe</p> <p>Raises:</p> Type Description <code>Exception</code> <p>description</p> Source code in <code>lexsi_sdk/core/synthetic.py</code> <pre><code>def anonymity_score(self) -&gt; pd.DataFrame:\n    \"\"\"get anonymity score\n    :return: anonymity score dataframe\n    :raises Exception: _description_\n    \"\"\"\n    payload = {\n        \"project_name\": self.project_name,\n        \"model_name\": self.model_name,\n    }\n\n    res = self.api_client.post(GET_ANONYMITY_SCORE_URI, payload)\n\n    if not res[\"success\"]:\n        print(res[\"details\"])\n        raise Exception(\"Error while getting anonymity score.\")\n\n    print(\"metadata:\")\n    print(res[\"details\"][\"metadata\"])\n    print(\"\\n\")\n\n    return pd.DataFrame(res[\"details\"][\"scores\"], index=[0])\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.synthetic.SyntheticModel.generate_anonymity_score","title":"generate_anonymity_score","text":"<pre><code>generate_anonymity_score(aux_columns, control_tag, node)\n</code></pre> <p>generate anonymity score</p> <p>Parameters:</p> Name Type Description Default <code>aux_columns</code> <code>List[str]</code> <p>list of features</p> required <code>control_tag</code> <code>str</code> <p>tag</p> required <code>node</code> <code>str</code> <p>type of node to run training for all available node check lexsi.available_node_servers(type=\"GPU\") defaults to shared</p> required <p>Returns:</p> Type Description <code>str</code> <p>Response message</p> Source code in <code>lexsi_sdk/core/synthetic.py</code> <pre><code>def generate_anonymity_score(\n    self,\n    aux_columns: List[str],\n    control_tag: str,\n    node: str,\n) -&gt; str:\n    \"\"\"generate anonymity score\n\n    :param aux_columns: list of features\n    :param control_tag: tag\n    :param node: type of node to run training\n        for all available node check lexsi.available_node_servers(type=\"GPU\")\n        defaults to shared\n\n    :return: Response message\n    \"\"\"\n    if node != \"shared\":\n        available_servers = self.api_client.get(\n            AVAILABLE_SYNTHETIC_CUSTOM_SERVERS_URI\n        )[\"details\"]\n        servers = list(\n            map(lambda instance: instance[\"instance_name\"], available_servers)\n        )\n        Validate.value_against_list(\"instance_type\", node, servers)\n\n    if len(aux_columns) &lt; 2:\n        raise Exception(\"aux_columns requires minimum 2 columns.\")\n\n    project_config = self.project.config()[\"metadata\"]\n\n    Validate.value_against_list(\n        \"feature\", aux_columns, project_config[\"feature_include\"]\n    )\n\n    all_tags = self.project.all_tags()\n\n    Validate.value_against_list(\"tag\", [control_tag], all_tags)\n\n    payload = {\n        \"aux_columns\": aux_columns,\n        \"control_tag\": control_tag,\n        \"model_name\": self.model_name,\n        \"project_name\": self.project_name,\n        \"instance_type\": node,\n    }\n\n    res = self.api_client.post(GENERATE_ANONYMITY_SCORE_URI, payload)\n\n    if not res[\"success\"]:\n        raise Exception(res[\"details\"])\n\n    print(\"Calculating anonymity score...\")\n    poll_events(self.api_client, self.project_name, res[\"event_id\"])\n    print(\"Anonymity score calculated successfully.\\n\")\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.synthetic.SyntheticModel.generate_synthetic_data","title":"generate_synthetic_data","text":"<pre><code>generate_synthetic_data(num_of_datapoints, node)\n</code></pre> <p>Generate a specified number of synthetic data points using the model. Accepts the number of data points and an optional instance_type for compute resources. If instance_type is not shared, checks available servers and raises errors for invalid values.</p> <p>Parameters:</p> Name Type Description Default <code>num_of_datapoints</code> <code>int</code> <p>total datapoints to generate</p> required <code>node</code> <code>str</code> <p>type of node to run training for all available nodes check lexsi.available_node_servers() defaults to shared</p> required <p>Returns:</p> Type Description <code>str</code> <p>Response message</p> Source code in <code>lexsi_sdk/core/synthetic.py</code> <pre><code>def generate_synthetic_data(\n    self, num_of_datapoints: int, node: str\n) -&gt; str:\n    \"\"\"Generate a specified number of synthetic data points using the model. Accepts the number of data points and an optional instance_type for compute resources. If instance_type is not shared, checks available servers and raises errors for invalid values.\n\n    :param num_of_datapoints: total datapoints to generate\n    :param node: type of node to run training\n        for all available nodes check lexsi.available_node_servers()\n        defaults to shared\n    :return: Response message\n    \"\"\"\n    if node != \"shared\":\n        available_servers = self.api_client.get(\n            AVAILABLE_SYNTHETIC_CUSTOM_SERVERS_URI\n        )[\"details\"]\n        servers = list(\n            map(lambda instance: instance[\"instance_name\"], available_servers)\n        )\n        Validate.value_against_list(\"instance_type\", node, servers)\n\n    payload = {\n        \"project_name\": self.project_name,\n        \"model_name\": self.model_name,\n        \"instance_type\": node,\n        \"num_of_datapoints\": num_of_datapoints,\n    }\n\n    res = self.api_client.post(GENERATE_SYNTHETIC_DATA_URI, payload)\n\n    if not res[\"success\"]:\n        raise Exception(res[\"details\"])\n\n    print(\"Generating synthetic datapoints...\")\n    poll_events(\n        self.api_client,\n        self.project_name,\n        res[\"event_id\"],\n        progress_message=\"Synthetic Data generation progress\",\n    )\n    print(\"Synthetic datapoints generated successfully.\\n\")\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.synthetic.SyntheticModel.get_data_quality","title":"get_data_quality","text":"<pre><code>get_data_quality()\n</code></pre> <p>Return a DataFrame summarizing the overall synthetic data quality, including scores for column shapes and column pair trends.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>data quality metrics</p> Source code in <code>lexsi_sdk/core/synthetic.py</code> <pre><code>def get_data_quality(self) -&gt; pd.DataFrame:\n    \"\"\"Return a DataFrame summarizing the overall synthetic data quality, including scores for column shapes and column pair trends.\n\n    :return: data quality metrics\n    \"\"\"\n    quality = {\n        \"overall_quality_score\": self.overall_quality_score,\n        \"column_shapes\": self.column_shapes,\n        \"column_pair_trends\": self.column_pair_trends,\n    }\n\n    df = pd.DataFrame(quality, index=[0])\n\n    return df\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.synthetic.SyntheticModel.get_model_type","title":"get_model_type","text":"<pre><code>get_model_type()\n</code></pre> <p>Return the model type recorded in the metadata dictionary for the synthetic model (e.g., GAN, VAE).</p> <p>Returns:</p> Type Description <code>str</code> <p>model type</p> Source code in <code>lexsi_sdk/core/synthetic.py</code> <pre><code>def get_model_type(self) -&gt; str:\n    \"\"\"Return the model type recorded in the metadata dictionary for the synthetic model (e.g., GAN, VAE).\n\n    :return: model type\n    \"\"\"\n    return self.metadata[\"model_name\"]\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.synthetic.SyntheticModel.quality_plot","title":"quality_plot","text":"<pre><code>quality_plot()\n</code></pre> <p>Plot a PSI chart of synthetic data quality across different metrics (columns, quality scores, metric names).</p> Source code in <code>lexsi_sdk/core/synthetic.py</code> <pre><code>def quality_plot(self):\n    \"\"\"Plot a PSI chart of synthetic data quality across different metrics (columns, quality scores, metric names).\"\"\"\n    x_data = [item[\"Column\"] for item in self.plot_data]\n    y_data = [item[\"Quality Score\"] for item in self.plot_data]\n    metric_data = [item[\"Metric\"] for item in self.plot_data]\n\n    traces = []\n    for metric in set(metric_data):\n        indices = [i for i, val in enumerate(metric_data) if val == metric]\n        traces.append(\n            go.Bar(\n                x=[x_data[i] for i in indices],\n                y=[y_data[i] for i in indices],\n                name=metric,\n            )\n        )\n\n    fig = go.Figure(data=traces)\n\n    fig.update_layout(\n        barmode=\"relative\",\n        # xaxis_title=\"Column Names\",\n        # yaxis_title=\"Quality Score\",\n        height=450,\n        bargap=0.01,\n        legend_orientation=\"h\",\n        legend_x=0.1,\n        legend_y=1.1,\n    )\n\n    fig.show(config={\"displaylogo\": False})\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.synthetic.SyntheticPrompt","title":"SyntheticPrompt","text":"<pre><code>SyntheticPrompt(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Prompt abstraction used in synthetic data generation workflows. Defines the generation logic and constraints.</p> <p>Bind API client reference for prompt actions. Stores configuration and prepares the object for use.</p> Source code in <code>lexsi_sdk/core/synthetic.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Bind API client reference for prompt actions.\n    Stores configuration and prepares the object for use.\"\"\"\n    super().__init__(**kwargs)\n    self.api_client = kwargs.get(\"api_client\")\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.synthetic.SyntheticPrompt.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Return the stored configuration list for the synthetic prompt.</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>prompt configuration</p> Source code in <code>lexsi_sdk/core/synthetic.py</code> <pre><code>def get_config(self) -&gt; List[dict]:\n    \"\"\"Return the stored configuration list for the synthetic prompt.\n\n    :return: prompt configuration\n    \"\"\"\n    return self.configuration\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.synthetic.SyntheticPrompt.get_expression","title":"get_expression","text":"<pre><code>get_expression()\n</code></pre> <p>Construct the textual expression for the synthetic prompt by concatenating conditional expressions defined in its metadata.</p> <p>Returns:</p> Type Description <code>str</code> <p>prompt expression</p> Source code in <code>lexsi_sdk/core/synthetic.py</code> <pre><code>def get_expression(self) -&gt; str:\n    \"\"\"Construct the textual expression for the synthetic prompt by concatenating conditional expressions defined in its metadata.\n\n    :return: prompt expression\n    \"\"\"\n    expression_list = []\n\n    if not self.metadata:\n        raise Exception(\"Expression not found.\")\n\n    for item in self.metadata[\"expression\"]:\n        if isinstance(item, dict):\n            expression_list.append(\n                f\"{item['column']} {item['expression']} {item['value']}\"\n            )\n        else:\n            expression_list.append(item)\n\n    return \" \".join(expression_list)\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.agent.AgentProject","title":"AgentProject","text":"<pre><code>AgentProject(**kwargs)\n</code></pre> <p>               Bases: <code>Project</code></p> <p>Project abstraction for agent-based workflows. Enables tracing, guardrail enforcement, tool invocation tracking, and agent execution analysis.</p> Source code in <code>lexsi_sdk/core/project.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize a `Project` instance and attach the API client.\n    Populates model fields from `kwargs` and stores `api_client` for later requests.\n\n    :param kwargs: Project fields used to construct the instance (including `api_client`).\n    \"\"\"\n    super().__init__(**kwargs)\n    self.api_client = kwargs.get(\"api_client\")\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.agent.AgentProject.messages","title":"messages","text":"<pre><code>messages(session_id)\n</code></pre> <p>Return a DataFrame listing all messages for a given session. Requires the session_id.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>id of the session</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>response</p> Source code in <code>lexsi_sdk/core/agent.py</code> <pre><code>def messages(self, session_id: str) -&gt; pd.DataFrame:\n    \"\"\"Return a DataFrame listing all messages for a given session. Requires the session_id.\n\n    :param session_id: id of the session\n    :return: response\n    \"\"\"\n    res = self.api_client.get(\n        f\"{MESSAGES_URI}?project_name={self.project_name}&amp;session_id={session_id}\"\n    )\n    if not res[\"success\"]:\n        raise Exception(res.get(\"details\"))\n\n    return pd.DataFrame(res.get(\"details\"))\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.agent.AgentProject.sessions","title":"sessions","text":"<pre><code>sessions()\n</code></pre> <p>Return a DataFrame listing all conversation sessions for this agent project.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>response</p> Source code in <code>lexsi_sdk/core/agent.py</code> <pre><code>def sessions(self) -&gt; pd.DataFrame:\n    \"\"\"Return a DataFrame listing all conversation sessions for this agent project.\n\n    :return: response\n    \"\"\"\n    res = self.api_client.get(f\"{SESSIONS_URI}?project_name={self.project_name}\")\n    if not res[\"success\"]:\n        raise Exception(res.get(\"details\"))\n\n    return pd.DataFrame(res.get(\"details\"))\n</code></pre>"},{"location":"sdk/#lexsi_sdk.core.agent.AgentProject.traces","title":"traces","text":"<pre><code>traces(trace_id)\n</code></pre> <p>Retrieve execution traces for a given trace ID for agent conversations. Returns a DataFrame of trace details.</p> <p>Parameters:</p> Name Type Description Default <code>trace_id</code> <code>str</code> <p>id of the trace</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>response</p> Source code in <code>lexsi_sdk/core/agent.py</code> <pre><code>def traces(self, trace_id: str) -&gt; pd.DataFrame:\n    \"\"\"Retrieve execution traces for a given trace ID for agent conversations. Returns a DataFrame of trace details.\n\n    :param trace_id: id of the trace\n    :return: response\n    \"\"\"\n    res = self.api_client.get(\n        f\"{TRACES_URI}?project_name={self.project_name}&amp;trace_id={trace_id}\"\n    )\n    if not res[\"success\"]:\n        raise Exception(res.get(\"details\"))\n\n    return pd.DataFrame(res.get(\"details\"))\n</code></pre>"},{"location":"sdk/#data-classes","title":"Data Classes","text":""},{"location":"sdk/#lexsi_sdk.common.types.BatchCPUInstanceTypeValues","title":"BatchCPUInstanceTypeValues","text":"<p>               Bases: <code>TypedDict</code></p> <p>Allowed values for batch CPU instance types.</p> <p>Values::</p> <pre><code>\"small\"\n\"xsmall\"\n\"2xsmall\"\n\"3xsmall\"\n\n\"medium\"\n\"xmedium\"\n\"2xmedium\"\n\"3xmedium\"\n\n\"large\"\n\"xlarge\"\n\"2xlarge\"\n\"3xlarge\"\n</code></pre>"},{"location":"sdk/#lexsi_sdk.common.types.BatchGPUInstanceTypeValues","title":"BatchGPUInstanceTypeValues","text":"<p>               Bases: <code>TypedDict</code></p> <p>Allowed values for batch GPU instance types.</p> <p>Values::</p> <pre><code>\"T4.small\"\n\"T4.xsmall\"\n\"T4.2xsmall\"\n\"T4.3xsmall\"\n\n\"A10G.medium\"\n\"A10G.xmedium\"\n\"A10G.2xmedium\"\n\"A10G.3xmedium\"\n</code></pre>"},{"location":"sdk/#lexsi_sdk.common.types.CatBoostParams","title":"CatBoostParams","text":"<p>               Bases: <code>TypedDict</code></p> <p>CatBoost hyperparameter configuration.</p> <p>Provide only the keys you want to override.</p> <p>Parameters:</p> Name Type Description Default <code>iterations</code> <code>int | None</code> <p>Number of boosting iterations. Typical range: 100\u201350000.</p> required <code>learning_rate</code> <code>float | None</code> <p>Learning rate. Range: 0.0\u20131.0 (commonly 0.01\u20130.3).</p> required <code>depth</code> <code>int | None</code> <p>Depth of the tree. Typical range: 1\u201316.</p> required <code>subsample_cb</code> <code>float | None</code> <p>Subsample ratio of training data (CatBoost). Range: 0.0\u20131.0.</p> required <code>colsample_bylevel_cb</code> <code>float | None</code> <p>Subsample ratio of columns per level (CatBoost). Range: 0.0\u20131.0.</p> required <code>min_data_in_leaf</code> <code>int | None</code> <p>Minimum number of samples in a leaf node. Typical range: 1\u2013200.</p> required"},{"location":"sdk/#lexsi_sdk.common.types.CustomServerConfig","title":"CustomServerConfig","text":"<p>               Bases: <code>TypedDict</code></p> <p>Scheduling options when requesting dedicated inference compute.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>datetime | None</code> <p>Start time for the server.</p> required <code>stop</code> <code>datetime | None</code> <p>Stop time for the server.</p> required <code>shutdown_after</code> <code>int | None</code> <p>Auto-shutdown timeout (in hours).</p> required <code>op_hours</code> <code>bool | None</code> <p>Whether to restrict to business hours.</p> required <code>auto_start</code> <code>bool</code> <p>Automatically start the server when requested.</p> required"},{"location":"sdk/#lexsi_sdk.common.types.DataConfig","title":"DataConfig","text":"<p>               Bases: <code>TypedDict</code></p> <p>Configuration controlling data selection, preprocessing, sampling, imbalance handling, and explainability.</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>list[str] | None</code> <p>Tags used to filter training data.</p> required <code>test_tags</code> <code>list[str] | None</code> <p>Tags used to construct the test/holdout dataset.</p> required <code>feature_exclude</code> <code>list[str] | None</code> <p>Features to exclude from training.</p> required <code>feature_encodings</code> <code>dict[str, str] | None</code> <p>Mapping of feature names to encoding strategies. Example: <code>{\"feature_a\": \"labelencode\", \"feature_b\": \"countencode\"}</code></p> required <code>drop_duplicate_uid</code> <code>bool</code> <p>Drop duplicate records based on a unique identifier.</p> required <code>use_optuna</code> <code>bool</code> <p>Enable Optuna for hyperparameter optimization.</p> required <code>sample_percentage</code> <code>float</code> <p>Fraction of data used for training (0.0\u20131.0).</p> required <code>explainability_sample_percentage</code> <code>float</code> <p>Fraction of data used for explainability computations.</p> required <code>lime_explainability_iterations</code> <code>int</code> <p>Number of LIME perturbation iterations.</p> required <code>explainability_method</code> <code>Literal['shap', 'lime'] | None</code> <p>Explainability method to apply. Supported values: <code>\"shap\"</code>, <code>\"lime\"</code>.</p> required <code>handle_data_imbalance</code> <code>bool</code> <p>Apply SMOTE to address class imbalance.</p> required"},{"location":"sdk/#lexsi_sdk.common.types.DedicatedCPUInstanceTypeValues","title":"DedicatedCPUInstanceTypeValues","text":"<p>               Bases: <code>TypedDict</code></p> <p>Allowed values for dedicated CPU instance types.</p> <p>Values::</p> <pre><code>\"t3.xlarge\"\n\"t3.2xlarge\"\n\n\"m4.large\"\n\"m4.xlarge\"\n\"m4.2xlarge\"\n\"m4.4xlarge\"\n\"m4.10xlarge\"\n\"m4.16xlarge\"\n\n\"c4.large\"\n\"c4.xlarge\"\n\"c4.2xlarge\"\n\"c4.4xlarge\"\n\"c4.8xlarge\"\n\n\"c5.9xlarge\"\n\"c5.12xlarge\"\n\"c5.18xlarge\"\n\"c5.24xlarge\"\n</code></pre>"},{"location":"sdk/#lexsi_sdk.common.types.DedicatedGPUInstanceTypeValues","title":"DedicatedGPUInstanceTypeValues","text":"<p>               Bases: <code>TypedDict</code></p> <p>Allowed values for dedicated GPU instance types.</p> <p>Values::</p> <pre><code>\"xlargeT4\"\n\"2xlargeT4\"\n\"4xlargeT4\"\n\"8xlargeT4\"\n\"12xlargeT4\"\n\"16xlargeT4\"\n\"32xlargeT4\"\n\n\"xlargeA10G\"\n\"2xlargeA10G\"\n\"4xlargeA10G\"\n\"8xlargeA10G\"\n\n\"4xlargeH100\"\n</code></pre>"},{"location":"sdk/#lexsi_sdk.common.types.FoundationalModelParams","title":"FoundationalModelParams","text":"<p>               Bases: <code>TypedDict</code></p> <p>Tabular foundational model configuration (TabTune Library).</p> <p>This config is used when <code>model_type</code> is one of the foundational models (e.g., <code>TabPFN</code>, <code>TabICL</code>, <code>TabDPT</code>, <code>OrionMSP</code>, <code>OrionBix</code>, <code>Mitra</code>, <code>ContextTab</code>). It controls execution device, fitting behavior, reproducibility, and probability calibration.</p>"},{"location":"sdk/#lexsi_sdk.common.types.FoundationalModelParams--notes","title":"Notes","text":"<ul> <li>This wrapper passes these fields into the underlying TabTune model runner.   Unsupported fields are ignored or may raise validation errors depending on   wrapper strictness.</li> <li>Some foundational models may not use all fields (e.g., <code>n_estimators</code>).</li> </ul> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Literal['cpu', 'cuda', 'auto'] | None</code> <p>Execution device for the foundational model. Supported by this wrapper: - <code>\"cpu\"</code>: Force CPU execution - <code>\"cuda\"</code>: Force GPU execution - <code>\"auto\"</code>: Select device automatically</p> required <code>fit_mode</code> <code>str | None</code> <p>Controls what is \"fit\" during the training stage. Common wrapper modes: - <code>\"fit_preprocessors\"</code>: fit only preprocessing / encoders - <code>\"fit_model\"</code>: fit the foundational model (and preprocessors if needed) - <code>\"fit_all\"</code>: run full pipeline fitting (preprocessors + model) If your wrapper only supports a subset, document/validate accordingly.</p> required <code>n_estimators</code> <code>int | None</code> <p>Number of estimators / ensemble members (if supported by the model). For models that don\u2019t use ensembles, this may be ignored.</p> required <code>n_jobs</code> <code>int | None</code> <p>Number of parallel jobs/threads to use. Use <code>-1</code> to utilize all available cores.</p> required <code>random_state</code> <code>int | None</code> <p>Random seed for reproducibility.</p> required <code>softmax_temperature</code> <code>float | None</code> <p>Temperature applied to logits before softmax for probability calibration. - <code>1.0</code> keeps probabilities unchanged - <code>&lt; 1.0</code> sharpens probabilities - <code>&gt; 1.0</code> smooths probabilities</p> required"},{"location":"sdk/#lexsi_sdk.common.types.GCSConfig","title":"GCSConfig","text":"<p>               Bases: <code>TypedDict</code></p> <p>Google Cloud Storage connector configuration.</p> <p>Parameters:</p> Name Type Description Default <code>project_id</code> <code>str</code> <p>GCP project identifier.</p> required <code>gcp_project_name</code> <code>str</code> <p>GCP project name.</p> required <code>type</code> <code>str</code> <p>Credentials type.</p> required <code>private_key_id</code> <code>str</code> <p>Service account private key ID.</p> required <code>private_key</code> <code>str</code> <p>Service account private key PEM string.</p> required <code>client_email</code> <code>str</code> <p>Service account email.</p> required <code>client_id</code> <code>str</code> <p>Service account client ID.</p> required <code>auth_uri</code> <code>str</code> <p>Auth URI.</p> required <code>token_uri</code> <code>str</code> <p>Token URI.</p> required"},{"location":"sdk/#lexsi_sdk.common.types.GDriveConfig","title":"GDriveConfig","text":"<p>               Bases: <code>TypedDict</code></p> <p>Google Drive connector configuration.</p> <p>Parameters:</p> Name Type Description Default <code>project_id</code> <code>str</code> <p>GCP project identifier.</p> required <code>type</code> <code>str</code> <p>Credentials type.</p> required <code>private_key_id</code> <code>str</code> <p>Service account private key ID.</p> required <code>private_key</code> <code>str</code> <p>Service account private key PEM string.</p> required <code>client_email</code> <code>str</code> <p>Service account email.</p> required <code>client_id</code> <code>str</code> <p>Service account client ID.</p> required <code>auth_uri</code> <code>str</code> <p>Auth URI.</p> required <code>token_uri</code> <code>str</code> <p>Token URI.</p> required"},{"location":"sdk/#lexsi_sdk.common.types.InferenceCompute","title":"InferenceCompute","text":"<p>               Bases: <code>TypedDict</code></p> <p>Inference compute selection payload.</p> <p>Parameters:</p> Name Type Description Default <code>compute_type</code> <code>Union[DedicatedCPUInstanceTypeValues, DedicatedGPUInstanceTypeValues, ServerlessInstanceTypeValues]</code> <p>Instance type identifier. Use str values from supported instance types defined in classes: - <code>DedicatedCPUInstanceTypeValues</code> - <code>DedicatedGPUInstanceTypeValues</code> - <code>ServerlessInstanceTypeValues</code></p> required <code>custom_server_config</code> <code>CustomServerConfig | None</code> <p>Optional scheduling configuration.</p> required"},{"location":"sdk/#lexsi_sdk.common.types.InferenceSettings","title":"InferenceSettings","text":"<p>               Bases: <code>TypedDict</code></p> <p>Inference settings that can be applied to models.</p> <p>Parameters:</p> Name Type Description Default <code>inference_engine</code> <code>str</code> <p>Inference engine for the models Inference Engine - <code>vLLM-AUTO</code> - <code>vLLM-FLASH_ATTN</code> - <code>vLLM-FLASHINFER</code> - <code>vLLM-XFORMERS</code> - <code>SGLang-AUTO</code> - <code>SGLang-TORCH_NATIVE</code> - <code>SGLang-FLASHINFER</code> - <code>SGLang-FA3</code> - <code>Transformers</code> - <code>Transformers-Serverless</code></p> required"},{"location":"sdk/#lexsi_sdk.common.types.LightGBMParams","title":"LightGBMParams","text":"<p>               Bases: <code>TypedDict</code></p> <p>LightGBM hyperparameter configuration.</p> <p>Keys correspond to common LightGBM training parameters. Provide only the keys you want to override.</p> <p>Parameters:</p> Name Type Description Default <code>boosting_type</code> <code>Literal['gbdt', 'dart'] | None</code> <p>Boosting algorithm type. Allowed values: <code>\"gbdt\"</code>, <code>\"dart\"</code>.</p> required <code>num_leaves</code> <code>int | None</code> <p>Maximum number of leaves in one tree. Typical range: 16\u20131024.</p> required <code>max_depth</code> <code>int | None</code> <p>Maximum tree depth. Use -1 for no limit (LightGBM convention) if your training code supports it.</p> required <code>learning_rate</code> <code>float | None</code> <p>Learning rate (shrinkage). Range: 0.0\u20131.0 (commonly 0.01\u20130.3).</p> required <code>n_estimators</code> <code>int | None</code> <p>Number of boosting iterations / trees. Typical range: 50\u20135000.</p> required <code>min_child_samples</code> <code>int | None</code> <p>Minimum number of data points in a leaf. Typical range: 5\u2013200.</p> required <code>min_child_weight</code> <code>float | None</code> <p>Minimum sum of hessian in one leaf. Range: &gt;= 0.</p> required <code>min_split_gain</code> <code>float | None</code> <p>Minimum gain required to make a split. Range: &gt;= 0.</p> required <code>subsample</code> <code>float | None</code> <p>Subsample ratio of the training instances (a.k.a. bagging_fraction). Range: 0.0\u20131.0.</p> required <code>colsample_bytree</code> <code>float | None</code> <p>Subsample ratio of columns when constructing each tree (a.k.a. feature_fraction). Range: 0.0\u20131.0.</p> required <code>tree_learner</code> <code>Literal['serial', 'voting', 'data', 'feature'] | None</code> <p>Tree learning algorithm. Allowed values: <code>\"serial\"</code>, <code>\"voting\"</code>, <code>\"data\"</code>, <code>\"feature\"</code>.</p> required <code>class_weight</code> <code>Literal['balanced'] | None</code> <p>Class weights. Allowed values: <code>\"balanced\"</code>.</p> required <code>random_state</code> <code>int | None</code> <p>Random seed for reproducibility.</p> required"},{"location":"sdk/#lexsi_sdk.common.types.PEFTParams","title":"PEFTParams","text":"<p>               Bases: <code>TypedDict</code></p> <p>Parameter-Efficient Fine-Tuning (PEFT) configuration (TabTune wrapper).</p> <p>This config enables lightweight adaptation (e.g., LoRA) for foundational models. It is typically used when <code>tuning_strategy=\"peft\"</code>.</p>"},{"location":"sdk/#lexsi_sdk.common.types.PEFTParams--notes","title":"Notes","text":"<ul> <li>Applies only to models/backbones that support PEFT in the wrapper.</li> <li>If the underlying model does not support PEFT, these options may be ignored   or raise an error depending on wrapper strictness.</li> <li>All parameters are optional; unspecified values fall back to defaults.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>int | None</code> <p>Rank of the low-rank adaptation matrices (LoRA rank). Typical values: <code>4</code>, <code>8</code>, <code>16</code>, <code>32</code>. Default: <code>8</code>.</p> required <code>lora_alpha</code> <code>int | None</code> <p>Scaling factor for LoRA layers. Typical values: <code>8</code>, <code>16</code>, <code>32</code>, <code>64</code>. Default: <code>16</code>.</p> required <code>lora_dropout</code> <code>float | None</code> <p>Dropout rate applied within LoRA layers. Range: <code>0.0</code> \u2013 <code>0.5</code> (commonly <code>0.0</code> \u2013 <code>0.1</code>). Default: <code>0.05</code>.</p> required"},{"location":"sdk/#lexsi_sdk.common.types.ProcessorParams","title":"ProcessorParams","text":"<p>               Bases: <code>TypedDict</code></p> <p>Data preprocessing and feature engineering configuration.</p> <p>These parameters control how input data is cleaned and transformed prior to training. The wrapper typically applies them before fitting either classical ML models or foundational tabular models.</p> <p>Parameters:</p> Name Type Description Default <code>imputation_strategy</code> <code>Literal['mean', 'median', 'mode', 'knn'] | None</code> <p>Strategy to handle missing values. Supported by this wrapper: - <code>\"mean\"</code>: numerical mean - <code>\"median\"</code>: numerical median - <code>\"mode\"</code>: most frequent value - <code>\"knn\"</code>: kNN-based imputation</p> required <code>scaling_strategy</code> <code>Literal['standard', 'minmax', 'robust'] | None</code> <p>Feature scaling method. Supported by this wrapper: - <code>\"standard\"</code>: standardization (z-score) - <code>\"minmax\"</code>: min-max scaling - <code>\"robust\"</code>: robust scaling (median/IQR)</p> required <code>resampling_strategy</code> <code>Literal['smote', 'random_oversample', 'none'] | None</code> <p>Strategy to address class imbalance (classification). Supported by this wrapper: - <code>\"smote\"</code>: SMOTE oversampling - <code>\"random_oversample\"</code>: random oversampling - <code>\"none\"</code>: do not resample</p> required"},{"location":"sdk/#lexsi_sdk.common.types.ProjectConfig","title":"ProjectConfig","text":"<p>               Bases: <code>TypedDict</code></p> <p>Configuration keys required to describe a project.</p> <p>Parameters:</p> Name Type Description Default <code>project_type</code> <code>str | None</code> <p>Project type identifier.</p> required <code>model_name</code> <code>str | None</code> <p>Model name associated with the project.</p> required <code>unique_identifier</code> <code>str</code> <p>Column name used as the unique identifier.</p> required <code>true_label</code> <code>str</code> <p>Column name containing ground-truth labels.</p> required <code>tag</code> <code>str</code> <p>Column name used to tag/filter records.</p> required <code>pred_label</code> <code>str | None</code> <p>Column name containing predicted labels (if present).</p> required <code>feature_exclude</code> <code>list[str] | None</code> <p>Features to exclude from training/inference.</p> required <code>drop_duplicate_uid</code> <code>bool | None</code> <p>Drop duplicate records based on the unique identifier.</p> required <code>handle_errors</code> <code>bool | None</code> <p>Whether to handle/ignore errors during processing.</p> required <code>feature_encodings</code> <code>dict | None</code> <p>Mapping of feature names to encoding strategies.</p> required <code>handle_data_imbalance</code> <code>bool | None</code> <p>Apply imbalance handling (e.g., SMOTE).</p> required <code>sample_percentage</code> <code>float | None</code> <p>Fraction of data used for training (0.0\u20131.0).</p> required <code>explainability_method</code> <code>list[str] | None</code> <p>Explainability methods to apply.</p> required"},{"location":"sdk/#lexsi_sdk.common.types.RandomForestParams","title":"RandomForestParams","text":"<p>               Bases: <code>TypedDict</code></p> <p>RandomForest hyperparameter configuration.</p> <p>Provide only the keys you want to override.</p> <p>Parameters:</p> Name Type Description Default <code>max_depth</code> <code>int | None</code> <p>Maximum depth of the tree. Use <code>None</code> for unlimited depth (if supported by your training wrapper).</p> required <code>max_features</code> <code>int | float | Literal['auto', 'sqrt', 'log2'] | None</code> <p>Number of features to consider when looking for the best split. Allowed values: - <code>\"auto\"</code> (implementation-dependent; often same as <code>\"sqrt\"</code> for classification) - <code>\"sqrt\"</code> - <code>\"log2\"</code> - <code>int</code> (absolute number of features) - <code>float</code> (fraction of features, 0.0\u20131.0)</p> required <code>max_leaf_nodes</code> <code>int | None</code> <p>Maximum number of leaf nodes.</p> required <code>min_samples_leaf</code> <code>int | None</code> <p>Minimum number of samples required to be at a leaf node. Typical range: 1\u201350.</p> required <code>min_samples_split</code> <code>int | None</code> <p>Minimum number of samples required to split an internal node. Typical range: 2\u2013200.</p> required <code>n_estimators</code> <code>int | None</code> <p>Number of trees in the forest. Typical range: 10\u20135000.</p> required <code>criterion</code> <code>Literal['gini', 'entropy', 'mse', 'squared_error'] | None</code> <p>Function to measure the quality of a split. Allowed values: - Classification: <code>\"gini\"</code>, <code>\"entropy\"</code> - Regression: <code>\"squared_error\"</code>, <code>\"mse\"</code></p> required"},{"location":"sdk/#lexsi_sdk.common.types.S3Config","title":"S3Config","text":"<p>               Bases: <code>TypedDict</code></p> <p>Amazon S3 connector configuration.</p> <p>Parameters:</p> Name Type Description Default <code>region</code> <code>str | None</code> <p>AWS region (e.g., <code>\"us-east-1\"</code>).</p> required <code>access_key</code> <code>str</code> <p>AWS access key ID.</p> required <code>secret_key</code> <code>str</code> <p>AWS secret access key.</p> required"},{"location":"sdk/#lexsi_sdk.common.types.SFTPConfig","title":"SFTPConfig","text":"<p>               Bases: <code>TypedDict</code></p> <p>SFTP connector configuration.</p> <p>Parameters:</p> Name Type Description Default <code>hostname</code> <code>str</code> <p>SFTP host.</p> required <code>port</code> <code>str</code> <p>SFTP port.</p> required <code>username</code> <code>str</code> <p>SFTP username.</p> required <code>password</code> <code>str</code> <p>SFTP password.</p> required"},{"location":"sdk/#lexsi_sdk.common.types.ServerlessInstanceTypeValues","title":"ServerlessInstanceTypeValues","text":"<p>               Bases: <code>TypedDict</code></p> <p>Allowed values for serverless instance types.</p> <p>Values::</p> <pre><code>\"nova-0.5\"\n\"nova-1\"\n\"nova-1.5\"\n\"nova-2\"\n\"nova-4\"\n\"nova-6\"\n\"nova-8\"\n\"nova-10\"\n\n\"gova-0.5\"\n\"gova-1\"\n\"gova-1.5\"\n\"gova-2\"\n\"gova-4\"\n\"gova-6\"\n\"gova-8\"\n\"gova-10\"\n\"gova-12\"\n\"gova-14\"\n\"gova-16\"\n\"gova-18\"\n\"gova-20\"\n\"gova-22\"\n\"gova-24\"\n</code></pre>"},{"location":"sdk/#lexsi_sdk.common.types.SyntheticDataConfig","title":"SyntheticDataConfig","text":"<p>               Bases: <code>TypedDict</code></p> <p>Configuration required when generating synthetic data.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Synthetic model name (e.g., CTGAN/GPT2 tabular).</p> required <code>tags</code> <code>list[str]</code> <p>Tags used to filter source data.</p> required <code>feature_exclude</code> <code>list[str]</code> <p>Features to exclude from synthetic training/generation.</p> required <code>feature_include</code> <code>list[str]</code> <p>Features to include for synthetic training/generation.</p> required <code>feature_actual_used</code> <code>list[str]</code> <p>Final set of features actually used (post-validation).</p> required <code>drop_duplicate_uid</code> <code>bool</code> <p>Drop duplicate records based on a unique identifier.</p> required"},{"location":"sdk/#lexsi_sdk.common.types.SyntheticModelHyperParams","title":"SyntheticModelHyperParams","text":"<p>               Bases: <code>TypedDict</code></p> <p>Common hyperparameter keys for supported synthetic models.</p> <p>GPT2-related keys:</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int | None</code> <p>Training batch size.</p> required <code>early_stopping_patience</code> <code>int | None</code> <p>Epochs to wait before early stopping.</p> required <code>early_stopping_threshold</code> <code>float | None</code> <p>Minimum improvement threshold for early stopping.</p> required <code>epochs</code> <code>int | None</code> <p>Training epochs.</p> required <code>model_type</code> <code>str | None</code> <p>Model type identifier.</p> required <code>random_state</code> <code>int | None</code> <p>Random seed.</p> required <code>tabular_config</code> <code>str | None</code> <p>Tabular configuration identifier/name.</p> required <code>train_size</code> <code>float | None  CTGAN-related keys:</code> <p>Fraction of data used for training (0.0\u20131.0).</p> required <code>test_ratio</code> <code>float | None</code> <p>Fraction of data used for validation/testing (0.0\u20131.0).</p> required"},{"location":"sdk/#lexsi_sdk.common.types.TuningParams","title":"TuningParams","text":"<p>               Bases: <code>TypedDict</code></p> <p>Hyperparameter tuning / fine-tuning configuration (TabTune wrapper).</p> <p>This config controls optimization loops used in: - meta-learning or episodic training - few-shot adaptation - iterative fine-tuning / search</p>"},{"location":"sdk/#lexsi_sdk.common.types.TuningParams--notes","title":"Notes","text":"<ul> <li>These fields may be used only for foundational models (depending on the   wrapper logic).</li> <li>If both episodic (support/query) and standard training fields are provided,   the wrapper should define precedence clearly.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>epochs</code> <code>int | None</code> <p>Number of training epochs. Typical range: 1\u2013200 (depends on model and dataset size).</p> required <code>learning_rate</code> <code>float | None</code> <p>Learning rate used during optimization. Common range: 1e-5\u20131e-1.</p> required <code>batch_size</code> <code>int | None</code> <p>Number of samples per batch. Typical range: 8\u20134096 depending on model and memory.</p> required <code>support_size</code> <code>int | None</code> <p>Number of support samples per episode (few-shot). Example: 16, 32, 64.</p> required <code>query_size</code> <code>int | None</code> <p>Number of query samples per episode (few-shot). Example: 16, 32, 64.</p> required <code>n_episodes</code> <code>int | None</code> <p>Number of episodes for meta-learning / episodic training. Typical range: 50\u20135000.</p> required <code>steps_per_epoch</code> <code>int | None</code> <p>Number of optimization steps per epoch. If not provided, the wrapper may infer it from dataset size and batch size.</p> required"},{"location":"sdk/#lexsi_sdk.common.types.XGBoostParams","title":"XGBoostParams","text":"<p>               Bases: <code>TypedDict</code></p> <p>XGBoost hyperparameter configuration.</p> <p>Keys correspond to common XGBoost training parameters. Provide only the keys you want to override.</p> <p>Parameters:</p> Name Type Description Default <code>objective</code> <code>str | None</code> <p>Learning objective. Common values: <code>\"binary:logistic\"</code>, <code>\"binary:logitraw\"</code>, <code>\"multi:softprob\"</code>, <code>\"reg:squarederror\"</code>, <code>\"reg:logistic\"</code>, <code>\"rank:pairwise\"</code>.</p> required <code>booster</code> <code>Literal['gbtree', 'gblinear'] | None</code> <p>Booster type. Allowed values: <code>\"gbtree\"</code>, <code>\"gblinear\"</code>.</p> required <code>eval_metric</code> <code>str | None</code> <p>Evaluation metric used during training. Common values: <code>\"logloss\"</code>, <code>\"auc\"</code>, <code>\"aucpr\"</code>, <code>\"error\"</code>, <code>\"rmse\"</code>, <code>\"mae\"</code>, <code>\"merror\"</code>, <code>\"mlogloss\"</code>.</p> required <code>grow_policy</code> <code>Literal['depthwise', 'lossguide'] | None</code> <p>Tree growth policy (tree-based boosters). Allowed values: <code>\"depthwise\"</code>, <code>\"lossguide\"</code>.</p> required <code>max_depth</code> <code>int | None</code> <p>Maximum depth of a tree. Typical range: 1\u201316.</p> required <code>max_leaves</code> <code>int | None</code> <p>Maximum number of leaves per tree (used with <code>lossguide</code>). Typical range: 0\u20134096 (0 means \"no limit\" depending on implementation).</p> required <code>min_child_weight</code> <code>float | None</code> <p>Minimum sum of instance weight needed in a child. Higher values make the model more conservative.</p> required <code>colsample_bytree</code> <code>float | None</code> <p>Subsample ratio of columns when constructing each tree. Range: 0.0\u20131.0.</p> required <code>colsample_bylevel</code> <code>float | None</code> <p>Subsample ratio of columns for each level. Range: 0.0\u20131.0.</p> required <code>colsample_bynode</code> <code>float | None</code> <p>Subsample ratio of columns for each split/node. Range: 0.0\u20131.0.</p> required <code>learning_rate</code> <code>float | None</code> <p>Step size shrinkage (eta). Range: 0.0\u20131.0 (commonly 0.01\u20130.3).</p> required <code>n_estimators</code> <code>int | None</code> <p>Number of boosting rounds / trees. Typical range: 50\u20135000.</p> required <code>subsample</code> <code>float | None</code> <p>Subsample ratio of the training instances. Range: 0.0\u20131.0.</p> required <code>alpha</code> <code>float | None</code> <p>L1 regularization term on weights (reg_alpha). Range: &gt;= 0.</p> required <code>lambda_</code> <code>float | None</code> <p>L2 regularization term on weights (reg_lambda). Range: &gt;= 0.</p> required <code>seed</code> <code>int | None</code> <p>Random seed for reproducibility.</p> required"},{"location":"sdk/#lexsi_sdk.common.monitoring.BiasMonitoringPayload","title":"BiasMonitoringPayload","text":"<p>               Bases: <code>TypedDict</code></p> <p>Payload schema for bias monitoring dashboards.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>Project identifier,</p> required <code>base_line_tag</code> <code>List[str]</code> <p>List of tags to be used for Baseline dataset.</p> required <code>date_feature</code> <code>Optional[str]</code> <p>Optional feature used for Date Feature.</p> required <code>baseline_date</code> <code>Optional[dict]</code> <p>Optional baseline date range filter { \"start_date\": \"\", \"end_date\": \"\"}.</p> required <code>model_type</code> <code>str</code> <p>Model type for drift analysis <code>classification</code>, <code>regression</code>.</p> required <code>baseline_true_label</code> <code>str</code> <p>Baseline true label column name.</p> required <code>baseline_pred_label</code> <code>str</code> <p>Baseline predicted label column name.</p> required <code>features_to_use</code> <code>List[str]</code> <p>List of feature names to be used for bias analysis.</p> required <code>stat_test_threshold</code> <p>Threshold for the statistical test.</p> required"},{"location":"sdk/#lexsi_sdk.common.monitoring.DataDriftPayload","title":"DataDriftPayload","text":"<p>               Bases: <code>TypedDict</code></p> <p>Payload schema for data drift dashboards.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>Optional[str]</code> <p>Project identifier,</p> required <code>base_line_tag</code> <code>List[str]</code> <p>List of tags to be used for Baseline dataset.</p> required <code>current_tag</code> <code>List[str]</code> <p>List of tags to be used for Current dataset.</p> required <code>date_feature</code> <code>Optional[str]</code> <p>Optional feature used for Date Feature.</p> required <code>baseline_date</code> <code>Optional[dict]</code> <p>Optional baseline date range filter { \"start_date\": \"\", \"end_date\": \"\"}.</p> required <code>current_date</code> <code>Optional[dict]</code> <p>Optional current date range filter { \"start_date\": \"\", \"end_date\": \"\"}.</p> required <code>features_to_use</code> <code>List[str]</code> <p>List of feature names to be used for drift analysis.</p> required <code>stat_test_name</code> <code>str</code> <p>Statistical test name for drift detection. key values for payload: stat_test_name <code>chisquare</code> (Chi-Square test): default for categorical features if the number of labels for feature &gt; 2 only for categorical features returns p_value default threshold 0.05 drift detected when p_value &lt; threshold <code>jensenshannon</code> (Jensen-Shannon distance): for numerical and categorical features returns distance default threshold 0.05 drift detected when distance &gt;= threshold <code>ks</code> (Kolmogorov\u2013Smirnov (K-S) test): default for numerical features only for numerical features returns p_value default threshold 0.05 drift detected when p_value &lt; threshold <code>kl_div</code> (Kullback-Leibler divergence): for numerical and categorical features returns divergence default threshold 0.05 drift detected when divergence &gt;= threshold, <code>psi</code> (Population Stability Index): for numerical and categorical features returns psi_value default_threshold=0.1 drift detected when psi_value &gt;= threshold <code>wasserstein</code> (Wasserstein distance (normed)): only for numerical features returns distance default threshold 0.05 drift detected when distance &gt;= threshold <code>z</code> (Ztest): default for categorical features if the number of labels for feature &lt;= 2 only for categorical features returns p_value default threshold 0.05 drift detected when p_value &lt; threshold</p> required <code>stat_test_threshold</code> <code>str</code> <p>Threshold for the statistical test.</p> required"},{"location":"sdk/#lexsi_sdk.common.monitoring.ImageDashboardPayload","title":"ImageDashboardPayload","text":"<p>               Bases: <code>TypedDict</code></p> <p>Payload schema for image monitoring dashboards.</p> <p>Parameters:</p> Name Type Description Default <code>base_line_tag</code> <code>List[str]</code> <p>Baseline dataset tags to compare against.</p> required <code>current_tag</code> <code>List[str]</code> <p>Current dataset tags to compare.</p> required"},{"location":"sdk/#lexsi_sdk.common.monitoring.ModelPerformancePayload","title":"ModelPerformancePayload","text":"<p>               Bases: <code>TypedDict</code></p> <p>Payload schema for model performance dashboards.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>Project identifier.</p> required <code>base_line_tag</code> <code>List[str]</code> <p>list of Baseline dataset tags to compare against.</p> required <code>current_tag</code> <code>List[str]</code> <p>list of Current dataset tags to compare.</p> required <code>date_feature</code> <code>Optional[str]</code> <p>Optional feature used for temporal filtering.</p> required <code>baseline_date</code> <code>Optional[dict]</code> <p>Optional baseline date range filter { \"start_date\": \"\", \"end_date\": \"\"}.</p> required <code>current_date</code> <code>Optional[dict]</code> <p>Optional current date range filter { \"start_date\": \"\", \"end_date\": \"\"}.</p> required <code>baseline_true_label</code> <code>str</code> <p>Baseline true label column name.</p> required <code>baseline_pred_label</code> <code>str</code> <p>Baseline predicted label column name.</p> required <code>current_true_label</code> <code>str</code> <p>Current true label column name.</p> required <code>current_pred_label</code> <code>str</code> <p>Current predicted label column name.</p> required <code>model_type</code> <code>str</code> <p>Model type for performance evaluation.</p> required"},{"location":"sdk/#lexsi_sdk.common.monitoring.TargetDriftPayload","title":"TargetDriftPayload","text":"<p>               Bases: <code>TypedDict</code></p> <p>Payload schema for target drift dashboards.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>Project identifier,</p> required <code>base_line_tag</code> <code>List[str]</code> <p>List of tags to be used for Baseline dataset.</p> required <code>current_tag</code> <code>List[str]</code> <p>List of tags to be used for Current dataset.</p> required <code>date_feature</code> <code>Optional[str]</code> <p>Optional feature used for Date Feature.</p> required <code>baseline_date</code> <code>Optional[dict]</code> <p>Optional baseline date range filter { \"start_date\": \"\", \"end_date\": \"\"}.</p> required <code>current_date</code> <code>Optional[dict]</code> <p>Optional current date range filter { \"start_date\": \"\", \"end_date\": \"\"}.</p> required <code>model_type</code> <code>str</code> <p>Model type for drift analysis <code>classification</code>, <code>regression</code>.</p> required <code>baseline_true_label</code> <code>str</code> <p>Baseline true label column name.</p> required <code>current_true_label</code> <code>str</code> <p>Current true label column name.</p> required <code>stat_test_name</code> <p>Statistical test name for drift detection. key values for payload: stat_test_name <code>chisquare</code> (Chi-Square test): default for categorical features if the number of labels for feature &gt; 2 only for categorical features returns p_value default threshold 0.05 drift detected when p_value &lt; threshold <code>jensenshannon</code> (Jensen-Shannon distance): for numerical and categorical features returns distance default threshold 0.05 drift detected when distance &gt;= threshold <code>ks</code> (Kolmogorov\u2013Smirnov (K-S) test): default for numerical features only for numerical features returns p_value default threshold 0.05 drift detected when p_value &lt; threshold <code>kl_div</code> (Kullback-Leibler divergence): for numerical and categorical features returns divergence default threshold 0.05 drift detected when divergence &gt;= threshold, <code>psi</code> (Population Stability Index): for numerical and categorical features returns psi_value default_threshold=0.1 drift detected when psi_value &gt;= threshold <code>wasserstein</code> (Wasserstein distance (normed)): only for numerical features returns distance default threshold 0.05 drift detected when distance &gt;= threshold <code>z</code> (Ztest): default for categorical features if the number of labels for feature &lt;= 2 only for categorical features returns p_value default threshold 0.05 drift detected when p_value &lt; threshold :type stat_test_name: str</p> required <code>stat_test_threshold</code> <code>float</code> <p>Threshold for the statistical test.</p> required"}]}